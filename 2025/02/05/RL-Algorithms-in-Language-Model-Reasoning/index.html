<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"right","width":200,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Summary of RL Algorithms">
<meta property="og:type" content="article">
<meta property="og:title" content="Review of Reinforcement Learning">
<meta property="og:url" content="http://example.com/2025/02/05/RL-Algorithms-in-Language-Model-Reasoning/index.html">
<meta property="og:site_name" content="Blog of Ethan@UW">
<meta property="og:description" content="Summary of RL Algorithms">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/image-20250209201756802.png">
<meta property="article:published_time" content="2025-02-06T01:59:57.000Z">
<meta property="article:modified_time" content="2025-02-10T05:19:17.558Z">
<meta property="article:author" content="Ethan Deng">
<meta property="article:tag" content="Research">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20250209201756802.png">

<link rel="canonical" href="http://example.com/2025/02/05/RL-Algorithms-in-Language-Model-Reasoning/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Review of Reinforcement Learning | Blog of Ethan@UW</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?f00a163a49048bdcbc142b35e0ee9704";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Blog of Ethan@UW</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/05/RL-Algorithms-in-Language-Model-Reasoning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="Ethan Deng">
      <meta itemprop="description" content="Oops!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Ethan@UW">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Review of Reinforcement Learning
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-02-05 17:59:57" itemprop="dateCreated datePublished" datetime="2025-02-05T17:59:57-08:00">2025-02-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-02-09 21:19:17" itemprop="dateModified" datetime="2025-02-09T21:19:17-08:00">2025-02-09</time>
              </span>

          
            <span id="/2025/02/05/RL-Algorithms-in-Language-Model-Reasoning/" class="post-meta-item leancloud_visitors" data-flag-title="Review of Reinforcement Learning" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
            <div class="post-description">Summary of RL Algorithms</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="rl-basics">RL basics</h1>
<p>In a RL problem, we have an agent and an environment that the agent
can interact with. At each time step, the agent is given a state of the
environment <span class="math inline">\(s\)</span>, and the agent is
supposed to take an action <span class="math inline">\(a\)</span> to
interact with the environment. Eventually, the agent will get a reward
<span class="math inline">\(r\)</span>. The goal of the agent is to
learn a policy <span class="math inline">\(\pi\)</span>, defined as
<span class="math display">\[
\pi(s, a) = \Pr[a~|~s].
\]</span> which gives the probability of an action <span
class="math inline">\(a\)</span> being sampled at the given state <span
class="math inline">\(s\)</span>.</p>
<p>Given a policy <span class="math inline">\(\pi\)</span> and a
starting state <span class="math inline">\(s\)</span>, we define the
<strong>value</strong> of the state <span
class="math inline">\(s\)</span> to be the expected reward of starting
from <span class="math inline">\(s\)</span> with policy <span
class="math inline">\(\pi\)</span>, hence the <strong>value
function</strong> <span class="math inline">\(V^{\pi}(s)\)</span> is
defined as <span class="math display">\[
V^{\pi}(s) := \mathbb{E}_{\pi}[\mathbf{R}_t ~|~ s_t=s] =
\mathbb{E}_{\pi}\left[\sum_{k=0}^\infty\gamma^tr_{t+k+1} ~|~ s_t =
s\right],
\]</span> where <span class="math inline">\(\gamma \in (0,1)\)</span> is
a reward decaying parameter and <span
class="math inline">\(\mathbf{R}_t\)</span> is defined to be the
cumulative reward starting from time <span
class="math inline">\(t\)</span>. Given a policy <span
class="math inline">\(\pi\)</span>, a state <span
class="math inline">\(s\)</span> and an action <span
class="math inline">\(a\)</span>, we can define the action-value
function Q to be the expected reward of the state-action pair <span
class="math inline">\((s, a)\)</span>, defined as <span
class="math display">\[
Q^\pi(s,a) := \mathbb{E}_\pi[\mathbf{R}_t~|~s_t=s,a_t=a].
\]</span> Given the definitions, we have the following relation <span
class="math display">\[
V^\pi(s) = \sum_{a \in A}\pi(a~|~s)Q^{\pi}(s,a).
\]</span></p>
<h1 id="markov-decision-process">Markov Decision Process</h1>
<p>If the environment state is a Markov process and there is a model
describing the environment that for each triplet of a starting state
<span class="math inline">\(s\)</span>, an action <span
class="math inline">\(a\)</span> taken and a ending state <span
class="math inline">\(s&#39;\)</span>, it gives the probability in the
form of <span class="math display">\[
\Pr(s&#39;, s, a) = \Pr[s_{k+1} = s&#39;~|~ s_k = s, a_k= a].
\]</span> then it is a <strong>Markov Decision Process</strong>, or MDP.
It is a Model-based type RL problem.</p>
<p>Assume the reward is defined as <span class="math display">\[
R(s&#39;, s,a) = \mathbb{E}[r_{k+1} ~|~ s_{k+1} = s&#39;, s_k = s, a_k=
a].
\]</span> ## Bellman's Equations</p>
<p>For the value functions, we have <span class="math display">\[
\begin{align*}
    V(s)=&amp;~ \mathbb{E}_\pi \left[r_{t+1} +
\sum_{k=1}^\infty\gamma^tr_{t+k+1} ~|~ s_{t} = s\right] \\
        =&amp;~ \mathbb{E}_\pi \left[r_{t+1} + \gamma V(s&#39;)~|~ s_{t}
= s\right],
\end{align*}
\]</span> where <span class="math inline">\(s&#39;\)</span> is a random
state dominated by current state and action. Here we get a recursive
expression named <strong>Bellman's Expectation Equation</strong>.</p>
<p>Again, our goal in RL is to find a policy, which maximizes the
expected return from a starting state. Let's now define the partial
order between the policies: if for all state <span
class="math inline">\(s\)</span>, we have <span
class="math inline">\(V^\pi(s) \ge V^{\pi&#39;}(s)\)</span>, then we say
<span class="math inline">\(\pi&gt;\pi&#39;\)</span>. In MDP with finite
state set and action set, there exists at lease one policy <span
class="math inline">\(\pi^*\)</span> such that for any other policy
<span class="math inline">\(\pi&#39;\)</span>, it holds that <span
class="math inline">\(\pi^*&gt;\pi&#39;\)</span>. We call it the
<strong>optimal policy</strong>. With this, we can define the optimal
value function <span class="math display">\[
V^*(s) := \max_{\pi}V^{\pi}(s),~~\forall s \in \mathcal{S}.
\]</span> Also we can define the optimal Q function <span
class="math display">\[
Q^*(s,a) = \max_{\pi}Q^{\pi}(s,a),~~\forall s \in \mathcal{S}, a \in
\mathcal{A}.
\]</span> To connect them, we have <span class="math display">\[
\begin{align*}
Q^*(s,a) =&amp;~ r(s,a) + \gamma
\sum_{s&#39;\in\cal{S}}\Pr[s&#39;~|~s,a]V^*(s&#39;) \\
V^*(s) =&amp;~ \max_{a \in \cal{A}}Q^*(s,a).
\end{align*}
\]</span> Hence we will have the following <strong>Bellman's Optimality
Equation</strong>: <span class="math display">\[
\begin{align*}
    V^*(s)  =&amp;~ \max_{a\in\cal{A}} \left\{r(s,a) +
\gamma\sum_{s&#39; \in \cal{S}}\Pr[s&#39;~|~a,a]V^*(s&#39;)\right\} \\
    Q^*(s,a)=&amp;~ r(s,a) +
\gamma\sum_{s&#39;\in\cal{S}}\Pr[s&#39;~|~s,a]\max_{a&#39; \in
\cal{A}}Q^*(s&#39;,s&#39;).
\end{align*}
\]</span></p>
<h2 id="value-iteration">Value Iteration</h2>
<p>The goal of value iteration is to find the optimal value function
<span class="math inline">\(V^*\)</span>. Starting from random (or
all-zero) initialization, given Bellman's Optimality Equation, for every
round, we update the value function by <span class="math display">\[
\begin{align*}
    V^{k+1}(s) \gets &amp;~ \max_{a}\sum_{s&#39;}\Pr[s&#39;~|~s,
a]\left(r(s,a) + \gamma V^{k}(s&#39;)\right) \\
        =&amp;~\max_{a}Q^k(s,a)
\end{align*}
\]</span> where <span class="math inline">\(\Pr(s&#39;, s, a)\)</span>
and <span class="math inline">\(r(s,a)\)</span> are assumed known, and
we use $ V^k$ and <span class="math inline">\(Q^k\)</span> to represent
the intermediate <span class="math inline">\(V\)</span> and <span
class="math inline">\(Q\)</span> for approximating during the algorithm.
We start by initiating <span class="math inline">\(\hat{V}\)</span>'s
for every <span class="math inline">\(s\)</span> to be <span
class="math inline">\(0\)</span> or random. Then starting from a random
state <span class="math inline">\(s_0\)</span> and update the value by
picking the action <span class="math inline">\(a\)</span> that maximizes
the value. Then we get to the next state. By iteratively doing this, we
are able to finally construct good estimate to <span
class="math inline">\(\hat{V}\)</span>'s. When the values converges, we
get the optimal policy <span class="math inline">\(\pi^*\)</span> by
<span class="math display">\[
\begin{align*}
    \pi^*(s) =&amp;~ {\arg\max}_{a}\sum_{s&#39;}\Pr[s&#39;~|~s,
a]\left(r(s,a) + \gamma V^*(s&#39;)\right) \\
        =&amp;~{\arg\max}_{a}Q^*(s,a)
\end{align*}
\]</span> It's a type of dynamic programming.</p>
<h2 id="policy-iteration">Policy Iteration</h2>
<p>Here we start by locking the policy <span
class="math inline">\(\pi\)</span>, and instead of updating optimal
values by taking the best action like we did in value iteration, we
update the value function with policy <span
class="math inline">\(\pi\)</span> by picking actions using <span
class="math inline">\(\pi\)</span>. Like we did in the Bellman's
Equation for optimal value functions <span
class="math inline">\(V^*\)</span>'s, we can get the same for <span
class="math inline">\(V^\pi\)</span>: <span class="math display">\[
\begin{align*}
    V^\pi(s)=&amp;~ \mathbb{E}_\pi[R(s&#39;,s,\pi(s)) + \gamma
V^\pi(s&#39;)] \\
        =&amp;~\sum_{s&#39;}\Pr[s&#39;~|~s, \pi(s)] \left(r(s,\pi(s)) +
\gamma V^\pi(s&#39;)\right)\\
        =&amp;~ Q(s,\pi(s))
\end{align*}
\]</span> And we update policy by <span class="math display">\[
\pi^{k+1}(s) = {\arg\max}_a \mathbb{E}[r(s,a) + \gamma
V^{\pi_{k}}(s&#39;)] = {\arg\max}_aQ^{\pi_k}(s,a)
\]</span> Then we lock the policy and update the value functions. By
iteratively doing this, we are able to get converged results.</p>
<h2 id="use-of-q-function">Use of Q-function</h2>
<p>By using Q function, the value iteration and the policy iteration can
be summarized by simply <span class="math display">\[
\begin{align*}
    V(s) =&amp;~ \max_aQ(s,a) \\
    \text{and}~~\pi(s) =&amp;~ {\arg\max}_aQ(s,a).
\end{align*}
\]</span> The good thing here is that Q function naturally encodes the
information about the future so we can directly maintain its value
instead of calculating value and pi each time.</p>
<h1 id="q-learning">Q-Learning</h1>
<p>Q-learning is used when we don't have access of the model of the
environment (model-free RL).</p>
<h2 id="monte-carlo-learning">Monte Carlo Learning</h2>
<p>We start from Monte Carlo Learning, which is the naive way (pure
trial and error) of model-free RL. By the trials of the game, we have
the cumulative reward over an episode <span class="math display">\[
\mathbf{R}_t := \sum_{k = 0}^{n}\gamma^{t}r_{t+k+1}.
\]</span> Then update the value functions or Q functions by <span
class="math display">\[
\begin{align*}
    V^{\text{new}}(s_k) &amp;~\gets
V^{\text{old}}(s_k)+\frac{1}{n}\left(\mathbf{R}_k -
V^{\text{old}}(s_k)\right)~~\forall k \in [n] \\
    Q^{\text{new}}(s_k, a_k) &amp;~\gets
Q^{\text{old}}(s_k,a_k)+\frac{1}{n}\left(\mathbf{R}_k -
Q^{\text{old}}(s_k,a_k)\right)~~\forall k \in [n]
\end{align*}
\]</span> Mathematically this is not biased. So this in theory will
finally converges. But it is relatively inefficient.</p>
<h2 id="temporal-difference-learning">Temporal Difference Learning</h2>
<p>TD learning integrates the information of time intervals, which can
be connected with biological learning or some topics in neuroscience. We
start with TD(0) learning. Recall the Bellman's Equation <span
class="math display">\[
V(s_k) = \mathbb{E}[r_k + \gamma V(s_{k+1})]
\]</span> Then in TD(0), we update the value function by <span
class="math display">\[
\begin{align*}
    V^{\text{new}}(s_k) &amp;~\gets
V^{\text{old}}(s_k)+\alpha\left(\overbrace{\underbrace{r_k + \gamma
V^{\text{old}}(s_{k+1})}_{\text{TD target estimate }\mathbf{R_k}} -
V^{\text{old}}(s_k)}^{\text{TD Error}}\right) ~~\forall k \in [n]
\end{align*}
\]</span> For TD(N), we expand the value function as <span
class="math display">\[
\begin{align*}
    V(s_k) =&amp;~ \mathbb{E}[r_k + \gamma V(s_{k+1})] \\
        =&amp;~ \mathbb{E} [r_k + \gamma r_{k+1} + \gamma^2V(s_{k+2})]\\
        =&amp;~ \mathbb{E} \left[\sum_{j=0}^N \gamma^j r_{k+j} +
\gamma^{N+1}V(s_{k+N+1})\right]
\end{align*}
\]</span> where we define <span class="math inline">\(\mathbf{R}^{(N)}
:= \sum_{j=0}^N \gamma^j r_{k+j} + \gamma^{N+1}V(s_{k+N+1})\)</span>.
And hence the value function is updated by <span class="math display">\[
\begin{align*}    
    V^{\text{new}}(s_k) &amp;~\gets
V^{\text{old}}(s_k)+\alpha\left(\overbrace{\underbrace{\sum_{j=0}^N
\gamma^j r_{k+j} + \gamma^{N+1}V^{\text{old}}(s_{k+N+1})}_{\text{TD
target estimate }\mathbf{R_t}} - V^{\text{old}}(s_k)}^{\text{TD
Error}}\right) ~~\forall k \in [n].
\end{align*}
\]</span> By letting <span class="math inline">\(N\)</span> to the
episode size or infinity, this converges to Monte Carlo learning.</p>
<p>There is another type of TD learning algorithms named TD-<span
class="math inline">\(\lambda\)</span>, which calculates cumulative
rewards from TD(0) to TD(N) and taking their weighted some in the
following way <span class="math display">\[
\mathbf{R}^{\lambda} := (1-\lambda)\sum_{n=1}^\infty
\lambda^{n-1}\mathbf{R}^{(n)}.
\]</span></p>
<h2 id="q-learning-1">Q-Learning</h2>
<p>Q-Learning is temporal learning on Q functions, where in its TD(0)
version, the Q functions are updated by <span class="math display">\[
Q^{\text{new}}(s_k,a_k) \gets Q^{\text{old}}(s_k,a_k) + \alpha \left(r_k
+ \gamma\max_{a}Q^{\text{old}}(s_{k+1}, a)-Q^{\text{old}}(s_k,a_k)
\right)
\]</span> Note that here we do not need necessarily to take the optimal
action a to get to the next state, so this TD(0) Q-learning is
<strong>off policy</strong>. In Q-learning, every time when the agent is
about to take an action, a widely used way is by <span
class="math inline">\(\epsilon\)</span>-greedy policy, which means that
it has a probability <span
class="math inline">\(1-\epsilon\in(0,1)\)</span> to take the best
action, and <span class="math inline">\(\epsilon\)</span> to take a
random action from all the action space.</p>
<h2 id="sarsa-state-action-reward-state-action">SARSA:
State-Action-Reward-State-Action</h2>
<p>SARSA is a variant of Q learning which is an
<strong>on-policy</strong> algorithm, by updating Q functions in the
following way <span class="math display">\[
Q^{\text{new}}(s_k,a_k) \gets Q^{\text{old}}(s_k,a_k) + \alpha \left(r_k
+ \gamma {\color{red} Q^{\text{old}}(s_{k+1},
a_{k+1})}-Q^{\text{old}}(s_k,a_k) \right)
\]</span> SARSA also works with TD(N) for any <span
class="math inline">\(N &gt; 0\)</span>.</p>
<h1 id="deep-rl">Deep RL</h1>
<h2 id="deep-policy-network-policy-gradient">Deep Policy Network (Policy
Gradient)</h2>
<p>By introducing the neural network into the policy function, we use a
network to approximate the policy, so a policy function parameterized by
<span class="math inline">\(\theta\)</span> can be written as <span
class="math display">\[
\pi_{\theta}(s,a).
\]</span> Then to update the policy <span
class="math inline">\(\pi\)</span>, we update the parameters <span
class="math inline">\(\theta\)</span> by <span class="math display">\[
\theta^{\text{new}} \gets \theta^{\text{old}} + \alpha \nabla_\theta
\mathbf{R}_\theta
\]</span> We calculate the gradient here, first note that <span
class="math inline">\(\nabla_{\theta}\mathbf{R}_\theta =
\nabla_{\theta}V^{\pi_\theta}(s_0)\)</span>, where <span
class="math inline">\(s\)</span> is the starting state. By policy
gradient theorem we have <span class="math display">\[
\nabla_{\theta}V^{\pi_\theta}(s_0) \propto \sum_{s \in
S}\nu^{\pi_{\theta}}(s)\sum_{a \in A} Q^{\pi_{\theta}}(s,a)\nabla_\theta
\pi_{\theta}(a~|~s). \label{pg_thm1} \tag{1}
\]</span> The proof of policy gradient theorem can be found in <a
href="#proof-of-policy-gradient-theorem">this section</a>. So we have
<span class="math display">\[
\begin{align*}
    \nabla_{\theta}V^{\pi_\theta}(s_0)
\propto&amp;~\sum_{s \in S}\nu^{\pi_{\theta}}(s)\sum_{a \in A}
Q^{\pi_{\theta}}(s,a)\nabla_\theta \pi_{\theta}(a~|~s) \\
=&amp;~ \sum_{x \in S}\nu^{\pi_{\theta}}(s)\sum_{a \in A}
\pi_{\theta}(a~|~s)Q^{\pi_{\theta}}(s,a)
\frac{\nabla_\theta\pi_{\theta}(a~|~s)}{\pi_{\theta}(a~|~s)} \\
=&amp;~ \mathbb{E}_{\pi_\theta}[Q^{\pi_{\theta}}(s,a)\nabla_\theta\log
\pi_\theta(a~|~s)]
\end{align*}
\]</span> We can use it in the update of <span
class="math inline">\(\theta\)</span>.</p>
<h2 id="reinforce">REINFORCE</h2>
<p>In policy gradient, we need to know <span
class="math inline">\(Q^{\pi_\theta}(s,a)\)</span>. There are multiple
ways of estimating it. REINFORCE is a Monte Carlo variant of a policy
gradient algorithm that estimates <span class="math inline">\(Q\)</span>
via sampling several steps. For example, in an environment with a finite
steps (<span class="math inline">\(T\)</span> steps), REINFORCE
calculates the policy gradient by <span class="math display">\[
\nabla_\theta V^{\pi_\theta} =
\mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^T\left(\sum_{t&#39;=t}^T\gamma^{t&#39;-t}r_{t&#39;}\nabla_\theta\log\pi_{\theta}(a_t~|~s_t)\right)\right].
\]</span> The algorithm runs in steps:</p>
<ol type="1">
<li>Sample trajectory <span
class="math inline">\(\{s_1,a_1,r_1,\dots,s_T,a_T,r_T\}\)</span> using
current <span class="math inline">\(\pi_\theta\)</span>;</li>
<li>calculate return <span
class="math inline">\(\hat{R}_t\gets\sum_{t&#39;=t}^T\gamma^{t&#39;-t}r_{t&#39;}\)</span>
for every time step <span class="math inline">\(r\)</span>;</li>
<li>update <span class="math inline">\(\theta\)</span> by <span
class="math inline">\(\theta_{\mathrm{new}}\gets \theta +
\alpha\sum_{t=1}^T\hat{R}_t\nabla_\theta
\log\pi(a_t~|~s_t)\)</span>.</li>
</ol>
<h2 id="deep-q-learning-dqn">Deep Q-Learning (DQN)</h2>
<p>Just like Q-Learning, instead, model the Q function using neural
network parameterized by <span class="math inline">\(\theta\)</span>,
i.e., <span class="math display">\[
Q(s,a) \approx Q_\theta(s,a)
\]</span> Recall that in Q-learning, we update Q value by <span
class="math display">\[
Q^{\text{new}}(s_k,a_k) \gets Q^{\text{old}}(s_k,a_k) + \alpha \left(r_k
+ \gamma\max_{a}Q^{\text{old}}(s_{k+1}, a)-Q^{\text{old}}(s_k,a_k)
\right)
\]</span> So the loss here is defined as <span class="math display">\[
L=\mathbb{E}\left[\left(r_k + \gamma\max_aQ_\theta(s_{k+1}, a) -
Q_\theta(s_k,a_k)\right)^2\right]
\]</span> Here are some tricks of DQN.</p>
<ol type="1">
<li><p>experience replay:</p>
<p>In normal supervised learning, the data will be used to train the
network for multiple times. However in DQN, each data is only used once.
To avoid this, we maintain a buffer to store the 4-tuple of (state,
action, reward, next state). When training, we pick batch of data from
the buffer and use it to train the network.</p></li>
<li><p>target network:</p>
<p>The target of DQN is to approximate <span class="math inline">\(r +
\gamma\max_aQ_\theta(s, a)\)</span>. Since the output of the network is
included in the TD loss, it will be unstable during the training. To
avoid this, we freeze the network by using a copy of it. Let's say
original DQN to be <span
class="math inline">\(Q_{\theta_1}(s,a)\)</span> and a new network
(target network) <span class="math inline">\(Q_{\theta_2}(s,a)\)</span>,
the loss is then calculated as <span class="math display">\[
L =\mathbb{E}\left[\left(r_k + \gamma\max_aQ_{\theta_1}(s_{k+1}, a) -
Q_{\theta_2}(s_k,a_k)\right)^2\right].
\]</span> The first network is updated using gradient descent, while the
target network is only synced with the main network every <span
class="math inline">\(C\)</span> steps.</p></li>
</ol>
<p>Another variant of Deep Q-Learning is Deep Dueling Q Network (DDQN),
in which we split Q network into two parts, the value network <span
class="math inline">\(V_{\theta_1}(s)\)</span> and the advantage network
<span class="math inline">\(A_{\theta_2}(s,a)\)</span> such that <span
class="math display">\[
Q_\theta(s,a) = V_{\theta_1}(s)+A_{\theta_2}(s,a).
\]</span> By this, the value network learns the value function and the
advantage network learns the advantage of an action could take to
current state.</p>
<h2 id="actor-critic-network">Actor-Critic Network</h2>
<p>Recall that in Policy Gradient method, we calculated the gradient as
following <span class="math display">\[
\nabla_{\theta}J_\theta
= \mathbb{E}_{\pi_\theta}[Q^{\pi_{\theta}}(s,a)\nabla_\theta\log
\pi_\theta(a~|~s)].
\]</span> Actually there is a generalized expression that <span
class="math display">\[
\nabla_{\theta}J_\theta=
\mathbb{E}_{\pi_\theta}[\psi_t\cdot\nabla_\theta\log \pi_\theta(a~|~s)],
\]</span> where <span class="math inline">\(\psi_t\)</span> can be one
of</p>
<ol type="1">
<li><span
class="math inline">\(\sum_{t&#39;=0}^T\gamma^{t&#39;}r_t&#39;\)</span>:
total return of the trajectory;</li>
<li><span
class="math inline">\(\sum_{t&#39;=t}^T\gamma^{t&#39;-t}r_{t&#39;}\)</span>:
return after action <span class="math inline">\(a_t\)</span>;</li>
<li><span class="math inline">\(Q^{\pi_\theta}(s_r, a_t)\)</span>: Q
function;</li>
<li><span class="math inline">\(A^{\pi_\theta}(s_t, a_t)\)</span>:
advantage function;</li>
<li><span class="math inline">\(r_t+\gamma V^{\pi_\theta}(s_{t+1}) -
V^{\pi_\theta}(s_t)\)</span>: temporal difference residual.</li>
</ol>
<p>In Actor-Critic setting, we have two neural networks, one is policy
network (actor) <span class="math inline">\(\pi_{\theta_1}(s,a)\)</span>
and the other is value network (critic) <span
class="math inline">\(V_{\theta_2}(s)\)</span>. The loss of the value
function is defined as <span class="math display">\[
L(\theta_2) := (r+\gamma V_{\theta_2}(s_{t+1}) - V_{\theta_2}(s_t))^2.
\]</span> Like we did in the target network in DQN, the first part <span
class="math inline">\(r+\gamma V_{\theta_2}(s_{t+1})\)</span> is TD
target and will not be calculated into gradient, we the gradient will be
<span class="math display">\[
\nabla_{\theta_2}L(\theta_2) = -2(r+\gamma V_{\theta_2}(s_{t+1}) -
V_{\theta_2}(s_t))\nabla_{\theta_2}V_{\theta_2}(s_t).
\]</span> Hence the updated rule of the two networks are <span
class="math display">\[
\begin{align*}
    \theta_1 \gets&amp;~ \theta_1 +
\alpha_{\theta_1}\sum_{t}\delta_t\nabla_{\theta_1}\log\pi_{\theta_1}(a_t~|~s_t)
\\
    \theta_2 \gets&amp;~ \theta_2 +
\alpha_{\theta_2}\sum_{t}\delta_t\nabla_{\theta_2}V_{\theta_2}(s_t),
\end{align*}
\]</span> where <span class="math inline">\(\delta_t := r_t +\gamma
V_{\theta_2}(s_{t+1}) - V_{\theta_2}(s_t)\)</span>.</p>
<h2 id="trpo-trust-region-policy-optimization-paper-link">TRPO: Trust
Region Policy Optimization [<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1502.05477">paper link</a>]</h2>
<p>All previous policy-based algorithms face a problem of instability
during training. To avoid this, we consider a trust region when updating
the parameters, with some security guarantee. Theoretically it can
guarantee the monotonicity of performance during training.</p>
<p>Given current policy <span class="math inline">\(\pi_\theta\)</span>,
we consider looking for a better parameter <span
class="math inline">\(\theta&#39;\)</span> such that <span
class="math inline">\(J(\theta&#39;) \ge J(\theta)\)</span>. Since the
distribution of starting state <span class="math inline">\(s_0\)</span>
is independent of the policy. We can use expectation under new policy
<span class="math inline">\(\pi_{\theta&#39;}\)</span> to describe
current optimization target: <span class="math display">\[
\begin{align*}
    J(\theta)
=&amp;~\mathbb{E}_{s_0}\left[V^{\pi_\theta}(s_0)\right] \\
=&amp;~\mathbb{E}_{\pi_{\theta&#39;}}\left[\sum_{t=0}^\infty\gamma^t
V^{\pi_{\theta}}(s_t) -
\sum_{t=1}^\infty\gamma^tV^{\pi_\theta}(s_t)\right] \\
=&amp;~-\mathbb{E}_{\pi_{\theta&#39;}}\left[\sum_{t=0}^\infty\gamma^t
(\gamma V^{\pi_{\theta}}(s_{t+1}) - V^{\pi_\theta}(s_t))\right]
\end{align*}
\]</span> Given this, we can calculate the difference between the two
target functions as <span class="math display">\[
\begin{align*}
    J(\theta&#39;) - J(\theta)
=&amp;~ \mathbb{E}_{s_0}\left[V^{\pi_{\theta&#39;}}(s_0)\right] -
\mathbb{E}_{s_0}\left[V^{\pi_\theta}(s_0)\right] \\
=&amp;~ \mathbb{E}_{\pi_{\theta&#39;}}\left[\sum_{t = 0}^\infty\gamma^t
r(s_t,a_t)\right] +
\mathbb{E}_{\pi_{\theta&#39;}}\left[\sum_{t=0}^\infty\gamma^t (\gamma
V^{\pi_{\theta}}(s_{t+1}) - V^{\pi_\theta}(s_t))\right] \\
=&amp;~
\mathbb{E}_{\pi_{\theta&#39;}}\left[\sum_{t=0}^\infty\gamma^t(r(s_t,a_t)
+ \gamma V^{\pi_{\theta}}(s_{t+1}) - V^{\pi_\theta}(s_t))\right]
\end{align*}
\]</span> By defining temporal difference residual as the advantage
<span class="math inline">\(A^{\pi_\theta}(s_t,a_t) := r(s_t,a_t) +
\gamma V^{\pi_{\theta}}(s_{t+1}) - V^{\pi_\theta}(s_t)\)</span>, we have
<span class="math display">\[
\begin{align*}
        J(\theta&#39;) - J(\theta)
    =&amp;~
\mathbb{E}_{\pi_{\theta&#39;}}\left[\sum_{t=0}^\infty\gamma^tA^{\pi_\theta}(s_t,a_t)\right]
\\
    =&amp;~ \sum_{t=1}^\infty \gamma^t\mathbb{E}_{s_t \sim
P_t^{\pi_{\theta&#39;}},
a_t\sim\pi_{\theta&#39;}(\cdot~|~s_t)}[A^{\pi_\theta}(s_t,a_t)] \\
    =&amp;~\frac{1}{1-\gamma}\mathbb{E}_{s \sim \nu^{\pi_{\theta&#39;}},
a\sim\pi_{\theta&#39;}(\cdot~|~s)}[A^{\pi_\theta}(s,a)]
\end{align*}
\]</span> where <span class="math inline">\(\nu^{\pi_\theta}(s)\)</span>
is the state visitation distribution, defined as <span
class="math display">\[
\nu^\pi (s):=(1-\gamma) \sum_{t=0}^\infty \gamma^tP_t^\pi(s),
\]</span> where <span class="math inline">\(P_t^\pi(s)\)</span> is the
probability that by policy <span class="math inline">\(\pi\)</span>,
agent is at state <span class="math inline">\(s\)</span> in time step
<span class="math inline">\(t\)</span>. So we are now looking for a
parameter <span class="math inline">\(\theta&#39;\)</span> with <span
class="math inline">\(\mathbb{E}_{s \sim \nu^{\pi_{\theta&#39;}},
a\sim\pi_{\theta&#39;}(\cdot~|~s)}[A^{\pi_\theta}(s,a)] \ge 0\)</span>,
then it will guarantee <span class="math inline">\(J(\theta&#39;) \ge
J(\theta)\)</span>. But it is hard or impossible to solve this directly.
So we ignore the difference between the state visitation distribution of
the two policies and use the distribution of the old policy <span
class="math inline">\(\pi_\theta\)</span>, hence we define the following
optimization target <span class="math display">\[
L_{\theta}(\theta&#39;) = J(\theta) + \frac{1}{1-\gamma}\mathbb{E}_{s
\sim \nu^{\pi_{\theta}},
a\sim\pi_{\theta&#39;}(\cdot~|~s)}[A^{\pi_\theta}(s,a)].
\]</span> For the actions, we can use importance sampling and hence we
get <span class="math display">\[
L_{\theta}(\theta&#39;) = J(\theta) + \frac{1}{1-\gamma}\mathbb{E}_{s
\sim \nu^{\pi_{\theta}},
a\sim\pi_{\theta}(\cdot~|~s)}\left[\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta}(a~|~s)}A^{\pi_\theta}(s,a)\right].
\]</span> To make the policies are close enough, we use Kullback-Leibler
Divergence constraint and hence we get the following optimization
problem <span class="math display">\[
\begin{align}
    \max_{\theta&#39;}&amp;~~L_\theta(\theta&#39;)\notag\\
     \text{s.t.}&amp;~~
\mathbb{E}_{s\sim\nu^{\pi_{\theta}}}[D_{KL}(\pi_{\theta}(\cdot~|~s),
\pi_{\theta&#39;}(\cdot~|~s))] \le \delta.\label{trpo}\tag{2}
\end{align}
\]</span> The constraint here actually defines a KL ball in the policy
space, called <strong>trust region</strong>.</p>
<h2 id="ppo-proximal-policy-optimization-paper-link">PPO: Proximal
Policy Optimization [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1707.06347">paper
link</a>]</h2>
<p>Consider the same optimization problem in TRPO <span
class="math display">\[
\begin{align*}    
\max_{\theta&#39;}&amp;~~\mathbb{E}_{s \sim \nu^{\pi_{\theta_k}},
a\sim\pi_{\theta_k}(\cdot~|~s)}\left[\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta}(a~|~s)}A^{\pi_{\theta_k}}(s,a)\right]\\     
\text{s.t.}&amp;~~
\mathbb{E}_{s\sim\nu^{\pi_{\theta_k}}}[D_{KL}(\pi_{\theta_k}(\cdot~|~s),
\pi_{\theta&#39;}(\cdot~|~s))] \le \delta.
\end{align*}
\]</span> There are some methods to solve this optimization, including
Taylor approximation, conjugate gradient method and linear search. But
in general it is still hard to solve. So PPO was introduced. There are
two types of PPO, PPO-Penalty and PPO-Clip.</p>
<h3 id="ppo-penalty">PPO-Penalty</h3>
<p>PPO-penalty use the method of Lagrange multipliers to put the
constraint of KL divergence into the optimization function, hence we
have the unconstrained optimization <span class="math display">\[
\theta\gets {\arg\max}_{\theta&#39;}\mathbb{E}_{s \sim
\nu^{\pi_{\theta_k}},
a\sim\pi_{\theta_k}(\cdot~|~s)}\left[\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta_k}(a~|~s)}A^{\pi_{\theta_k}}(s,a)
- \beta D_{KL}(\pi_{\theta_k}(\cdot~|~s),
\pi_{\theta&#39;}(\cdot~|~s))\right].
\]</span> Let <span class="math inline">\(d_k =
D_{KL}^{\nu^{\pi_{\theta_k}}}(\pi_{\theta_k},
\pi_{\theta&#39;})\)</span>, the rule of updating <span
class="math inline">\(\beta\)</span> is</p>
<ul>
<li>If <span class="math inline">\(d_k &lt; \delta/1.5\)</span>, let
<span class="math inline">\(\beta_{k+1} = \beta_k/2\)</span></li>
<li>If <span class="math inline">\(d_k &gt;1.5 \cdot\delta\)</span>, let
<span class="math inline">\(\beta_{k+1} = 2\beta_k\)</span></li>
<li>else, let <span class="math inline">\(\beta_{k+1} =
\beta_k\)</span>.</li>
</ul>
<p>where <span class="math inline">\(\delta\)</span> is a preset
hyperparameter.</p>
<h3 id="ppo-clip">PPO-Clip</h3>
<p>PPO-Clip is more straightforward, by restricting the difference of
the policies directly in the optimization function using both, i.e.,
<span class="math display">\[
\theta_{k+1}\gets {\arg\max}_{\theta&#39;}\mathbb{E}_{s \sim
\nu^{\pi_{\theta_k}},
a\sim\pi_{\theta_k}(\cdot~|~s)}\left[\min\left(\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta_k}(a~|~s)}A^{\pi_{\theta_k}}(s,a),
\mathrm{clip}\left(\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta_k}(a~|~s)},
1-\epsilon, 1+\epsilon\right)\cdot
A^{\pi_{\theta_k}}(s,a)\right)\right].
\]</span></p>
<h3 id="gae-generalized-advantage-estimation">GAE: Generalized Advantage
Estimation</h3>
<p>In order to apply TRPO or PPO, we still need to know the values of
advantage function <span
class="math inline">\(A^{\pi_{\theta_k}}(s,a)\)</span>. One way we can
approximate know the advantage is by GAE. Let <span
class="math inline">\(\delta_t = r_t + \gamma V(s_t+1) - V_t\)</span> be
the temporal difference, where <span class="math inline">\(V\)</span> is
a learned value function. Hence we can define for different steps: <span
class="math display">\[
\begin{align*}
    A_t^{(1)} = &amp;~\delta_t &amp;= -V(s_t) + r_t + \gamma V(s_{t+1})
\\
    A_t^{(2)} = &amp;~ \delta_t + \gamma\delta_{t+1} &amp;= -V(s_t) +
r_t + \gamma r_{t+1}+\gamma^2 V(s_{t+2})\\
    \vdots &amp; &amp;\vdots\\
    A_t^{(k)} = &amp;~\sum_{l=0}^{k-1}\gamma^l\delta_{t+1} &amp;=-V(s_t)
+ r_t + \gamma r_{t+1}+\gamma^2+\dots+ \gamma^kV(s_{t+k})
\end{align*}
\]</span> And we calculate the weighted average over them to get: <span
class="math display">\[
\begin{align*}
    A_t^{\mathrm{GAE}}
:=&amp;~(1-\lambda)(A_t^{(1)}+ A_t^{(2)}+A_t^{(3)}+\dots) \\
=&amp;~ (1-\lambda)(\delta_t +
\lambda(\delta_t+\lambda\delta_{t+1})+\lambda^2(\delta_t+\lambda\delta_{t+1}+\lambda^2\delta_{t+2})+\dots)
\\
=&amp;~(1-\lambda)(\delta_t(1+\lambda+\lambda^2+\dots)+\gamma\delta_{t+1}(\lambda+\lambda^2+\lambda^3+\dots)+\dots)\\
=&amp;~(1-\lambda)(\delta_t\frac{1}{1-\lambda}+\gamma\delta_{t+1}\frac{\lambda}{1-\lambda}
+ \gamma^2\delta_{t+2}\frac{\lambda^2}{1-\lambda} + \dots) \\
=&amp;~\sum_{l=0}^\infty(\gamma\lambda)^l\delta_{t+l},
\end{align*}
\]</span> where <span class="math inline">\(\lambda \in [0,1]\)</span>
is a hyperparameter. For <span class="math inline">\(\lambda=0\)</span>,
this becomes <span class="math inline">\(\delta_t\)</span>, and for
<span class="math inline">\(\lambda=1\)</span>, this becomes average
over <span class="math inline">\(A_t^{(k)}\)</span>'s.</p>
<h1 id="rl-in-language-models">RL in Language Models</h1>
<p>Some RL techniques are widely used in LLM alignment, though they are
not intentionally designed for this.</p>
<h2 id="rlhf-reinforcement-learning-with-human-feedback">RLHF:
Reinforcement Learning with Human Feedback</h2>
<p>It's hard to teach LLMs generating ``good'' text that aligns with
human preference via regular Supervised Fine-Tuning (SFT). However, we
can do that by RLHF. Typically, given a pretrained language model, there
are several steps of RLHF:</p>
<ol type="1">
<li>Gathering data by generating multiple responses from the LM, and use
human annotation to rank the responses</li>
<li>Train a reward model for predicting the human preference with future
reward responses.</li>
<li>Use RL methods to finetune the model to maximize the rewards, which
will increase the probability of generating responses human like and
decrease the probability of generating responses that human
dislike.</li>
</ol>
<h2 id="reward-modeling">Reward Modeling</h2>
<p>Given a reward model <span
class="math inline">\(R_{\theta}(q,a)\)</span> parametrized by <span
class="math inline">\(\theta\)</span> and predict the rating of the
question-answer pair <span class="math inline">\((q,a)\)</span>,
following the Bradley-Terry model, which defines the probability that a
rater prefers <span class="math inline">\(a_i\)</span> over <span
class="math inline">\(a_j\)</span> as <span class="math display">\[
\Pr[a_i \succ a_j] = \frac{\exp(R_\theta(q,a_i))}{\exp(R_\theta(q,a_i))
+ \exp(R_\theta(q,a_j))}.
\]</span> Taking the negative log-likelihood of the probability, we get
the training objective loss function for pairs <span
class="math inline">\((q,a_i)\)</span> and <span
class="math inline">\((q,a_j)\)</span> as <span class="math display">\[
L(\theta) = -\log\sigma(R_\theta(q,a_i)) - \exp(R_\theta(q,a_j)).
\]</span></p>
<h2 id="ppo-in-rlhf">PPO in RLHF</h2>
<p>In this case, we typically train the critic model (used as value
function) to be aligned with the output of the reward model. The
objective can be written as <span class="math display">\[
L(\theta_{c}) = \mathbb{E}_t[(V_{\theta_c}(s_t) - R_{\theta_r}(s_T))^2],
\]</span> where <span class="math inline">\(R_{\theta_r}\)</span> is the
pretrained reward model. And instead of using PPO-clip or penalty alone
and optimize the objective function, we update the policy by calculating
the loss of it to maximize the objective using both methods: <span
class="math display">\[
L_{\text{ppo}}(\theta) =\mathbb{E}_{s \sim \nu^{\pi_{\theta_k}},
a\sim\pi_{\theta_k}(\cdot~|~s)}\left[\min\left(\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta_k}(a~|~s)}A^{\pi_{\theta_k}}(s,a),
\mathrm{clip}\left(\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta_k}(a~|~s)},
1-\epsilon, 1+\epsilon\right)\cdot A^{\pi_{\theta_k}}(s,a)\right)\right]
- \mathbb{E}_{s \sim \nu^{\pi_{\theta_k}}}\left[\beta
D_{KL}(\pi_{\theta_k}(\cdot~|~s), \pi_{\theta&#39;}(\cdot~|~s))\right]
\]</span> Adding them together, we get the loss function to optimize in
RLHF using PPO: <span class="math display">\[
L = L(\theta_r) + L_{\text{ppo}}(\theta).
\]</span></p>
<h2 id="grpo-group-relative-policy-optimization">GRPO: Group Relative
Policy Optimization</h2>
<p><img src="/images/image-20250209201756802.png" title="Demonstration of PPO and GRPO" width="700"/></p>
<p>GRPO is proposed to avoid the use of the additional value function
approximation. Specifically, for each question <span
class="math inline">\(q\)</span>, GRPO samples a group of outputs <span
class="math inline">\(\{o_1, o_2, \dots, o_G\}\)</span> from current
policy <span class="math inline">\(\pi_{\theta_k}\)</span>, then use
them to optimize <span class="math display">\[
\begin{align*}
&amp;L_{\text{GRPO}}(\theta) = \mathop{\mathbb{E}}\limits_{q\sim{\cal
Q}, \{o_i\}_{i\in[G]}\sim\pi_{\theta_k}(\cdot~|~q)}\\
&amp;\left[\frac{1}{G}\sum_{i=1}^G\frac{1}{|o_i|}\left\{\min\left\{\frac{\pi_{\theta}(o_{i,t}~|~q,o_{1,&lt;t})}{\pi_{\theta_k}(o_{i,t}~|~q,o_{1,&lt;t})}A^{\pi_{\theta_k}}_{i,t},\mathrm{clip}\left(\frac{\pi_{\theta}(o_{i,t}~|~q,o_{1,&lt;t})}{\pi_{\theta_k}(o_{i,t}~|~q,o_{1,&lt;t})},
1-\epsilon,
1+\epsilon\right)A^{\pi_{\theta_k}}_{i,t}\right\}\right\}-\beta
D_{KL}(\pi_\theta~|~\pi_{\theta_k})\right],
\end{align*}
\]</span> where <span class="math inline">\(\epsilon\)</span> and <span
class="math inline">\(\beta\)</span> are hyper parameters. The KL
divergence here is estimated by unbiased estimator <span
class="math display">\[
D_{KL}=\frac{\pi_{\theta_k}(o_{i,t}~|~q,o_{1,&lt;t})}{\pi_{\theta}(o_{i,t}~|~q,o_{1,&lt;t})}
-
\log\frac{\pi_{\theta_k}(o_{i,t}~|~q,o_{1,&lt;t})}{\pi_{\theta}(o_{i,t}~|~q,o_{1,&lt;t})}.
\]</span> <span class="math inline">\(A^{\pi_{\theta_k}}_{i,t}\)</span>
is the advantage calculated based on relative rewards of the outputs
inside each group only. For <strong>outcome based RL</strong>, i.e., the
reward model outputs rewards based on the outcome for each of the
outcomes <span
class="math inline">\(\mathbf{r}:=\{r_1,r_2,\dots,r_G\}\)</span>, we
assign the advantage of each internal token with the normalized reward,
i.e., <span class="math display">\[
\hat{A}_{i,t} \gets
\frac{r_i-\mathrm{mean}(\mathbf{r})}{\mathrm{std}(\mathbf{r})}.
\]</span> For <strong>process based RL</strong>, i.e., the reward model
outputs rewards based on every reasoning step of each outcomes <span
class="math inline">\(\mathbf{R}
:=\{\{r_1^{\mathrm{ind}(1)},\dots,r_1^{\mathrm{ind}(k_1)}\},\dots,\{r_G^{\mathrm{ind}(1)},\dots,r_G^{\mathrm{ind}(k_G)}\}\}\)</span>,
where <span class="math inline">\(k_j\)</span> is the <span
class="math inline">\(\mathrm{ind}(j)\)</span> is the ending token index
of <span class="math inline">\(j\)</span>-th step, and <span
class="math inline">\(k_i\)</span> is the total number of steps in the
<span class="math inline">\(i\)</span>-th output. We assign the
advantage by <span class="math display">\[
\begin{align*}
\hat{r}_i^{\mathrm{ind}(j)} \gets&amp;~
\frac{r_i^{\mathrm{ind}(j)}-\mathrm{mean}(\mathbf{R})}{\mathrm{std}(\mathbf{R})}
\\
\hat{A}_{i,t} \gets&amp;~ \sum_{\mathrm{ind}(j)\ge
t}\hat{r}_i^{\mathrm{ind}(j)}.
\end{align*}
\]</span></p>
<h1 id="appendix">Appendix</h1>
<h2 id="proof-of-policy-gradient-theorem">Proof of Policy Gradient
Theorem</h2>
<p>Here we give the proof for Eq.<span
class="math inline">\(\eqref{pg_thm1}\)</span>.</p>
<p>We have <span class="math display">\[
\begin{align*}
    \nabla_{\theta}V^{\pi_\theta}(s)
=&amp;~ \nabla_\theta\left(\sum_{a \in A}\pi_\theta(a~|~s) \cdot
Q^{\pi_\theta}(s,a) \right) \\
=&amp;~ \sum_{a \in A} \left(\nabla_\theta
\pi_\theta(a~|~s)Q^{\pi_\theta}(s,a) + \pi_\theta(a~|~s)\nabla_\theta
Q^{\pi_\theta}(s,a) \right)\\
=&amp;~ \sum_{a \in A} \left(\nabla_\theta
\pi_\theta(a~|~s)Q^{\pi_\theta}(s,a) + \pi_\theta(a~|~s)\nabla_\theta
\sum_{s&#39;,r}\Pr[s&#39;,r~|~s,a](r+\gamma V^{\pi_\theta}(s&#39;))
\right)\\
=&amp;~ \sum_{a \in A} \left(\nabla_\theta
\pi_\theta(a~|~s)Q^{\pi_\theta}(s,a) + \gamma
\pi_\theta(a~|~s)\sum_{s&#39;,r}\Pr[s&#39;,r~|~s,a]\gamma \nabla_\theta
V^{\pi_\theta}(s&#39;) \right)\\
=&amp;~ \sum_{a \in A} \left(\nabla_\theta
\pi_\theta(a~|~s)Q^{\pi_\theta}(s,a) + \gamma
\pi_\theta(a~|~s)\sum_{s&#39;}\Pr[s&#39;~|~s,a]\gamma \nabla_\theta
V^{\pi_\theta}(s&#39;) \right)\\
\end{align*}
\]</span> Define <span class="math inline">\(\phi(s) := \sum_{a \in
A}\nabla_\theta\pi_\theta(a~|~s)Q^{\pi_\theta}(s,a)\)</span>. Define
<span class="math inline">\(p^{\pi_\theta}(s \rightarrow x, k)\)</span>
to be the the probability of policy <span
class="math inline">\(\pi\)</span> starting from state <span
class="math inline">\(s\)</span> to arrive at state <span
class="math inline">\(x\)</span> after <span
class="math inline">\(k\)</span> steps. Hence we have <span
class="math display">\[
\begin{align*}
    \nabla_{\theta}V^{\pi_\theta}(s)
=&amp;~ \phi(s)+\gamma\sum_{a\in
A}\pi_\theta(a~|~s)\sum_{s&#39;}\Pr[s&#39;~|~s,a]\gamma \nabla_\theta
V^{\pi_\theta}(s&#39;) \\
=&amp;~ \phi(s) + \gamma \sum_{a \in A}\sum_{s&#39;}
\pi_\theta(a~|~s)\Pr[s&#39;|s,a]\nabla_\theta V^{\pi_\theta}(s&#39;) \\
=&amp;~ \phi(s) + \gamma \sum_{s&#39;}p^{\pi_\theta}(s \rightarrow
s&#39;, 1)\nabla_\theta V^{\pi_\theta}(s&#39;) \\
=&amp;~ \phi(s) + \gamma \sum_{s&#39;}p^{\pi_\theta}(s \rightarrow
s&#39;, 1)\left(\phi(s&#39;) + \gamma\sum_{s&#39;&#39;}p^{\pi_\theta}(s
\rightarrow s&#39;&#39;, 2)\nabla_\theta
V^{\pi_\theta}(s&#39;&#39;)\right) \\
=&amp;~ \phi(s) + \gamma \sum_{s&#39;}p^{\pi_\theta}(s \rightarrow
s&#39;, 1)\phi(s&#39;) + \gamma^2 \sum_{s&#39;&#39;}p^{\pi_\theta}(s
\rightarrow s&#39;&#39;, 2)\nabla_\theta V^{\pi_\theta}(s&#39;&#39;) \\
=&amp;~ \cdots \\
=&amp;~ \sum_{x \in S}\sum_{k=0}^\infty \gamma^k p^{\pi_\theta}(s
\rightarrow x, k)\phi(x).
\end{align*}
\]</span> Define <span class="math inline">\(\eta(s) = \sum_{k=0}^\infty
\gamma^k p^{\pi_\theta}(s \rightarrow x, k)\)</span>. We get <span
class="math display">\[
\begin{align*}
\nabla_\theta V^{\pi_\theta}(s)
=&amp;~ \sum_{x \in S}\eta(x)\phi(x) \\
=&amp;~ \underbrace{\sum_{x \in S}\eta(x)}_{\text{Constant}} \cdot
\sum_{x \in S}\frac{\eta(x)}{\sum_{x \in S}\eta(x)}\phi(x) \\
\propto&amp;~ \sum_{x \in S}\frac{\eta(x)}{\sum_{x \in S}\eta(x)}\phi(x)
\\
=&amp;~ \sum_{x \in S} \nu^{\pi_\theta}(s) \sum_{a \in
A}Q^{\pi_\theta}(s,a)\nabla_\theta\pi_\theta(a~|~s),
\end{align*}
\]</span> where <span class="math inline">\(\nu^{\pi_\theta}(s)\)</span>
is the state visitation distribution, defined as <span
class="math display">\[
\nu^\pi (s):=(1-\gamma) \sum_{t=0}^\infty \gamma^tP_t^\pi(s),
\]</span> where <span class="math inline">\(P_t^\pi(s)\)</span> is the
probability that by policy <span class="math inline">\(\pi\)</span>,
agent is at state <span class="math inline">\(s\)</span> in time step
<span class="math inline">\(t\)</span>.</p>

    </div>

    
    
    
        

  <div class="followme">
    <p>Welcome to my other publishing channels</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://twitter.com/SGEthan">
            <span class="icon">
              <i class="fab fa-twitter"></i>
            </span>

            <span class="label">Twitter</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Research/" rel="tag"><i class="fa fa-tag"></i>Research</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/02/16/%E5%AD%A6%E6%9C%9F%E6%80%BB%E7%BB%93/" rel="prev" title="学期总结">
      <i class="fa fa-chevron-left"></i> 学期总结
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#rl-basics"><span class="nav-number">1.</span> <span class="nav-text">RL basics</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#markov-decision-process"><span class="nav-number">2.</span> <span class="nav-text">Markov Decision Process</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#value-iteration"><span class="nav-number">2.1.</span> <span class="nav-text">Value Iteration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#policy-iteration"><span class="nav-number">2.2.</span> <span class="nav-text">Policy Iteration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#use-of-q-function"><span class="nav-number">2.3.</span> <span class="nav-text">Use of Q-function</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#q-learning"><span class="nav-number">3.</span> <span class="nav-text">Q-Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#monte-carlo-learning"><span class="nav-number">3.1.</span> <span class="nav-text">Monte Carlo Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#temporal-difference-learning"><span class="nav-number">3.2.</span> <span class="nav-text">Temporal Difference Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#q-learning-1"><span class="nav-number">3.3.</span> <span class="nav-text">Q-Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sarsa-state-action-reward-state-action"><span class="nav-number">3.4.</span> <span class="nav-text">SARSA:
State-Action-Reward-State-Action</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#deep-rl"><span class="nav-number">4.</span> <span class="nav-text">Deep RL</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#deep-policy-network-policy-gradient"><span class="nav-number">4.1.</span> <span class="nav-text">Deep Policy Network (Policy
Gradient)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reinforce"><span class="nav-number">4.2.</span> <span class="nav-text">REINFORCE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#deep-q-learning-dqn"><span class="nav-number">4.3.</span> <span class="nav-text">Deep Q-Learning (DQN)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#actor-critic-network"><span class="nav-number">4.4.</span> <span class="nav-text">Actor-Critic Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#trpo-trust-region-policy-optimization-paper-link"><span class="nav-number">4.5.</span> <span class="nav-text">TRPO: Trust
Region Policy Optimization [paper link]</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ppo-proximal-policy-optimization-paper-link"><span class="nav-number">4.6.</span> <span class="nav-text">PPO: Proximal
Policy Optimization [paper
link]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ppo-penalty"><span class="nav-number">4.6.1.</span> <span class="nav-text">PPO-Penalty</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ppo-clip"><span class="nav-number">4.6.2.</span> <span class="nav-text">PPO-Clip</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gae-generalized-advantage-estimation"><span class="nav-number">4.6.3.</span> <span class="nav-text">GAE: Generalized Advantage
Estimation</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#rl-in-language-models"><span class="nav-number">5.</span> <span class="nav-text">RL in Language Models</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#rlhf-reinforcement-learning-with-human-feedback"><span class="nav-number">5.1.</span> <span class="nav-text">RLHF:
Reinforcement Learning with Human Feedback</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reward-modeling"><span class="nav-number">5.2.</span> <span class="nav-text">Reward Modeling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ppo-in-rlhf"><span class="nav-number">5.3.</span> <span class="nav-text">PPO in RLHF</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#grpo-group-relative-policy-optimization"><span class="nav-number">5.4.</span> <span class="nav-text">GRPO: Group Relative
Policy Optimization</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#appendix"><span class="nav-number">6.</span> <span class="nav-text">Appendix</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#proof-of-policy-gradient-theorem"><span class="nav-number">6.1.</span> <span class="nav-text">Proof of Policy Gradient
Theorem</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ethan Deng"
      src="/uploads/avatar.jpg">
  <p class="site-author-name" itemprop="name">Ethan Deng</p>
  <div class="site-description" itemprop="description">Oops!</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/SGEthan" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;SGEthan" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:ycdeng@cs.washington.edu" title="E-Mail → mailto:ycdeng@cs.washington.edu" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.cs.washington.edu/" title="https:&#x2F;&#x2F;www.cs.washington.edu&#x2F;" rel="noopener" target="_blank">UW CSE</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ethan Deng</span>
</div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"jzHmTGtwAmilmUmPwuDeXsBz-gzGzoHsz","app_key":"eqsp2TpNEy7LJbFy6PGWTm1t","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
