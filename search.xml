<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Linux服务端程序的一次尝试</title>
    <url>/2021/08/28/Linux%E6%9C%8D%E5%8A%A1%E7%AB%AF%E7%A8%8B%E5%BA%8F%E7%9A%84%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95/</url>
    <content><![CDATA[<h1 id="服务器端程序设计文档">服务器端程序设计文档</h1>
<h2 id="系统概述">系统概述</h2>
<p>为了实现广域网五子棋连接对战，即在不同局域网下的机器互相连接，且在不考虑内网穿透等实现手段的情况下，我们需要一个服务器端，用于对战双方数据的转发，服务器端程序部署于云服务器，用于示例的服务器IP地址为<code>101.34.252.176</code>，服务提供商为<a
href="https://cloud.tencent.com/">腾讯云</a>。</p>
<h2 id="运行环境">运行环境</h2>
<p>用于示例的云服务器系统环境为<code>CentOS 7.6 64bit</code></p>
<h2 id="功能设计">功能设计</h2>
<p>在服务器端程序开启的情况下，会监听三个端口，<code>CONF_PORT</code>，<code>HOST_PORT</code>，<code>GUEST_PORT</code>。其中，<code>CONF_PORT</code>用于接收客户端程序的申请，另外两个端口用于建立连接，实现信息的转发。具体过程如下：</p>
<ol type="1">
<li>客户端向服务器端发出申请，连接到<code>CONF_PORT</code>。</li>
<li>服务器端向<code>CONF_PORT</code>发送出当前的游戏状态<code>Game_Status</code>，分为<code>VACANT</code>，<code>WAITING</code>，<code>ONGOING</code>，分别表示空闲，已有玩家等待中，以及游戏进行中。
<ul>
<li>若当前为空闲，则当前申请的客户端将与<code>HOST_PORT</code>建立连接，等待另一位玩家进入游戏，并将游戏状态<code>Game_Status</code>修改为<code>WAITING</code>。</li>
<li>若当前为等待，则当前申请的客户端将与<code>GUEST_PORT</code>建立连接，开始游戏，并将并将游戏状态<code>Game_Status</code>修改为<code>ONGOING</code>。</li>
<li>若当前为游戏中，则断开连接，并返回信息。</li>
</ul></li>
<li>当双方玩家都分别建立连接后，开始进行对战，服务器同时监听双方的信息，并不作修改地转发。</li>
<li>当一局游戏结束之后，玩家断开连接，将游戏状态<code>Game_Status</code>修改为<code>VACANT</code>，并且还原为最初监听状态。</li>
</ol>
<h2 id="关键算法设计">关键算法设计</h2>
<h3 id="socket">Socket</h3>
<p>由于需要实现与客户端的信息交互，因此使用<code>Socket</code>进行信息的传递。为了避免信息的丢失，保证消息传递的可靠，使用<code>TCP</code>协议，流式传输信息。</p>
<p>分别在不同子进程中使用<code>Socket</code>，和不同端口进行消息的接收与发送。以<code>HOST_PORT</code>为例，主要代码如下所示：</p>
<ol type="1">
<li><p>申请<code>Socket</code>（<code>fd</code>表示文件描述符，用于接收<code>socket()</code>函数的返回值）</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> socket_fd_Guest = socket(AF_INET, SOCK_STREAM, <span class="number">0</span>);<span class="comment">//Apply for a socket</span></span><br></pre></td></tr></table></figure></li>
<li><p>本地地址初始化及绑定Socket：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span>  <span class="title">servaddr_Host</span>;</span></span><br><span class="line"><span class="built_in">memset</span>(&amp;servaddr_Host, <span class="number">0</span>, <span class="keyword">sizeof</span>(servaddr_Host));</span><br><span class="line">servaddr_Host.sin_family = AF_INET;</span><br><span class="line">servaddr_Host.sin_addr.s_addr = htonl(INADDR_ANY);	<span class="comment">//Listen to all Addresses</span></span><br><span class="line">servaddr_Host.sin_port = htons(HOST_PORT);			<span class="comment">//Set listening port as default</span></span><br><span class="line">bind(socket_fd_Host, (struct sockaddr*)&amp;servaddr_Host, <span class="keyword">sizeof</span>(servaddr_Host)；<span class="comment">//Bind</span></span><br></pre></td></tr></table></figure>
<p>将Socket设置为对所有IP地址进行监听。</p></li>
<li><p>在子进程中开启对于申请的监听：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">listen(socket_fd_Host, <span class="number">10</span>);</span><br><span class="line"><span class="keyword">int</span> connect_fd_Host = accept(socket_fd_Host, (struct sockaddr*)<span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br></pre></td></tr></table></figure>
<p>新建一个描述符<code>connnect_fd_Host</code>用于接收<code>accept()</code>传递回的Socket描述符。</p></li>
<li><p>对消息的读取和发送：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">recv(connect_fd_Guest, buff_H, <span class="number">4096</span>, <span class="number">0</span>);</span><br><span class="line">send(connect_fd_Guest, buff_G, length, <span class="number">0</span>);</span><br></pre></td></tr></table></figure>
<p>其中<code>buff_H</code>和<code>buff_G</code>分别是接收和发送消息的缓冲区，为<code>char*</code>类型。</p></li>
</ol>
<h3 id="多进程">多进程</h3>
<p>为了实现对三个端口的同时监听，以及互不干扰的并行运行，选择使用<code>MultiProcess</code>实现这项功能。当服务器处于正常监听状态时，进程树如下：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">Server(<span class="number">4257</span>)─┬─Server(<span class="number">15329</span>)<span class="comment">//Listening to CONF_PORT</span></span><br><span class="line">             ├─Server(<span class="number">15330</span>)<span class="comment">//Listening to HOST_PORT</span></span><br><span class="line">             └─Server(<span class="number">15331</span>)<span class="comment">//Listening to GUEST_PORT</span></span><br></pre></td></tr></table></figure>
<p>如图，主进程<code>Server（4257）</code>有三个子进程，分别实现对于三个端口的监听。</p>
<p>当<code>CONF_PORT</code>收到来自客户端的申请时，会从<code>Game_Status</code>文件获取当前的游戏状态，并将其发送给发出申请的客户端。</p>
<p>当游戏成功建立的时候，监听<code>HOST_PORT</code>和<code>GUEST_PORT</code>的两个进程将分别<code>fork()</code>一个子进程，用于向对应端口发送信息，实现发送和监听的分离。在这种状态下，进程树如图所示：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">Server(<span class="number">4257</span>)─┬─Server(<span class="number">15329</span>)<span class="comment">//Listening to CONF_PORT</span></span><br><span class="line">             ├─Server(<span class="number">15330</span>)-Server(<span class="number">15897</span>)<span class="comment">//Listening to HOST_PORT</span></span><br><span class="line">             └─Server(<span class="number">15331</span>)-Server(<span class="number">16372</span>)<span class="comment">//Listening to GUEST_PORT</span></span><br></pre></td></tr></table></figure>
<h3 id="进程间通信">进程间通信</h3>
<p>由于程序涉及到多个进程，而不同进程之间的变量是不共享的，且不同进程之间有通信的需求，因此需要使用一些方法，实现进程之间的通信。这里主要用了三种方法：共享文件，管道通信，以及信号机制。</p>
<h4 id="共享文件">共享文件</h4>
<p>关于游戏状态<code>Game_Status</code>的存储和读取，选择使用文件，在目录下建立一个<code>Game_Status</code>文件，用于存储当前的游戏状态。服务器程序每次运行的时候，都会将其初始化为<code>VACANT</code>，以供后续进程读取和修改。</p>
<p>读取过程如代码所示：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">Game_Status_fd = open(<span class="string">&quot;Game_Status&quot;</span>, O_RDONLY);</span><br><span class="line">read(Game_Status_fd, &amp;Game_Status, <span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">close(Game_Status_fd);</span><br></pre></td></tr></table></figure>
<p>其中<code>Game_Status_fd</code>和<code>Game_Status</code>均为<code>int</code>型变量，<code>Game_Status_fd</code>用作文件描述符，<code>Game_Status</code>用于读取文件中存储的单个<code>int</code>型数据，表示不同状态，如下：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VACANT 0</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> WAITING 1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ONGOING 2</span></span><br></pre></td></tr></table></figure>
<p>当<code>HOST</code>玩家建立连接时，将会对<code>Game_Status</code>文件进行修改，修改过程如代码所示：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">Game_Status = WAITING;</span><br><span class="line">Game_Status_fd = open(<span class="string">&quot;Game_Status&quot;</span>, O_WRONLY);</span><br><span class="line">write(Game_Status_fd, &amp;Game_Status, <span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">close(Game_Status_fd);</span><br></pre></td></tr></table></figure>
<p>由此，可以实现对游戏状态的共享访存。</p>
<h4 id="管道通信">管道通信</h4>
<p>在双方都建立连接时，总共有四个进程，分别实现双方消息的接收和发送，这就需要在不同进程之间互相传递信息，才能实现消息的实时转发，这里选择使用匿名管道<code>pipe()</code>来实现此项功能。</p>
<ol type="1">
<li><p>主进程中初始化两个管道：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(pipe(fd_1) == <span class="number">-1</span>)</span><br><span class="line">&#123;</span><br><span class="line">    perror(<span class="string">&quot;pipe&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> *write_H2G = &amp;fd_1[<span class="number">1</span>];</span><br><span class="line"><span class="keyword">int</span> *read_H2G = &amp;fd_1[<span class="number">0</span>];<span class="comment">//H2G:Host to Guest</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(pipe(fd_2) == <span class="number">-1</span>)</span><br><span class="line">&#123;</span><br><span class="line">    perror(<span class="string">&quot;pipe&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> *write_G2H = &amp;fd_2[<span class="number">1</span>];</span><br><span class="line"><span class="keyword">int</span> *read_G2H = &amp;fd_2[<span class="number">0</span>];<span class="comment">//G2H:Guest to Host</span></span><br></pre></td></tr></table></figure>
<p>其中<code>fd_1</code>和<code>fd_2</code>均为<code>int[2]</code>数组，而<code>write_H2G</code>，<code>read_H2G</code>，<code>write_G2H</code>，<code>read_G2H</code>是为了后面调用方便而初始化的别名。</p></li>
<li><p>初始化管道之后，主进程分别<code>fork()</code>两个子进程用于实现信息交流，并在<code>fork()</code>结束之后关闭自己的四个读写端口：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">close(*write_H2G);</span><br><span class="line">close(*read_G2H);</span><br><span class="line">close(*write_G2H);</span><br><span class="line">close(*read_H2G);</span><br></pre></td></tr></table></figure>
<p>两个子进程分别关闭与对方对应的读写端口：</p>
<ul>
<li><p><code>Host</code>进程：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">close(*write_G2H);</span><br><span class="line">close(*read_H2G);</span><br></pre></td></tr></table></figure></li>
<li><p><code>Client</code>进程：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">close(*write_H2G);</span><br><span class="line">close(*read_G2H);</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><p>这样实现消息的实时转发（以<code>Host</code>进程为例）：</p>
<ul>
<li><p>每当Socket传来消息的时候，都将消息不作改动地传递至管道的写端：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">    n = recv(connect_fd_Host, buff_H, <span class="number">4096</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">-1</span> || n == <span class="number">0</span>)</span><br><span class="line">    &#123;<span class="comment">//when connexion shut down</span></span><br><span class="line">        kill(My_Child, SIGKILL);</span><br><span class="line">        close(connect_fd_Host);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    write(*write_H2G, buff_H, n);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每当没有新消息时，便阻塞于<code>recv()</code>函数处。</p></li>
<li><p>新开一个子进程，实现：每当管道另一端有新消息传递来，将消息不做改动地经有Socket发送出去：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">    length = read(*read_G2H, buff_G, <span class="number">4096</span>);</span><br><span class="line">    send(connect_fd_Host, buff_G, length, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ol>
<p>由此，可以实现消息的实时转发</p>
<h4 id="信号机制">信号机制</h4>
<p>通过以上几种方式，已经可以实现消息的转发，但还存在一些问题。如在程序终止时，主进程的结束不会让各个子进程也结束，以及子进程没有良好的结束和回收机制，因此会造成僵尸进程和孤儿进程的存在。这是需要解决的，我们希望在主进程结束的时候，子进程也能同步结束，以及当有某一方断开连接的时候，各个子进程能够结束，同时让主进程回到最原初的监听状态。</p>
<p>这里选择使用Linux系统的信号（<code>SIGNAL</code>）机制来解决这些问题。</p>
<ul>
<li><p>在主进程中，为了在接收外界信号的同时能够杀死所有的子进程，定义了这样的处理函数：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">handler</span><span class="params">(<span class="keyword">int</span> signum)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; <span class="number">3</span>; i++)</span><br><span class="line">        kill(pid[i], SIGUSR1);</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中<code>pid[3]</code>为<code>pid_t</code>型数组，存储了三个子进程的<code>pid</code>。</p>
<p>同时在主进程中将信号<code>SIGINT</code>和<code>SIGTERM</code>与之连接：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">signal(SIGINT, handler);</span><br><span class="line">signal(SIGTERM, handler);</span><br></pre></td></tr></table></figure>
<p>由此可以实现，主进程在收到<code>ctrl+C</code>的信号以及系统<code>kill</code>指令时，可以向所有的子进程发送一个自定义信号<code>SIGUSR1</code>。</p></li>
<li><p>在子进程中，为了在接收主进程信号的同时能够杀死对应的子进程，定义了这样的处理函数：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Be_Killed_Handler</span><span class="params">(<span class="keyword">int</span> signum)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    kill(My_Child, SIGKILL);</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中<code>My_Child</code>为<code>pid_t</code>型变量，存储了各自对应的子进程<code>pid</code>。</p>
<p>并在子进程中将信号<code>SIGUSR1</code>与之相连：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">signal(SIGUSR1, Be_Killed_Handler);</span><br></pre></td></tr></table></figure>
<p>由此可以实现，当子进程收到来自主进程的<code>SIGUSR1</code>时，可以杀死孙进程，然后自我结束。</p></li>
<li><p>在孙进程中，为了防止变成孤儿进程，定义了这样的处理函数：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Pipe_Handler</span><span class="params">(<span class="keyword">int</span> signum)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>并在子进程中将信号<code>SIGPIPE</code>与之相连：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">signal(SIGPIPE, Pipe_Handler);</span><br></pre></td></tr></table></figure>
<p>由此可以实现，在管道另一端关闭时，孙进程自己结束，而不会在对应子进程结束后成为孤儿进程。</p></li>
</ul>
<h4 id="其他">其他</h4>
<p>除了以上三种方式之外，还在主进程循环的末尾有这样的语句：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> Exit_Pid = wait(<span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> i;</span><br><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; <span class="number">3</span>; i++)</span><br><span class="line">	kill(pid[i], SIGUSR1);</span><br></pre></td></tr></table></figure>
<p>主进程会阻塞在这里，等待着第一个子进程的结束，一旦有一个子进程结束，主进程将会结束所有的子进程，然后回到循环的开头，回到等待连接的状态。</p>
<h2 id="部署与运行">部署与运行</h2>
<p>服务器端代码命名为<code>Server.c</code>，在<code>Linux</code>服务器上编译运行的指令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ gcc Server.c -o Server</span><br><span class="line">$ nohup ./Server &amp;</span><br></pre></td></tr></table></figure>
<p>这里使用<code>nohup</code>以及<code>&amp;</code>是为了让程序能够在后台运行，并且关闭终端也能持续运行。</p>
<h2 id="心得与展望">心得与展望</h2>
<p>服务端程序的编写涉及到了很多Linux操作系统以及计算机网络的的内容，涉及范围较广，对于个人能力提升较大。后期考虑添加账户系统，以及排名系统，使之作为联网游戏更加完整。</p>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Review of Reinforcement Learning</title>
    <url>/2025/02/05/RL-Algorithms-in-Language-Model-Reasoning/</url>
    <content><![CDATA[<h1 id="rl-basics">RL basics</h1>
<p>In a RL problem, we have an agent and an environment that the agent
can interact with. At each time step, the agent is given a state of the
environment <span class="math inline">\(s\)</span>, and the agent is
supposed to take an action <span class="math inline">\(a\)</span> to
interact with the environment. Eventually, the agent will get a reward
<span class="math inline">\(r\)</span>. The goal of the agent is to
learn a policy <span class="math inline">\(\pi\)</span>, defined as
<span class="math display">\[
\pi(s, a) = \Pr[a~|~s].
\]</span> which gives the probability of an action <span
class="math inline">\(a\)</span> being sampled at the given state <span
class="math inline">\(s\)</span>.</p>
<p>Given a policy <span class="math inline">\(\pi\)</span> and a
starting state <span class="math inline">\(s\)</span>, we define the
<strong>value</strong> of the state <span
class="math inline">\(s\)</span> to be the expected reward of starting
from <span class="math inline">\(s\)</span> with policy <span
class="math inline">\(\pi\)</span>, hence the <strong>value
function</strong> <span class="math inline">\(V^{\pi}(s)\)</span> is
defined as <span class="math display">\[
V^{\pi}(s) := \mathbb{E}_{\pi}[\mathbf{R}_t ~|~ s_t=s] =
\mathbb{E}_{\pi}\left[\sum_{k=0}^\infty\gamma^tr_{t+k+1} ~|~ s_t =
s\right],
\]</span> where <span class="math inline">\(\gamma \in (0,1)\)</span> is
a reward decaying parameter and <span
class="math inline">\(\mathbf{R}_t\)</span> is defined to be the
cumulative reward starting from time <span
class="math inline">\(t\)</span>. Given a policy <span
class="math inline">\(\pi\)</span>, a state <span
class="math inline">\(s\)</span> and an action <span
class="math inline">\(a\)</span>, we can define the action-value
function Q to be the expected reward of the state-action pair <span
class="math inline">\((s, a)\)</span>, defined as <span
class="math display">\[
Q^\pi(s,a) := \mathbb{E}_\pi[\mathbf{R}_t~|~s_t=s,a_t=a].
\]</span> Given the definitions, we have the following relation <span
class="math display">\[
V^\pi(s) = \sum_{a \in A}\pi(a~|~s)Q^{\pi}(s,a).
\]</span></p>
<h1 id="markov-decision-process">Markov Decision Process</h1>
<p>If the environment state is a Markov process and there is a model
describing the environment that for each triplet of a starting state
<span class="math inline">\(s\)</span>, an action <span
class="math inline">\(a\)</span> taken and a ending state <span
class="math inline">\(s&#39;\)</span>, it gives the probability in the
form of <span class="math display">\[
\Pr(s&#39;, s, a) = \Pr[s_{k+1} = s&#39;~|~ s_k = s, a_k= a].
\]</span> then it is a <strong>Markov Decision Process</strong>, or MDP.
It is a Model-based type RL problem.</p>
<p>Assume the reward is defined as <span class="math display">\[
R(s&#39;, s,a) = \mathbb{E}[r_{k+1} ~|~ s_{k+1} = s&#39;, s_k = s, a_k=
a].
\]</span> ## Bellman's Equations</p>
<p>For the value functions, we have <span class="math display">\[
\begin{align*}
    V(s)=&amp;~ \mathbb{E}_\pi \left[r_{t+1} +
\sum_{k=1}^\infty\gamma^tr_{t+k+1} ~|~ s_{t} = s\right] \\
        =&amp;~ \mathbb{E}_\pi \left[r_{t+1} + \gamma V(s&#39;)~|~ s_{t}
= s\right],
\end{align*}
\]</span> where <span class="math inline">\(s&#39;\)</span> is a random
state dominated by current state and action. Here we get a recursive
expression named <strong>Bellman's Expectation Equation</strong>.</p>
<p>Again, our goal in RL is to find a policy, which maximizes the
expected return from a starting state. Let's now define the partial
order between the policies: if for all state <span
class="math inline">\(s\)</span>, we have <span
class="math inline">\(V^\pi(s) \ge V^{\pi&#39;}(s)\)</span>, then we say
<span class="math inline">\(\pi&gt;\pi&#39;\)</span>. In MDP with finite
state set and action set, there exists at lease one policy <span
class="math inline">\(\pi^*\)</span> such that for any other policy
<span class="math inline">\(\pi&#39;\)</span>, it holds that <span
class="math inline">\(\pi^*&gt;\pi&#39;\)</span>. We call it the
<strong>optimal policy</strong>. With this, we can define the optimal
value function <span class="math display">\[
V^*(s) := \max_{\pi}V^{\pi}(s),~~\forall s \in \mathcal{S}.
\]</span> Also we can define the optimal Q function <span
class="math display">\[
Q^*(s,a) = \max_{\pi}Q^{\pi}(s,a),~~\forall s \in \mathcal{S}, a \in
\mathcal{A}.
\]</span> To connect them, we have <span class="math display">\[
\begin{align*}
Q^*(s,a) =&amp;~ r(s,a) + \gamma
\sum_{s&#39;\in\cal{S}}\Pr[s&#39;~|~s,a]V^*(s&#39;) \\
V^*(s) =&amp;~ \max_{a \in \cal{A}}Q^*(s,a).
\end{align*}
\]</span> Hence we will have the following <strong>Bellman's Optimality
Equation</strong>: <span class="math display">\[
\begin{align*}
    V^*(s)  =&amp;~ \max_{a\in\cal{A}} \left\{r(s,a) +
\gamma\sum_{s&#39; \in \cal{S}}\Pr[s&#39;~|~a,a]V^*(s&#39;)\right\} \\
    Q^*(s,a)=&amp;~ r(s,a) +
\gamma\sum_{s&#39;\in\cal{S}}\Pr[s&#39;~|~s,a]\max_{a&#39; \in
\cal{A}}Q^*(s&#39;,s&#39;).
\end{align*}
\]</span></p>
<h2 id="value-iteration">Value Iteration</h2>
<p>The goal of value iteration is to find the optimal value function
<span class="math inline">\(V^*\)</span>. Starting from random (or
all-zero) initialization, given Bellman's Optimality Equation, for every
round, we update the value function by <span class="math display">\[
\begin{align*}
    V^{k+1}(s) \gets &amp;~ \max_{a}\sum_{s&#39;}\Pr[s&#39;~|~s,
a]\left(r(s,a) + \gamma V^{k}(s&#39;)\right) \\
        =&amp;~\max_{a}Q^k(s,a)
\end{align*}
\]</span> where <span class="math inline">\(\Pr(s&#39;, s, a)\)</span>
and <span class="math inline">\(r(s,a)\)</span> are assumed known, and
we use $ V^k$ and <span class="math inline">\(Q^k\)</span> to represent
the intermediate <span class="math inline">\(V\)</span> and <span
class="math inline">\(Q\)</span> for approximating during the algorithm.
We start by initiating <span class="math inline">\(\hat{V}\)</span>'s
for every <span class="math inline">\(s\)</span> to be <span
class="math inline">\(0\)</span> or random. Then starting from a random
state <span class="math inline">\(s_0\)</span> and update the value by
picking the action <span class="math inline">\(a\)</span> that maximizes
the value. Then we get to the next state. By iteratively doing this, we
are able to finally construct good estimate to <span
class="math inline">\(\hat{V}\)</span>'s. When the values converges, we
get the optimal policy <span class="math inline">\(\pi^*\)</span> by
<span class="math display">\[
\begin{align*}
    \pi^*(s) =&amp;~ {\arg\max}_{a}\sum_{s&#39;}\Pr[s&#39;~|~s,
a]\left(r(s,a) + \gamma V^*(s&#39;)\right) \\
        =&amp;~{\arg\max}_{a}Q^*(s,a)
\end{align*}
\]</span> It's a type of dynamic programming.</p>
<h2 id="policy-iteration">Policy Iteration</h2>
<p>Here we start by locking the policy <span
class="math inline">\(\pi\)</span>, and instead of updating optimal
values by taking the best action like we did in value iteration, we
update the value function with policy <span
class="math inline">\(\pi\)</span> by picking actions using <span
class="math inline">\(\pi\)</span>. Like we did in the Bellman's
Equation for optimal value functions <span
class="math inline">\(V^*\)</span>'s, we can get the same for <span
class="math inline">\(V^\pi\)</span>: <span class="math display">\[
\begin{align*}
    V^\pi(s)=&amp;~ \mathbb{E}_\pi[R(s&#39;,s,\pi(s)) + \gamma
V^\pi(s&#39;)] \\
        =&amp;~\sum_{s&#39;}\Pr[s&#39;~|~s, \pi(s)] \left(r(s,\pi(s)) +
\gamma V^\pi(s&#39;)\right)\\
        =&amp;~ Q(s,\pi(s))
\end{align*}
\]</span> And we update policy by <span class="math display">\[
\pi^{k+1}(s) = {\arg\max}_a \mathbb{E}[r(s,a) + \gamma
V^{\pi_{k}}(s&#39;)] = {\arg\max}_aQ^{\pi_k}(s,a)
\]</span> Then we lock the policy and update the value functions. By
iteratively doing this, we are able to get converged results.</p>
<h2 id="use-of-q-function">Use of Q-function</h2>
<p>By using Q function, the value iteration and the policy iteration can
be summarized by simply <span class="math display">\[
\begin{align*}
    V(s) =&amp;~ \max_aQ(s,a) \\
    \text{and}~~\pi(s) =&amp;~ {\arg\max}_aQ(s,a).
\end{align*}
\]</span> The good thing here is that Q function naturally encodes the
information about the future so we can directly maintain its value
instead of calculating value and pi each time.</p>
<h1 id="q-learning">Q-Learning</h1>
<p>Q-learning is used when we don't have access of the model of the
environment (model-free RL).</p>
<h2 id="monte-carlo-learning">Monte Carlo Learning</h2>
<p>We start from Monte Carlo Learning, which is the naive way (pure
trial and error) of model-free RL. By the trials of the game, we have
the cumulative reward over an episode <span class="math display">\[
\mathbf{R}_t := \sum_{k = 0}^{n}\gamma^{t}r_{t+k+1}.
\]</span> Then update the value functions or Q functions by <span
class="math display">\[
\begin{align*}
    V^{\text{new}}(s_k) &amp;~\gets
V^{\text{old}}(s_k)+\frac{1}{n}\left(\mathbf{R}_k -
V^{\text{old}}(s_k)\right)~~\forall k \in [n] \\
    Q^{\text{new}}(s_k, a_k) &amp;~\gets
Q^{\text{old}}(s_k,a_k)+\frac{1}{n}\left(\mathbf{R}_k -
Q^{\text{old}}(s_k,a_k)\right)~~\forall k \in [n]
\end{align*}
\]</span> Mathematically this is not biased. So this in theory will
finally converges. But it is relatively inefficient.</p>
<h2 id="temporal-difference-learning">Temporal Difference Learning</h2>
<p>TD learning integrates the information of time intervals, which can
be connected with biological learning or some topics in neuroscience. We
start with TD(0) learning. Recall the Bellman's Equation <span
class="math display">\[
V(s_k) = \mathbb{E}[r_k + \gamma V(s_{k+1})]
\]</span> Then in TD(0), we update the value function by <span
class="math display">\[
\begin{align*}
    V^{\text{new}}(s_k) &amp;~\gets
V^{\text{old}}(s_k)+\alpha\left(\overbrace{\underbrace{r_k + \gamma
V^{\text{old}}(s_{k+1})}_{\text{TD target estimate }\mathbf{R_k}} -
V^{\text{old}}(s_k)}^{\text{TD Error}}\right) ~~\forall k \in [n]
\end{align*}
\]</span> For TD(N), we expand the value function as <span
class="math display">\[
\begin{align*}
    V(s_k) =&amp;~ \mathbb{E}[r_k + \gamma V(s_{k+1})] \\
        =&amp;~ \mathbb{E} [r_k + \gamma r_{k+1} + \gamma^2V(s_{k+2})]\\
        =&amp;~ \mathbb{E} \left[\sum_{j=0}^N \gamma^j r_{k+j} +
\gamma^{N+1}V(s_{k+N+1})\right]
\end{align*}
\]</span> where we define <span class="math inline">\(\mathbf{R}^{(N)}
:= \sum_{j=0}^N \gamma^j r_{k+j} + \gamma^{N+1}V(s_{k+N+1})\)</span>.
And hence the value function is updated by <span class="math display">\[
\begin{align*}    
    V^{\text{new}}(s_k) &amp;~\gets
V^{\text{old}}(s_k)+\alpha\left(\overbrace{\underbrace{\sum_{j=0}^N
\gamma^j r_{k+j} + \gamma^{N+1}V^{\text{old}}(s_{k+N+1})}_{\text{TD
target estimate }\mathbf{R_t}} - V^{\text{old}}(s_k)}^{\text{TD
Error}}\right) ~~\forall k \in [n].
\end{align*}
\]</span> By letting <span class="math inline">\(N\)</span> to the
episode size or infinity, this converges to Monte Carlo learning.</p>
<p>There is another type of TD learning algorithms named TD-<span
class="math inline">\(\lambda\)</span>, which calculates cumulative
rewards from TD(0) to TD(N) and taking their weighted some in the
following way <span class="math display">\[
\mathbf{R}^{\lambda} := (1-\lambda)\sum_{n=1}^\infty
\lambda^{n-1}\mathbf{R}^{(n)}.
\]</span></p>
<h2 id="q-learning-1">Q-Learning</h2>
<p>Q-Learning is temporal learning on Q functions, where in its TD(0)
version, the Q functions are updated by <span class="math display">\[
Q^{\text{new}}(s_k,a_k) \gets Q^{\text{old}}(s_k,a_k) + \alpha \left(r_k
+ \gamma\max_{a}Q^{\text{old}}(s_{k+1}, a)-Q^{\text{old}}(s_k,a_k)
\right)
\]</span> Note that here we do not need necessarily to take the optimal
action a to get to the next state, so this TD(0) Q-learning is
<strong>off policy</strong>. In Q-learning, every time when the agent is
about to take an action, a widely used way is by <span
class="math inline">\(\epsilon\)</span>-greedy policy, which means that
it has a probability <span
class="math inline">\(1-\epsilon\in(0,1)\)</span> to take the best
action, and <span class="math inline">\(\epsilon\)</span> to take a
random action from all the action space.</p>
<h2 id="sarsa-state-action-reward-state-action">SARSA:
State-Action-Reward-State-Action</h2>
<p>SARSA is a variant of Q learning which is an
<strong>on-policy</strong> algorithm, by updating Q functions in the
following way <span class="math display">\[
Q^{\text{new}}(s_k,a_k) \gets Q^{\text{old}}(s_k,a_k) + \alpha \left(r_k
+ \gamma {\color{red} Q^{\text{old}}(s_{k+1},
a_{k+1})}-Q^{\text{old}}(s_k,a_k) \right)
\]</span> SARSA also works with TD(N) for any <span
class="math inline">\(N &gt; 0\)</span>.</p>
<h1 id="deep-rl">Deep RL</h1>
<h2 id="deep-policy-network-policy-gradient">Deep Policy Network (Policy
Gradient)</h2>
<p>By introducing the neural network into the policy function, we use a
network to approximate the policy, so a policy function parameterized by
<span class="math inline">\(\theta\)</span> can be written as <span
class="math display">\[
\pi_{\theta}(s,a).
\]</span> Then to update the policy <span
class="math inline">\(\pi\)</span>, we update the parameters <span
class="math inline">\(\theta\)</span> by <span class="math display">\[
\theta^{\text{new}} \gets \theta^{\text{old}} + \alpha \nabla_\theta
\mathbf{R}_\theta
\]</span> We calculate the gradient here, first note that <span
class="math inline">\(\nabla_{\theta}\mathbf{R}_\theta =
\nabla_{\theta}V^{\pi_\theta}(s_0)\)</span>, where <span
class="math inline">\(s\)</span> is the starting state. By policy
gradient theorem we have <span class="math display">\[
\nabla_{\theta}V^{\pi_\theta}(s_0) \propto \sum_{s \in
S}\nu^{\pi_{\theta}}(s)\sum_{a \in A} Q^{\pi_{\theta}}(s,a)\nabla_\theta
\pi_{\theta}(a~|~s). \label{pg_thm1} \tag{1}
\]</span> The proof of policy gradient theorem can be found in <a
href="#proof-of-policy-gradient-theorem">this section</a>. So we have
<span class="math display">\[
\begin{align*}
    \nabla_{\theta}V^{\pi_\theta}(s_0)
\propto&amp;~\sum_{s \in S}\nu^{\pi_{\theta}}(s)\sum_{a \in A}
Q^{\pi_{\theta}}(s,a)\nabla_\theta \pi_{\theta}(a~|~s) \\
=&amp;~ \sum_{x \in S}\nu^{\pi_{\theta}}(s)\sum_{a \in A}
\pi_{\theta}(a~|~s)Q^{\pi_{\theta}}(s,a)
\frac{\nabla_\theta\pi_{\theta}(a~|~s)}{\pi_{\theta}(a~|~s)} \\
=&amp;~ \mathbb{E}_{\pi_\theta}[Q^{\pi_{\theta}}(s,a)\nabla_\theta\log
\pi_\theta(a~|~s)]
\end{align*}
\]</span> We can use it in the update of <span
class="math inline">\(\theta\)</span>.</p>
<h2 id="reinforce">REINFORCE</h2>
<p>In policy gradient, we need to know <span
class="math inline">\(Q^{\pi_\theta}(s,a)\)</span>. There are multiple
ways of estimating it. REINFORCE is a Monte Carlo variant of a policy
gradient algorithm that estimates <span class="math inline">\(Q\)</span>
via sampling several steps. For example, in an environment with a finite
steps (<span class="math inline">\(T\)</span> steps), REINFORCE
calculates the policy gradient by <span class="math display">\[
\nabla_\theta V^{\pi_\theta} =
\mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^T\left(\sum_{t&#39;=t}^T\gamma^{t&#39;-t}r_{t&#39;}\nabla_\theta\log\pi_{\theta}(a_t~|~s_t)\right)\right].
\]</span> The algorithm runs in steps:</p>
<ol type="1">
<li>Sample trajectory <span
class="math inline">\(\{s_1,a_1,r_1,\dots,s_T,a_T,r_T\}\)</span> using
current <span class="math inline">\(\pi_\theta\)</span>;</li>
<li>calculate return <span
class="math inline">\(\hat{R}_t\gets\sum_{t&#39;=t}^T\gamma^{t&#39;-t}r_{t&#39;}\)</span>
for every time step <span class="math inline">\(r\)</span>;</li>
<li>update <span class="math inline">\(\theta\)</span> by <span
class="math inline">\(\theta_{\mathrm{new}}\gets \theta +
\alpha\sum_{t=1}^T\hat{R}_t\nabla_\theta
\log\pi(a_t~|~s_t)\)</span>.</li>
</ol>
<h2 id="deep-q-learning-dqn">Deep Q-Learning (DQN)</h2>
<p>Just like Q-Learning, instead, model the Q function using neural
network parameterized by <span class="math inline">\(\theta\)</span>,
i.e., <span class="math display">\[
Q(s,a) \approx Q_\theta(s,a)
\]</span> Recall that in Q-learning, we update Q value by <span
class="math display">\[
Q^{\text{new}}(s_k,a_k) \gets Q^{\text{old}}(s_k,a_k) + \alpha \left(r_k
+ \gamma\max_{a}Q^{\text{old}}(s_{k+1}, a)-Q^{\text{old}}(s_k,a_k)
\right)
\]</span> So the loss here is defined as <span class="math display">\[
L=\mathbb{E}\left[\left(r_k + \gamma\max_aQ_\theta(s_{k+1}, a) -
Q_\theta(s_k,a_k)\right)^2\right]
\]</span> Here are some tricks of DQN.</p>
<ol type="1">
<li><p>experience replay:</p>
<p>In normal supervised learning, the data will be used to train the
network for multiple times. However in DQN, each data is only used once.
To avoid this, we maintain a buffer to store the 4-tuple of (state,
action, reward, next state). When training, we pick batch of data from
the buffer and use it to train the network.</p></li>
<li><p>target network:</p>
<p>The target of DQN is to approximate <span class="math inline">\(r +
\gamma\max_aQ_\theta(s, a)\)</span>. Since the output of the network is
included in the TD loss, it will be unstable during the training. To
avoid this, we freeze the network by using a copy of it. Let's say
original DQN to be <span
class="math inline">\(Q_{\theta_1}(s,a)\)</span> and a new network
(target network) <span class="math inline">\(Q_{\theta_2}(s,a)\)</span>,
the loss is then calculated as <span class="math display">\[
L =\mathbb{E}\left[\left(r_k + \gamma\max_aQ_{\theta_1}(s_{k+1}, a) -
Q_{\theta_2}(s_k,a_k)\right)^2\right].
\]</span> The first network is updated using gradient descent, while the
target network is only synced with the main network every <span
class="math inline">\(C\)</span> steps.</p></li>
</ol>
<p>Another variant of Deep Q-Learning is Deep Dueling Q Network (DDQN),
in which we split Q network into two parts, the value network <span
class="math inline">\(V_{\theta_1}(s)\)</span> and the advantage network
<span class="math inline">\(A_{\theta_2}(s,a)\)</span> such that <span
class="math display">\[
Q_\theta(s,a) = V_{\theta_1}(s)+A_{\theta_2}(s,a).
\]</span> By this, the value network learns the value function and the
advantage network learns the advantage of an action could take to
current state.</p>
<h2 id="actor-critic-network">Actor-Critic Network</h2>
<p>Recall that in Policy Gradient method, we calculated the gradient as
following <span class="math display">\[
\nabla_{\theta}J_\theta
= \mathbb{E}_{\pi_\theta}[Q^{\pi_{\theta}}(s,a)\nabla_\theta\log
\pi_\theta(a~|~s)].
\]</span> Actually there is a generalized expression that <span
class="math display">\[
\nabla_{\theta}J_\theta=
\mathbb{E}_{\pi_\theta}[\psi_t\cdot\nabla_\theta\log \pi_\theta(a~|~s)],
\]</span> where <span class="math inline">\(\psi_t\)</span> can be one
of</p>
<ol type="1">
<li><span
class="math inline">\(\sum_{t&#39;=0}^T\gamma^{t&#39;}r_t&#39;\)</span>:
total return of the trajectory;</li>
<li><span
class="math inline">\(\sum_{t&#39;=t}^T\gamma^{t&#39;-t}r_{t&#39;}\)</span>:
return after action <span class="math inline">\(a_t\)</span>;</li>
<li><span class="math inline">\(Q^{\pi_\theta}(s_r, a_t)\)</span>: Q
function;</li>
<li><span class="math inline">\(A^{\pi_\theta}(s_t, a_t)\)</span>:
advantage function;</li>
<li><span class="math inline">\(r_t+\gamma V^{\pi_\theta}(s_{t+1}) -
V^{\pi_\theta}(s_t)\)</span>: temporal difference residual.</li>
</ol>
<p>In Actor-Critic setting, we have two neural networks, one is policy
network (actor) <span class="math inline">\(\pi_{\theta_1}(s,a)\)</span>
and the other is value network (critic) <span
class="math inline">\(V_{\theta_2}(s)\)</span>. The loss of the value
function is defined as <span class="math display">\[
L(\theta_2) := (r+\gamma V_{\theta_2}(s_{t+1}) - V_{\theta_2}(s_t))^2.
\]</span> Like we did in the target network in DQN, the first part <span
class="math inline">\(r+\gamma V_{\theta_2}(s_{t+1})\)</span> is TD
target and will not be calculated into gradient, we the gradient will be
<span class="math display">\[
\nabla_{\theta_2}L(\theta_2) = -2(r+\gamma V_{\theta_2}(s_{t+1}) -
V_{\theta_2}(s_t))\nabla_{\theta_2}V_{\theta_2}(s_t).
\]</span> Hence the updated rule of the two networks are <span
class="math display">\[
\begin{align*}
    \theta_1 \gets&amp;~ \theta_1 +
\alpha_{\theta_1}\sum_{t}\delta_t\nabla_{\theta_1}\log\pi_{\theta_1}(a_t~|~s_t)
\\
    \theta_2 \gets&amp;~ \theta_2 +
\alpha_{\theta_2}\sum_{t}\delta_t\nabla_{\theta_2}V_{\theta_2}(s_t),
\end{align*}
\]</span> where <span class="math inline">\(\delta_t := r_t +\gamma
V_{\theta_2}(s_{t+1}) - V_{\theta_2}(s_t)\)</span>.</p>
<h2 id="trpo-trust-region-policy-optimization-paper-link">TRPO: Trust
Region Policy Optimization [<a
href="https://arxiv.org/pdf/1502.05477">paper link</a>]</h2>
<p>All previous policy-based algorithms face a problem of instability
during training. To avoid this, we consider a trust region when updating
the parameters, with some security guarantee. Theoretically it can
guarantee the monotonicity of performance during training.</p>
<p>Given current policy <span class="math inline">\(\pi_\theta\)</span>,
we consider looking for a better parameter <span
class="math inline">\(\theta&#39;\)</span> such that <span
class="math inline">\(J(\theta&#39;) \ge J(\theta)\)</span>. Since the
distribution of starting state <span class="math inline">\(s_0\)</span>
is independent of the policy. We can use expectation under new policy
<span class="math inline">\(\pi_{\theta&#39;}\)</span> to describe
current optimization target: <span class="math display">\[
\begin{align*}
    J(\theta)
=&amp;~\mathbb{E}_{s_0}\left[V^{\pi_\theta}(s_0)\right] \\
=&amp;~\mathbb{E}_{\pi_{\theta&#39;}}\left[\sum_{t=0}^\infty\gamma^t
V^{\pi_{\theta}}(s_t) -
\sum_{t=1}^\infty\gamma^tV^{\pi_\theta}(s_t)\right] \\
=&amp;~-\mathbb{E}_{\pi_{\theta&#39;}}\left[\sum_{t=0}^\infty\gamma^t
(\gamma V^{\pi_{\theta}}(s_{t+1}) - V^{\pi_\theta}(s_t))\right]
\end{align*}
\]</span> Given this, we can calculate the difference between the two
target functions as <span class="math display">\[
\begin{align*}
    J(\theta&#39;) - J(\theta)
=&amp;~ \mathbb{E}_{s_0}\left[V^{\pi_{\theta&#39;}}(s_0)\right] -
\mathbb{E}_{s_0}\left[V^{\pi_\theta}(s_0)\right] \\
=&amp;~ \mathbb{E}_{\pi_{\theta&#39;}}\left[\sum_{t = 0}^\infty\gamma^t
r(s_t,a_t)\right] +
\mathbb{E}_{\pi_{\theta&#39;}}\left[\sum_{t=0}^\infty\gamma^t (\gamma
V^{\pi_{\theta}}(s_{t+1}) - V^{\pi_\theta}(s_t))\right] \\
=&amp;~
\mathbb{E}_{\pi_{\theta&#39;}}\left[\sum_{t=0}^\infty\gamma^t(r(s_t,a_t)
+ \gamma V^{\pi_{\theta}}(s_{t+1}) - V^{\pi_\theta}(s_t))\right]
\end{align*}
\]</span> By defining temporal difference residual as the advantage
<span class="math inline">\(A^{\pi_\theta}(s_t,a_t) := r(s_t,a_t) +
\gamma V^{\pi_{\theta}}(s_{t+1}) - V^{\pi_\theta}(s_t)\)</span>, we have
<span class="math display">\[
\begin{align*}
        J(\theta&#39;) - J(\theta)
    =&amp;~
\mathbb{E}_{\pi_{\theta&#39;}}\left[\sum_{t=0}^\infty\gamma^tA^{\pi_\theta}(s_t,a_t)\right]
\\
    =&amp;~ \sum_{t=1}^\infty \gamma^t\mathbb{E}_{s_t \sim
P_t^{\pi_{\theta&#39;}},
a_t\sim\pi_{\theta&#39;}(\cdot~|~s_t)}[A^{\pi_\theta}(s_t,a_t)] \\
    =&amp;~\frac{1}{1-\gamma}\mathbb{E}_{s \sim \nu^{\pi_{\theta&#39;}},
a\sim\pi_{\theta&#39;}(\cdot~|~s)}[A^{\pi_\theta}(s,a)]
\end{align*}
\]</span> where <span class="math inline">\(\nu^{\pi_\theta}(s)\)</span>
is the state visitation distribution, defined as <span
class="math display">\[
\nu^\pi (s):=(1-\gamma) \sum_{t=0}^\infty \gamma^tP_t^\pi(s),
\]</span> where <span class="math inline">\(P_t^\pi(s)\)</span> is the
probability that by policy <span class="math inline">\(\pi\)</span>,
agent is at state <span class="math inline">\(s\)</span> in time step
<span class="math inline">\(t\)</span>. So we are now looking for a
parameter <span class="math inline">\(\theta&#39;\)</span> with <span
class="math inline">\(\mathbb{E}_{s \sim \nu^{\pi_{\theta&#39;}},
a\sim\pi_{\theta&#39;}(\cdot~|~s)}[A^{\pi_\theta}(s,a)] \ge 0\)</span>,
then it will guarantee <span class="math inline">\(J(\theta&#39;) \ge
J(\theta)\)</span>. But it is hard or impossible to solve this directly.
So we ignore the difference between the state visitation distribution of
the two policies and use the distribution of the old policy <span
class="math inline">\(\pi_\theta\)</span>, hence we define the following
optimization target <span class="math display">\[
L_{\theta}(\theta&#39;) = J(\theta) + \frac{1}{1-\gamma}\mathbb{E}_{s
\sim \nu^{\pi_{\theta}},
a\sim\pi_{\theta&#39;}(\cdot~|~s)}[A^{\pi_\theta}(s,a)].
\]</span> For the actions, we can use importance sampling and hence we
get <span class="math display">\[
L_{\theta}(\theta&#39;) = J(\theta) + \frac{1}{1-\gamma}\mathbb{E}_{s
\sim \nu^{\pi_{\theta}},
a\sim\pi_{\theta}(\cdot~|~s)}\left[\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta}(a~|~s)}A^{\pi_\theta}(s,a)\right].
\]</span> To make the policies are close enough, we use Kullback-Leibler
Divergence constraint and hence we get the following optimization
problem <span class="math display">\[
\begin{align}
    \max_{\theta&#39;}&amp;~~L_\theta(\theta&#39;)\notag\\
     \text{s.t.}&amp;~~
\mathbb{E}_{s\sim\nu^{\pi_{\theta}}}[D_{KL}(\pi_{\theta}(\cdot~|~s),
\pi_{\theta&#39;}(\cdot~|~s))] \le \delta.\label{trpo}\tag{2}
\end{align}
\]</span> The constraint here actually defines a KL ball in the policy
space, called <strong>trust region</strong>.</p>
<h2 id="ppo-proximal-policy-optimization-paper-link">PPO: Proximal
Policy Optimization [<a href="https://arxiv.org/pdf/1707.06347">paper
link</a>]</h2>
<p>Consider the same optimization problem in TRPO <span
class="math display">\[
\begin{align*}    
\max_{\theta&#39;}&amp;~~\mathbb{E}_{s \sim \nu^{\pi_{\theta_k}},
a\sim\pi_{\theta_k}(\cdot~|~s)}\left[\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta}(a~|~s)}A^{\pi_{\theta_k}}(s,a)\right]\\     
\text{s.t.}&amp;~~
\mathbb{E}_{s\sim\nu^{\pi_{\theta_k}}}[D_{KL}(\pi_{\theta_k}(\cdot~|~s),
\pi_{\theta&#39;}(\cdot~|~s))] \le \delta.
\end{align*}
\]</span> There are some methods to solve this optimization, including
Taylor approximation, conjugate gradient method and linear search. But
in general it is still hard to solve. So PPO was introduced. There are
two types of PPO, PPO-Penalty and PPO-Clip.</p>
<h3 id="ppo-penalty">PPO-Penalty</h3>
<p>PPO-penalty use the method of Lagrange multipliers to put the
constraint of KL divergence into the optimization function, hence we
have the unconstrained optimization <span class="math display">\[
\theta\gets {\arg\max}_{\theta&#39;}\mathbb{E}_{s \sim
\nu^{\pi_{\theta_k}},
a\sim\pi_{\theta_k}(\cdot~|~s)}\left[\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta_k}(a~|~s)}A^{\pi_{\theta_k}}(s,a)
- \beta D_{KL}(\pi_{\theta_k}(\cdot~|~s),
\pi_{\theta&#39;}(\cdot~|~s))\right].
\]</span> Let <span class="math inline">\(d_k =
D_{KL}^{\nu^{\pi_{\theta_k}}}(\pi_{\theta_k},
\pi_{\theta&#39;})\)</span>, the rule of updating <span
class="math inline">\(\beta\)</span> is</p>
<ul>
<li>If <span class="math inline">\(d_k &lt; \delta/1.5\)</span>, let
<span class="math inline">\(\beta_{k+1} = \beta_k/2\)</span></li>
<li>If <span class="math inline">\(d_k &gt;1.5 \cdot\delta\)</span>, let
<span class="math inline">\(\beta_{k+1} = 2\beta_k\)</span></li>
<li>else, let <span class="math inline">\(\beta_{k+1} =
\beta_k\)</span>.</li>
</ul>
<p>where <span class="math inline">\(\delta\)</span> is a preset
hyperparameter.</p>
<h3 id="ppo-clip">PPO-Clip</h3>
<p>PPO-Clip is more straightforward, by restricting the difference of
the policies directly in the optimization function using both, i.e.,
<span class="math display">\[
\theta_{k+1}\gets {\arg\max}_{\theta&#39;}\mathbb{E}_{s \sim
\nu^{\pi_{\theta_k}},
a\sim\pi_{\theta_k}(\cdot~|~s)}\left[\min\left(\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta_k}(a~|~s)}A^{\pi_{\theta_k}}(s,a),
\mathrm{clip}\left(\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta_k}(a~|~s)},
1-\epsilon, 1+\epsilon\right)\cdot
A^{\pi_{\theta_k}}(s,a)\right)\right].
\]</span></p>
<h3 id="gae-generalized-advantage-estimation">GAE: Generalized Advantage
Estimation</h3>
<p>In order to apply TRPO or PPO, we still need to know the values of
advantage function <span
class="math inline">\(A^{\pi_{\theta_k}}(s,a)\)</span>. One way we can
approximate know the advantage is by GAE. Let <span
class="math inline">\(\delta_t = r_t + \gamma V(s_t+1) - V_t\)</span> be
the temporal difference, where <span class="math inline">\(V\)</span> is
a learned value function. Hence we can define for different steps: <span
class="math display">\[
\begin{align*}
    A_t^{(1)} = &amp;~\delta_t &amp;= -V(s_t) + r_t + \gamma V(s_{t+1})
\\
    A_t^{(2)} = &amp;~ \delta_t + \gamma\delta_{t+1} &amp;= -V(s_t) +
r_t + \gamma r_{t+1}+\gamma^2 V(s_{t+2})\\
    \vdots &amp; &amp;\vdots\\
    A_t^{(k)} = &amp;~\sum_{l=0}^{k-1}\gamma^l\delta_{t+1} &amp;=-V(s_t)
+ r_t + \gamma r_{t+1}+\gamma^2+\dots+ \gamma^kV(s_{t+k})
\end{align*}
\]</span> And we calculate the weighted average over them to get: <span
class="math display">\[
\begin{align*}
    A_t^{\mathrm{GAE}}
:=&amp;~(1-\lambda)(A_t^{(1)}+ A_t^{(2)}+A_t^{(3)}+\dots) \\
=&amp;~ (1-\lambda)(\delta_t +
\lambda(\delta_t+\lambda\delta_{t+1})+\lambda^2(\delta_t+\lambda\delta_{t+1}+\lambda^2\delta_{t+2})+\dots)
\\
=&amp;~(1-\lambda)(\delta_t(1+\lambda+\lambda^2+\dots)+\gamma\delta_{t+1}(\lambda+\lambda^2+\lambda^3+\dots)+\dots)\\
=&amp;~(1-\lambda)(\delta_t\frac{1}{1-\lambda}+\gamma\delta_{t+1}\frac{\lambda}{1-\lambda}
+ \gamma^2\delta_{t+2}\frac{\lambda^2}{1-\lambda} + \dots) \\
=&amp;~\sum_{l=0}^\infty(\gamma\lambda)^l\delta_{t+l},
\end{align*}
\]</span> where <span class="math inline">\(\lambda \in [0,1]\)</span>
is a hyperparameter. For <span class="math inline">\(\lambda=0\)</span>,
this becomes <span class="math inline">\(\delta_t\)</span>, and for
<span class="math inline">\(\lambda=1\)</span>, this becomes average
over <span class="math inline">\(A_t^{(k)}\)</span>'s.</p>
<h1 id="rl-in-language-models">RL in Language Models</h1>
<p>Some RL techniques are widely used in LLM alignment, though they are
not intentionally designed for this.</p>
<h2 id="rlhf-reinforcement-learning-with-human-feedback">RLHF:
Reinforcement Learning with Human Feedback</h2>
<p>It's hard to teach LLMs generating ``good'' text that aligns with
human preference via regular Supervised Fine-Tuning (SFT). However, we
can do that by RLHF. Typically, given a pretrained language model, there
are several steps of RLHF:</p>
<ol type="1">
<li>Gathering data by generating multiple responses from the LM, and use
human annotation to rank the responses</li>
<li>Train a reward model for predicting the human preference with future
reward responses.</li>
<li>Use RL methods to finetune the model to maximize the rewards, which
will increase the probability of generating responses human like and
decrease the probability of generating responses that human
dislike.</li>
</ol>
<h2 id="reward-modeling">Reward Modeling</h2>
<p>Given a reward model <span
class="math inline">\(R_{\theta}(q,a)\)</span> parametrized by <span
class="math inline">\(\theta\)</span> and predict the rating of the
question-answer pair <span class="math inline">\((q,a)\)</span>,
following the Bradley-Terry model, which defines the probability that a
rater prefers <span class="math inline">\(a_i\)</span> over <span
class="math inline">\(a_j\)</span> as <span class="math display">\[
\Pr[a_i \succ a_j] = \frac{\exp(R_\theta(q,a_i))}{\exp(R_\theta(q,a_i))
+ \exp(R_\theta(q,a_j))}.
\]</span> Taking the negative log-likelihood of the probability, we get
the training objective loss function for pairs <span
class="math inline">\((q,a_i)\)</span> and <span
class="math inline">\((q,a_j)\)</span> as <span class="math display">\[
L(\theta) = -\log\sigma(R_\theta(q,a_i)) - \exp(R_\theta(q,a_j)).
\]</span></p>
<h2 id="ppo-in-rlhf">PPO in RLHF</h2>
<p>In this case, we typically train the critic model (used as value
function) to be aligned with the output of the reward model. The
objective can be written as <span class="math display">\[
L(\theta_{c}) = \mathbb{E}_t[(V_{\theta_c}(s_t) - R_{\theta_r}(s_T))^2],
\]</span> where <span class="math inline">\(R_{\theta_r}\)</span> is the
pretrained reward model. And instead of using PPO-clip or penalty alone
and optimize the objective function, we update the policy by calculating
the loss of it to maximize the objective using both methods: <span
class="math display">\[
L_{\text{ppo}}(\theta) =\mathbb{E}_{s \sim \nu^{\pi_{\theta_k}},
a\sim\pi_{\theta_k}(\cdot~|~s)}\left[\min\left(\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta_k}(a~|~s)}A^{\pi_{\theta_k}}(s,a),
\mathrm{clip}\left(\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta_k}(a~|~s)},
1-\epsilon, 1+\epsilon\right)\cdot A^{\pi_{\theta_k}}(s,a)\right)\right]
- \mathbb{E}_{s \sim \nu^{\pi_{\theta_k}}}\left[\beta
D_{KL}(\pi_{\theta_k}(\cdot~|~s), \pi_{\theta&#39;}(\cdot~|~s))\right]
\]</span> Adding them together, we get the loss function to optimize in
RLHF using PPO: <span class="math display">\[
L = L(\theta_r) + L_{\text{ppo}}(\theta).
\]</span></p>
<h2 id="grpo-group-relative-policy-optimization">GRPO: Group Relative
Policy Optimization</h2>
<p><img src="/images/image-20250209201756802.png" title="Demonstration of PPO and GRPO" width="700"/></p>
<p>GRPO is proposed to avoid the use of the additional value function
approximation. Specifically, for each question <span
class="math inline">\(q\)</span>, GRPO samples a group of outputs <span
class="math inline">\(\{o_1, o_2, \dots, o_G\}\)</span> from current
policy <span class="math inline">\(\pi_{\theta_k}\)</span>, then use
them to optimize <span class="math display">\[
\begin{align*}
&amp;L_{\text{GRPO}}(\theta) = \mathop{\mathbb{E}}\limits_{q\sim{\cal
Q}, \{o_i\}_{i\in[G]}\sim\pi_{\theta_k}(\cdot~|~q)}\\
&amp;\left[\frac{1}{G}\sum_{i=1}^G\frac{1}{|o_i|}\left\{\min\left\{\frac{\pi_{\theta}(o_{i,t}~|~q,o_{1,&lt;t})}{\pi_{\theta_k}(o_{i,t}~|~q,o_{1,&lt;t})}A^{\pi_{\theta_k}}_{i,t},\mathrm{clip}\left(\frac{\pi_{\theta}(o_{i,t}~|~q,o_{1,&lt;t})}{\pi_{\theta_k}(o_{i,t}~|~q,o_{1,&lt;t})},
1-\epsilon,
1+\epsilon\right)A^{\pi_{\theta_k}}_{i,t}\right\}\right\}-\beta
D_{KL}(\pi_\theta~|~\pi_{\theta_k})\right],
\end{align*}
\]</span> where <span class="math inline">\(\epsilon\)</span> and <span
class="math inline">\(\beta\)</span> are hyper parameters. The KL
divergence here is estimated by unbiased estimator <span
class="math display">\[
D_{KL}=\frac{\pi_{\theta_k}(o_{i,t}~|~q,o_{1,&lt;t})}{\pi_{\theta}(o_{i,t}~|~q,o_{1,&lt;t})}
-
\log\frac{\pi_{\theta_k}(o_{i,t}~|~q,o_{1,&lt;t})}{\pi_{\theta}(o_{i,t}~|~q,o_{1,&lt;t})}.
\]</span> <span class="math inline">\(A^{\pi_{\theta_k}}_{i,t}\)</span>
is the advantage calculated based on relative rewards of the outputs
inside each group only. For <strong>outcome based RL</strong>, i.e., the
reward model outputs rewards based on the outcome for each of the
outcomes <span
class="math inline">\(\mathbf{r}:=\{r_1,r_2,\dots,r_G\}\)</span>, we
assign the advantage of each internal token with the normalized reward,
i.e., <span class="math display">\[
\hat{A}_{i,t} \gets
\frac{r_i-\mathrm{mean}(\mathbf{r})}{\mathrm{std}(\mathbf{r})}.
\]</span> For <strong>process based RL</strong>, i.e., the reward model
outputs rewards based on every reasoning step of each outcomes <span
class="math inline">\(\mathbf{R}
:=\{\{r_1^{\mathrm{ind}(1)},\dots,r_1^{\mathrm{ind}(k_1)}\},\dots,\{r_G^{\mathrm{ind}(1)},\dots,r_G^{\mathrm{ind}(k_G)}\}\}\)</span>,
where <span class="math inline">\(k_j\)</span> is the <span
class="math inline">\(\mathrm{ind}(j)\)</span> is the ending token index
of <span class="math inline">\(j\)</span>-th step, and <span
class="math inline">\(k_i\)</span> is the total number of steps in the
<span class="math inline">\(i\)</span>-th output. We assign the
advantage by <span class="math display">\[
\begin{align*}
\hat{r}_i^{\mathrm{ind}(j)} \gets&amp;~
\frac{r_i^{\mathrm{ind}(j)}-\mathrm{mean}(\mathbf{R})}{\mathrm{std}(\mathbf{R})}
\\
\hat{A}_{i,t} \gets&amp;~ \sum_{\mathrm{ind}(j)\ge
t}\hat{r}_i^{\mathrm{ind}(j)}.
\end{align*}
\]</span></p>
<h1 id="appendix">Appendix</h1>
<h2 id="proof-of-policy-gradient-theorem">Proof of Policy Gradient
Theorem</h2>
<p>Here we give the proof for Eq.<span
class="math inline">\(\eqref{pg_thm1}\)</span>.</p>
<p>We have <span class="math display">\[
\begin{align*}
    \nabla_{\theta}V^{\pi_\theta}(s)
=&amp;~ \nabla_\theta\left(\sum_{a \in A}\pi_\theta(a~|~s) \cdot
Q^{\pi_\theta}(s,a) \right) \\
=&amp;~ \sum_{a \in A} \left(\nabla_\theta
\pi_\theta(a~|~s)Q^{\pi_\theta}(s,a) + \pi_\theta(a~|~s)\nabla_\theta
Q^{\pi_\theta}(s,a) \right)\\
=&amp;~ \sum_{a \in A} \left(\nabla_\theta
\pi_\theta(a~|~s)Q^{\pi_\theta}(s,a) + \pi_\theta(a~|~s)\nabla_\theta
\sum_{s&#39;,r}\Pr[s&#39;,r~|~s,a](r+\gamma V^{\pi_\theta}(s&#39;))
\right)\\
=&amp;~ \sum_{a \in A} \left(\nabla_\theta
\pi_\theta(a~|~s)Q^{\pi_\theta}(s,a) + \gamma
\pi_\theta(a~|~s)\sum_{s&#39;,r}\Pr[s&#39;,r~|~s,a]\gamma \nabla_\theta
V^{\pi_\theta}(s&#39;) \right)\\
=&amp;~ \sum_{a \in A} \left(\nabla_\theta
\pi_\theta(a~|~s)Q^{\pi_\theta}(s,a) + \gamma
\pi_\theta(a~|~s)\sum_{s&#39;}\Pr[s&#39;~|~s,a]\gamma \nabla_\theta
V^{\pi_\theta}(s&#39;) \right)\\
\end{align*}
\]</span> Define <span class="math inline">\(\phi(s) := \sum_{a \in
A}\nabla_\theta\pi_\theta(a~|~s)Q^{\pi_\theta}(s,a)\)</span>. Define
<span class="math inline">\(p^{\pi_\theta}(s \rightarrow x, k)\)</span>
to be the the probability of policy <span
class="math inline">\(\pi\)</span> starting from state <span
class="math inline">\(s\)</span> to arrive at state <span
class="math inline">\(x\)</span> after <span
class="math inline">\(k\)</span> steps. Hence we have <span
class="math display">\[
\begin{align*}
    \nabla_{\theta}V^{\pi_\theta}(s)
=&amp;~ \phi(s)+\gamma\sum_{a\in
A}\pi_\theta(a~|~s)\sum_{s&#39;}\Pr[s&#39;~|~s,a]\gamma \nabla_\theta
V^{\pi_\theta}(s&#39;) \\
=&amp;~ \phi(s) + \gamma \sum_{a \in A}\sum_{s&#39;}
\pi_\theta(a~|~s)\Pr[s&#39;|s,a]\nabla_\theta V^{\pi_\theta}(s&#39;) \\
=&amp;~ \phi(s) + \gamma \sum_{s&#39;}p^{\pi_\theta}(s \rightarrow
s&#39;, 1)\nabla_\theta V^{\pi_\theta}(s&#39;) \\
=&amp;~ \phi(s) + \gamma \sum_{s&#39;}p^{\pi_\theta}(s \rightarrow
s&#39;, 1)\left(\phi(s&#39;) + \gamma\sum_{s&#39;&#39;}p^{\pi_\theta}(s
\rightarrow s&#39;&#39;, 2)\nabla_\theta
V^{\pi_\theta}(s&#39;&#39;)\right) \\
=&amp;~ \phi(s) + \gamma \sum_{s&#39;}p^{\pi_\theta}(s \rightarrow
s&#39;, 1)\phi(s&#39;) + \gamma^2 \sum_{s&#39;&#39;}p^{\pi_\theta}(s
\rightarrow s&#39;&#39;, 2)\nabla_\theta V^{\pi_\theta}(s&#39;&#39;) \\
=&amp;~ \cdots \\
=&amp;~ \sum_{x \in S}\sum_{k=0}^\infty \gamma^k p^{\pi_\theta}(s
\rightarrow x, k)\phi(x).
\end{align*}
\]</span> Define <span class="math inline">\(\eta(s) = \sum_{k=0}^\infty
\gamma^k p^{\pi_\theta}(s \rightarrow x, k)\)</span>. We get <span
class="math display">\[
\begin{align*}
\nabla_\theta V^{\pi_\theta}(s)
=&amp;~ \sum_{x \in S}\eta(x)\phi(x) \\
=&amp;~ \underbrace{\sum_{x \in S}\eta(x)}_{\text{Constant}} \cdot
\sum_{x \in S}\frac{\eta(x)}{\sum_{x \in S}\eta(x)}\phi(x) \\
\propto&amp;~ \sum_{x \in S}\frac{\eta(x)}{\sum_{x \in S}\eta(x)}\phi(x)
\\
=&amp;~ \sum_{x \in S} \nu^{\pi_\theta}(s) \sum_{a \in
A}Q^{\pi_\theta}(s,a)\nabla_\theta\pi_\theta(a~|~s),
\end{align*}
\]</span> where <span class="math inline">\(\nu^{\pi_\theta}(s)\)</span>
is the state visitation distribution, defined as <span
class="math display">\[
\nu^\pi (s):=(1-\gamma) \sum_{t=0}^\infty \gamma^tP_t^\pi(s),
\]</span> where <span class="math inline">\(P_t^\pi(s)\)</span> is the
probability that by policy <span class="math inline">\(\pi\)</span>,
agent is at state <span class="math inline">\(s\)</span> in time step
<span class="math inline">\(t\)</span>.</p>
]]></content>
      <tags>
        <tag>Research</tag>
      </tags>
  </entry>
  <entry>
    <title>test_my_site</title>
    <url>/2021/08/28/test-my-site/</url>
    <content><![CDATA[
]]></content>
  </entry>
  <entry>
    <title>学期总结</title>
    <url>/2022/02/16/%E5%AD%A6%E6%9C%9F%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>去年的时候，抓着暑假的尾巴搭建了自己的博客，但从那之后，似乎也没有再写过博文。如果不考虑用来做测试的那两篇，这应该算是第一篇博文了。</p>
<p>转眼一个学期和寒假就这么过去，总算得空，且有闲心来写一些东西。接近半年的时间没有摸自己的博客，竟花了半小时重温了一遍hexo的搭建和使用，才开始动笔写下这篇。</p>
<p>简单算算，差不多正好六个月，不长不短的一段时间。学到了很多新的东西，思考了很多新的问题，有高光有挫败。Anyway，我仍然期望自己的博客是技术向的，酸腐的文字在这里就不必多说了。</p>
<h2 id="学习相关">学习相关</h2>
<p>先从学业相关的东西来开始吧。</p>
<h3 id="学校课程">学校课程</h3>
<p>在课程方面，这一学期是从学校课程学到了一些有用的知识的，列举如下：</p>
<ul>
<li><p>Web信息处理与应用（<a
href="http://staff.ustc.edu.cn/~tongxu/webinfo/">课程主页</a>）</p>
<p>非常有意思的一门课程，徐童老师（<a
href="http://staff.ustc.edu.cn/~tongxu/">老师的主页</a>）也是非常风趣幽默的一个人，课程涉及知识非常之广范繁杂，但老师的PPT制作良好，适合异步学习。大致涉及了web信息的搜集（爬虫），处理（分类，聚类等大数据知识），检索（网页索引，搜索引擎，多模态检索等）。毕竟是选修课，大多数知识一笔带过，但作为web信息，大数据知识的入门课程还是非常不错的。该学期总共三次实验，大致内容和总结如下：</p>
<ul>
<li>实验一：首先对所给的数据集（美国经济相关新闻数据集）进行预处理（这里涉及到<span
class="math inline">\(NLP\)</span>的工具和知识），然后在数据集上构建一个简单的搜索引擎，要求实现布尔检索和语义检索。项目地址：<a
href="https://github.com/SGEthan/Info_Retrieving">SGEthan/Info_Retrieving</a></li>
<li>实验二：知识表示学习相关，要求进行关系预测，即，在所给数据集上，给出实体
<span class="math inline">\(entity:\mathbf{e}_1\)</span> 和关系 <span
class="math inline">\(relation:\mathbf{r}\)</span> ，要求计算给出与
<span class="math inline">\(\mathbf{e_1}\)</span> 有着关系 <span
class="math inline">\(\mathbf{r}\)</span> 的实体 <span
class="math inline">\(entity:\mathbf{e_1}\)</span> 的预测。项目地址：<a
href="https://github.com/SGEthan/Relation_Prediction">SGEthan/Relation_Prediction</a></li>
<li>实验三：推荐系统的构建，要求在所给数据集的基础上对每个用户进行音乐的推荐。我们采用了<strong>基于物品的协同过滤推荐</strong>。项目地址：<a
href="https://github.com/SGEthan/Recommend">SGEthan/Recommend</a></li>
</ul></li>
<li><p>机器学习 （<a
href="https://miralab.ai/course/ml_2021fall/">课程主页</a>）</p>
<p>是我在USTC两年半以来上过的最硬核的一门课程，没有之一，被称为<strong>西区数学之巅</strong>。这门课程的数学涉及范围之广泛，难度之大，是计科和信院大多数课程所不及的，这或许是王杰老师（<a
href="https://miralab.ai/people/jie-wang/">老师的主页</a>）的个人特点，但也确实契合科大扎实的数理基础风气；课程还有一个特色，使用全英文的参考资料，作业和考试也全部要求英语作答，也是大学以来的第一次。</p>
<p>从最基础的数学部分讲起，老师用一个学期从最底层的数学向上构建了学习理论的框架，并且介绍了一些应用的理论和算法。课程涉及知识内容列举如下：</p>
<ul>
<li>线性回归 Linear Regression</li>
<li>偏差-方差分解 Bias Variance Decomposition</li>
<li>贝叶斯线性回归 Bayesian Linear Regression</li>
<li>分析基础 Basics of Analysis</li>
<li>凸优化相关 Convex Sets, Convex Functions, Convex Optimization
Problems &amp; Separation Theorems</li>
<li>次梯度方法 Subdifferentials</li>
<li>决策树 Decision Tree</li>
<li>朴素贝叶斯分类器 Naive Bayes Classifier</li>
<li>逻辑回归 Logistic Regression</li>
<li>支持向量机（以及拉格朗日对偶方法） Support Vector Machine, with
Lagrange Duality</li>
<li>神经网络与卷积神经网络 Neural Networks &amp; Convolutional Neural
Networks</li>
<li>主成分分析 Principal Component Analysis</li>
<li>强化学习 Reinforcement Learning, with Multi-armed Bandits
Problem</li>
</ul>
<p>王杰老师对于学生的要求非常高，作业题目的难度非常之大，每次作业大约需要至少20个小时来完成，考试难度比作业略小，但期中期末的全班均分都没有及格。虽然很痛苦，但不可否认，这一门课程让我在一个学期学到了非常多的知识，也锻炼了应对英文资料的能力。</p>
<p>课程的final project是一个强化学习任务，要求我们训练一个Agent，在Atari
Pong游戏中取得一定的成果，而且为了锻炼我们的能力，不允许使用任何自动求导的工具包。我们组最终选择使用Policy
Gradient方法去完成（事实上是因为一开始采用的DQN方法没有work），项目地址：<a
href="https://github.com/SGEthan/RL_PG_Atari_Pong">SGEthan/RL_PG_Atari_Pong</a>
。</p></li>
<li><p>其他课程</p>
<p>有一说一，除了以上两门课，这学期其他课程给我带来的收益可以直接忽略，这其中有：</p>
<ul>
<li>计算机网络：我校计网只能说是纯文科，靠着期中期末前的突击，学到了一些东西的，但东西属实不多，大概就是一些计网的理论知识和简单应用。个人认为计算机网络知识的学习还是得靠实践，而且确实有很多有趣的相关项目。寒假在家里折腾了：
<ul>
<li>找ISP要来了公网IP，在家里老电脑开了个服务，硬盘映射出去，当作一个NAS使用。因为电脑太老，也没有刷NAS的操作系统，仅仅是简单跑了个服务，用的是<a
href="http://iscute.cn/chfs">chfs</a>，在这里特别感谢一下项目的开发者，我遇到的bug能够及时给予邮件回馈。</li>
<li>摸了一下V2Ray，重新搭了自己的服务，虽然仍然用了别人的一把梭脚本。下学期选过信安课程之后可能会考虑自己实现一个协议试试看。</li>
</ul></li>
<li>人工智能导论：纯文科，期末考前一天学完900页的AIMA，提前交卷居然还考得不错，属实没啥用的一门课</li>
<li>模式识别导论：了解了一些传统的统计学习方法，包括概率，几何，聚类方法。学到了一些东西，也写了一些没啥用的小项目，甚至都懒得传到Github的那种。最草的是，期末考试前，把每一种算法都复习的滚瓜烂熟，结果他娘的考了一张纯文科试卷，这道压轴题可能会让我记住很久：“模式识别的一般流程是什么样的？请按照上述流程设计一个过程来进行羽毛球的筛选”，整张卷子的数学不超过小学水平，一道算法都没考，全是概念，全靠硬糊，最后成绩自然也不甚理想，奶奶的。</li>
<li>编译原理：个人对于底层技术没有丝毫兴趣，这门课全靠队友带飞，非常感谢我的队友包容我摸鱼一学期（当然也有可能是因为我ML大作业带他飞了）。</li>
<li>随机过程：计科的最后一门数理基础课，还是比较有用的，Markov过程在AI方面的应用相当之广泛，现在接触的很多Learning
Theory都不可避免的需要随机过程的知识和方法，挺后悔没有好好学，不过期末速成的结果也挺满意，这样的知识密度，感觉也不太需要一个学期的时间来学习。</li>
<li>热学：我的评价是：寄，装逼装了一个学期，期末考试提前交卷还调戏助教，结果卷面计算全崩，差点挂科。总结之：再在课程群装逼我就是啥b。</li>
</ul></li>
</ul>
<h3 id="瞎折腾">瞎折腾</h3>
<p>学期中还是折腾了一些有意思的东西的，也有一些东西记录下来：</p>
<ul>
<li><p>一开始是想弄<a
href="https://github.com/ageitgey/face_recognition">face_recognition</a>，一番折腾之后，为了使用CUDA，弄了WSL，为了使用WSL的GUI，弄了Windows
11。然而从安装好Windows
11之后到现在几个月，也没重新弄起来face_recognition。这里有一些记录：</p>
<ul>
<li><p>CUDA的安装过程痛苦不堪，以国内的网络环境，各种源站崩的崩，慢的慢，整个安装过程属实折磨，配置环境整个过程花费了近一周时间，一直在跟魔幻的网络battle，也因此折腾了各种奇奇怪怪的代理方法，以与GFW斗智斗勇。如果有机会回顾一遍的话，我会将整个折腾过程记录下来。（希望没有机会回顾了🙏）</p></li>
<li><p>不同Ubuntu版本的apt源是有区别的，如果不一致，就会导致报错：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">E: Unable to correct problems, you have held broken packages.</span><br></pre></td></tr></table></figure>
<p>不同的源名称对应如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Ubuntu 12.04 (LTS)代号为precise。</span><br><span class="line">Ubuntu 14.04 (LTS)代号为trusty。</span><br><span class="line">Ubuntu 15.04 代号为vivid。</span><br><span class="line">Ubuntu 15.10 代号为wily。</span><br><span class="line">Ubuntu 16.04 (LTS)代号为xenial。</span><br><span class="line">Ubuntu 18.04 (LTS)代号为bionic。</span><br><span class="line">Ubuntu 20.04 (LTS)代号为focal。</span><br></pre></td></tr></table></figure>
<p>参考资料：<a
href="https://blog.csdn.net/woshiheweigui/article/details/115557020">ubuntu20.04
apt 安装报 E: Unable to correct problems, you have held broken
packages._woshiheweigui的博客-CSDN博客</a></p></li>
</ul></li>
<li><p>折腾模式识别大作业的时候记录了这些东西：</p>
<ul>
<li><p>可以用布尔数组来对<code>ndarray</code>进行截取，即给定一个同维相同shape的布尔数组，将这个布尔数组作为索引，就可以得到对应布尔值为真的位置的数字构成的一维数组。实验中的例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">col = x_pca[label == <span class="number">1</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>这里的<code>label</code>是一个一维数组，仅含<code>0</code>和<code>1</code>两种元素。故<code>label == 1</code>在这里会返回一个相同大小的布尔数组，<code>1</code>元素对应索引位置的元素为<code>True</code>，反之为<code>False</code>。</p>
<p><code>x_pca</code>是一个数据集，<code>768*8</code>的<code>ndarray</code>，如此截取，可以得到<code>x_pca</code>中所有输出为<code>1</code>的所对应的样本。</p></li>
<li><p>在 <span class="math inline">\(k-Fold\)</span>
交叉验证时，我们所用到的数据是<strong>训练集中的全部数据</strong>，<strong>不包含测试集</strong></p></li>
<li><p><code>numpy.ravel()</code>和<code>numpy.flatten()</code>的区别：<code>ravel()</code>返回的是一个<strong>视图</strong>，也就意味着，对其的修改，是会影响到原矩阵的；但<code>flatten()</code>返回的是一个<strong>拷贝</strong>，也就意味着，对其的修改，不会影响到原矩阵</p></li>
</ul></li>
</ul>
<h3 id="科研相关">科研相关</h3>
<p>这部分主要是寒假的工作了，整个寒假基本都交给了读论文。上学期虽然（名义上）加入了老师的实验室，做数据隐私和安全相关工作，但是由于自己课程太多，时间太紧张（大多交给了王者荣耀），并没有花费什么精力去做这个事情。期末和国外的教授联系对接之后，一整个寒假都在读论文，主要集中在Learning
Theory，略微看起来像applied tech一些的，也都是很理论的Graph
Learning问题。刚开始读论文的时候有一些记录，后来逐渐魔幻，就摆烂了，没有再将新的概念以笔记的形式记录下来。读第一篇论文的时候记录了这样几个Notation/Theorem：</p>
<ul>
<li><p><span class="math inline">\(\tilde{O}(g(n))\)</span>：</p>
<p>stands for <span
class="math inline">\(O(g(n)\log{g(n)})\)</span>.</p></li>
<li><p>Union Bound: <span class="math display">\[
\mathbb{P}[\mathop{\cup}\limits_{i}A_i]\leq \sum_i\mathbb{P}[A_i]
\]</span></p></li>
<li><p>Hoeffding's Inequality:</p>
<p>Let <span class="math inline">\(Z_1,\dots,Z_n\)</span> be independent
bounded random variables with <span
class="math inline">\(Z_i\in[a,b]\)</span> for all <span
class="math inline">\(i\)</span>, where <span
class="math inline">\(-\infty&lt;a\le b&lt;\infty\)</span>. Then <span
class="math display">\[
\mathbb{P}\left(\frac{1}{n}\sum_{i=1}^{n}(Z_i-\mathbb{E}[Z_i])\ge
t\right)\le\exp\left(-\frac{2nt^2}{(b-a)^2}\right)
\]</span> and <span class="math display">\[
\mathbb{P}\left(\frac{1}{n}\sum_{i=1}^{n}(Z_i-\mathbb{E}[Z_i])\le
-t\right)\ge\exp\left(-\frac{2nt^2}{(b-a)^2}\right)
\]</span> for all <span class="math inline">\(t\ge 0\)</span></p></li>
</ul>
<p>现在在等着选择一个确定的方向，来正式开始自己的本科科研。希望能在未来半年内做出一些成果吧，要不然属实没有大学读了……</p>
<h2 id="学习之外">学习之外</h2>
<p>半年多以来经历了很多事情，也有很多或深刻或奇怪的思考，难以总结之，也不适合作为博文放上来，在此就摘自己的跨年总结，来替代一下吧。</p>
<h3 id="跨年总结">跨年总结</h3>
<p>（写于2021年12月底，在空间跨年的时候发出）</p>
<p>开始写的时候突然惊觉，上一次在空间写这种长篇的，回忆总结性的文字，竟是两年前的跨年夜。不知不觉，已经700多天了。</p>
<p>曾于两年前定下的目标和计划，因为自己，因为其他的一些人和一些事，总与如今的事实有所偏差。好在，偏的并不算特别多。</p>
<p>去年的跨年夜，我处在前所未有，如今也未再复现过的满足中，那是今天为止，最接近自己希冀的状态的时刻。彼时彼刻，幸福感压倒了一切，连思考都变得不再必要，也就并无自我的总结与记录。
如今如此，从那般梦境坠落之后，又经历了更多的情节故事，这中间的思考和自省，也值得被记录写下。</p>
<p>两年，18岁，19岁。</p>
<p>成年之后的一段时间，遇见的人，经历的事，所施于我的影响，犹如脱胎换骨一般。如今回望
，自己已远不是曾经那样了。</p>
<p>与一些人从远到近，与一些人从近到远，熙熙攘攘，擦肩而过。不幸地失去了一些人，也幸运地留下了，发现了一些人。相识，相熟，相爱，相别，丰富之故事，复杂之情感，交织组成了这相当魔幻的一年多。</p>
<p>由于自己对事件时间精确的记忆，每一段独特的故事，都深刻于脑中，以看起来永不消逝的形式存在着，每沉于其中，总难以自拔。</p>
<p>“溺于可忆而不可追的过去， 执于可知而不可求的梦境。”</p>
<p>有过甜蜜，以至于看到了未来的尽头，沉于其中，似乎一切原本的苦涩都消之无形；有过坠落，仿佛所有生活的支柱都离我而去，未来的可能也不再有吸引力，生命垂于岌岌可危的一念之间；也有现在的释然和平淡，即已如今如此，不妨放下一切期待和愿景，摆脱所有自我限制的桎梏，享受每一个独特的当下。</p>
<p>曾被问到，如果有机会忘掉一切，回到几年前，会愿意吗？</p>
<p>我斩钉截铁的答案是，不愿意。虽然有不满，有困难，有这样那样的挫折和遗憾，但必须要承认，这段经历是丰富的，是精彩的，是我之所以成为我的必要缘由。</p>
<p>“我最近经历的，可真有点多了”</p>
<p>“这算是成年仪式吗”</p>
<p>“如果是，那它的效果可真好”</p>
<p>时光荏苒，如今的我，已不再会如两年前一样，写下具体而细致的期望和愿景。唯愿，平安喜乐，万事胜意。</p>
<h3 id="愿景">愿景</h3>
<p>倒没什么特别的期望，就希望一切顺利吧，科研，学业，身体，情绪。</p>
<p>麻了，开头还说想让自己的博客是纯技术向来着，结果最后还是写了这些酸酸的文字。就写到这里罢。</p>
]]></content>
      <tags>
        <tag>Thoughts</tag>
      </tags>
  </entry>
</search>
