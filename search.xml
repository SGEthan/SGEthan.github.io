<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Review of Reinforcement Learning</title>
    <url>/2025/02/05/RL-Algorithms-in-Language-Model-Reasoning/</url>
    <content><![CDATA[<h1 id="rl-basics">RL basics</h1>
<p>In a RL problem, we have an agent and an environment that the agent
can interact with. At each time step, the agent is given a state of the
environment <span class="math inline">\(s\)</span>, and the agent is
supposed to take an action <span class="math inline">\(a\)</span> to
interact with the environment. Eventually, the agent will get a reward
<span class="math inline">\(r\)</span>. The goal of the agent is to
learn a policy <span class="math inline">\(\pi\)</span>, defined as
<span class="math display">\[
\pi(s, a) = \Pr[a~|~s].
\]</span> which gives the probability of an action <span
class="math inline">\(a\)</span> being sampled at the given state <span
class="math inline">\(s\)</span>.</p>
<p>Given a policy <span class="math inline">\(\pi\)</span> and a
starting state <span class="math inline">\(s\)</span>, we define the
<strong>value</strong> of the state <span
class="math inline">\(s\)</span> to be the expected reward of starting
from <span class="math inline">\(s\)</span> with policy <span
class="math inline">\(\pi\)</span>, hence the <strong>value
function</strong> <span class="math inline">\(V^{\pi}(s)\)</span> is
defined as <span class="math display">\[
V^{\pi}(s) := \mathbb{E}_{\pi}[\mathbf{R}_t ~|~ s_t=s] =
\mathbb{E}_{\pi}\left[\sum_{k=0}^\infty\gamma^tr_{t+k+1} ~|~ s_t =
s\right],
\]</span> where <span class="math inline">\(\gamma \in (0,1)\)</span> is
a reward decaying parameter and <span
class="math inline">\(\mathbf{R}_t\)</span> is defined to be the
cumulative reward starting from time <span
class="math inline">\(t\)</span>. Given a policy <span
class="math inline">\(\pi\)</span>, a state <span
class="math inline">\(s\)</span> and an action <span
class="math inline">\(a\)</span>, we can define the action-value
function Q to be the expected reward of the state-action pair <span
class="math inline">\((s, a)\)</span>, defined as <span
class="math display">\[
Q^\pi(s,a) := \mathbb{E}_\pi[\mathbf{R}_t~|~s_t=s,a_t=a].
\]</span> Given the definitions, we have the following relation <span
class="math display">\[
V^\pi(s) = \sum_{a \in A}\pi(a~|~s)Q^{\pi}(s,a).
\]</span></p>
<h1 id="markov-decision-process">Markov Decision Process</h1>
<p>If the environment state is a Markov process and there is a model
describing the environment that for each triplet of a starting state
<span class="math inline">\(s\)</span>, an action <span
class="math inline">\(a\)</span> taken and a ending state <span
class="math inline">\(s&#39;\)</span>, it gives the probability in the
form of <span class="math display">\[
\Pr(s&#39;, s, a) = \Pr[s_{k+1} = s&#39;~|~ s_k = s, a_k= a].
\]</span> then it is a <strong>Markov Decision Process</strong>, or MDP.
It is a Model-based type RL problem.</p>
<p>Assume the reward is defined as <span class="math display">\[
R(s&#39;, s,a) = \mathbb{E}[r_{k+1} ~|~ s_{k+1} = s&#39;, s_k = s, a_k=
a].
\]</span></p>
<h2 id="bellmans-equations">Bellman's Equations</h2>
<p>For the value functions, we have <span class="math display">\[
\begin{align*}
    V(s)=&amp;~ \mathbb{E}_\pi \left[r_{t+1} +
\sum_{k=1}^\infty\gamma^tr_{t+k+1} ~|~ s_{t} = s\right] \\
        =&amp;~ \mathbb{E}_\pi \left[r_{t+1} + \gamma V(s&#39;)~|~ s_{t}
= s\right],
\end{align*}
\]</span> where <span class="math inline">\(s&#39;\)</span> is a random
state dominated by current state and action. Here we get a recursive
expression named <strong>Bellman's Expectation Equation</strong>.</p>
<p>Again, our goal in RL is to find a policy, which maximizes the
expected return from a starting state. Let's now define the partial
order between the policies: if for all state <span
class="math inline">\(s\)</span>, we have <span
class="math inline">\(V^\pi(s) \ge V^{\pi&#39;}(s)\)</span>, then we say
<span class="math inline">\(\pi&gt;\pi&#39;\)</span>. In MDP with finite
state set and action set, there exists at lease one policy <span
class="math inline">\(\pi^*\)</span> such that for any other policy
<span class="math inline">\(\pi&#39;\)</span>, it holds that <span
class="math inline">\(\pi^*&gt;\pi&#39;\)</span>. We call it the
<strong>optimal policy</strong>. With this, we can define the optimal
value function <span class="math display">\[
V^*(s) := \max_{\pi}V^{\pi}(s),~~\forall s \in \mathcal{S}.
\]</span> Also we can define the optimal Q function <span
class="math display">\[
Q^*(s,a) = \max_{\pi}Q^{\pi}(s,a),~~\forall s \in \mathcal{S}, a \in
\mathcal{A}.
\]</span> To connect them, we have <span class="math display">\[
\begin{align*}
Q^*(s,a) =&amp;~ r(s,a) + \gamma
\sum_{s&#39;\in\cal{S}}\Pr[s&#39;~|~s,a]V^*(s&#39;) \\
V^*(s) =&amp;~ \max_{a \in \cal{A}}Q^*(s,a).
\end{align*}
\]</span> Hence we will have the following <strong>Bellman's Optimality
Equation</strong>: <span class="math display">\[
\begin{align*}
    V^*(s)  =&amp;~ \max_{a\in\cal{A}} \left\{r(s,a) +
\gamma\sum_{s&#39; \in \cal{S}}\Pr[s&#39;~|~a,a]V^*(s&#39;)\right\} \\
    Q^*(s,a)=&amp;~ r(s,a) +
\gamma\sum_{s&#39;\in\cal{S}}\Pr[s&#39;~|~s,a]\max_{a&#39; \in
\cal{A}}Q^*(s&#39;,s&#39;).
\end{align*}
\]</span></p>
<h2 id="value-iteration">Value Iteration</h2>
<p>The goal of value iteration is to find the optimal value function
<span class="math inline">\(V^*\)</span>. Starting from random (or
all-zero) initialization, given Bellman's Optimality Equation, for every
round, we update the value function by <span class="math display">\[
\begin{align*}
    V^{k+1}(s) \gets &amp;~ \max_{a}\sum_{s&#39;}\Pr[s&#39;~|~s,
a]\left(r(s,a) + \gamma V^{k}(s&#39;)\right) \\
        =&amp;~\max_{a}Q^k(s,a)
\end{align*}
\]</span> where <span class="math inline">\(\Pr(s&#39;, s, a)\)</span>
and <span class="math inline">\(r(s,a)\)</span> are assumed known, and
we use $ V^k$ and <span class="math inline">\(Q^k\)</span> to represent
the intermediate <span class="math inline">\(V\)</span> and <span
class="math inline">\(Q\)</span> for approximating during the algorithm.
We start by initiating <span class="math inline">\(\hat{V}\)</span>'s
for every <span class="math inline">\(s\)</span> to be <span
class="math inline">\(0\)</span> or random. Then starting from a random
state <span class="math inline">\(s_0\)</span> and update the value by
picking the action <span class="math inline">\(a\)</span> that maximizes
the value. Then we get to the next state. By iteratively doing this, we
are able to finally construct good estimate to <span
class="math inline">\(\hat{V}\)</span>'s. When the values converges, we
get the optimal policy <span class="math inline">\(\pi^*\)</span> by
<span class="math display">\[
\begin{align*}
    \pi^*(s) =&amp;~ {\arg\max}_{a}\sum_{s&#39;}\Pr[s&#39;~|~s,
a]\left(r(s,a) + \gamma V^*(s&#39;)\right) \\
        =&amp;~{\arg\max}_{a}Q^*(s,a)
\end{align*}
\]</span> It's a type of dynamic programming.</p>
<h2 id="policy-iteration">Policy Iteration</h2>
<p>Here we start by locking the policy <span
class="math inline">\(\pi\)</span>, and instead of updating optimal
values by taking the best action like we did in value iteration, we
update the value function with policy <span
class="math inline">\(\pi\)</span> by picking actions using <span
class="math inline">\(\pi\)</span>. Like we did in the Bellman's
Equation for optimal value functions <span
class="math inline">\(V^*\)</span>'s, we can get the same for <span
class="math inline">\(V^\pi\)</span>: <span class="math display">\[
\begin{align*}
    V^\pi(s)=&amp;~ \mathbb{E}_\pi[R(s&#39;,s,\pi(s)) + \gamma
V^\pi(s&#39;)] \\
        =&amp;~\sum_{s&#39;}\Pr[s&#39;~|~s, \pi(s)] \left(r(s,\pi(s)) +
\gamma V^\pi(s&#39;)\right)\\
        =&amp;~ Q(s,\pi(s))
\end{align*}
\]</span> And we update policy by <span class="math display">\[
\pi^{k+1}(s) = {\arg\max}_a \mathbb{E}[r(s,a) + \gamma
V^{\pi_{k}}(s&#39;)] = {\arg\max}_aQ^{\pi_k}(s,a)
\]</span> Then we lock the policy and update the value functions. By
iteratively doing this, we are able to get converged results.</p>
<h2 id="use-of-q-function">Use of Q-function</h2>
<p>By using Q function, the value iteration and the policy iteration can
be summarized by simply <span class="math display">\[
\begin{align*}
    V(s) =&amp;~ \max_aQ(s,a) \\
    \text{and}~~\pi(s) =&amp;~ {\arg\max}_aQ(s,a).
\end{align*}
\]</span> The good thing here is that Q function naturally encodes the
information about the future so we can directly maintain its value
instead of calculating value and pi each time.</p>
<h1 id="q-learning">Q-Learning</h1>
<p>Q-learning is used when we don't have access of the model of the
environment (model-free RL).</p>
<h2 id="monte-carlo-learning">Monte Carlo Learning</h2>
<p>We start from Monte Carlo Learning, which is the naive way (pure
trial and error) of model-free RL. By the trials of the game, we have
the cumulative reward over an episode <span class="math display">\[
\mathbf{R}_t := \sum_{k = 0}^{n}\gamma^{k}r_{t+k+1}.
\]</span> Then update the value functions or Q functions by <span
class="math display">\[
\begin{align*}
    V^{\text{new}}(s_k) &amp;~\gets
V^{\text{old}}(s_k)+\frac{1}{n}\left(\mathbf{R}_k -
V^{\text{old}}(s_k)\right)~~\forall k \in [n] \\
    Q^{\text{new}}(s_k, a_k) &amp;~\gets
Q^{\text{old}}(s_k,a_k)+\frac{1}{n}\left(\mathbf{R}_k -
Q^{\text{old}}(s_k,a_k)\right)~~\forall k \in [n]
\end{align*}
\]</span> Mathematically this is not biased. So this in theory will
finally converges. But it is relatively inefficient.</p>
<h2 id="temporal-difference-learning">Temporal Difference Learning</h2>
<p>TD learning integrates the information of time intervals, which can
be connected with biological learning or some topics in neuroscience. We
start with TD(0) learning. Recall the Bellman's Equation <span
class="math display">\[
V(s_k) = \mathbb{E}[r_k + \gamma V(s_{k+1})]
\]</span> Then in TD(0), we update the value function by <span
class="math display">\[
\begin{align*}
    V^{\text{new}}(s_k) &amp;~\gets
V^{\text{old}}(s_k)+\alpha\left(\overbrace{\underbrace{r_k + \gamma
V^{\text{old}}(s_{k+1})}_{\text{TD target estimate }\mathbf{R_k}} -
V^{\text{old}}(s_k)}^{\text{TD Error}}\right) ~~\forall k \in [n]
\end{align*}
\]</span> For TD(N), we expand the value function as <span
class="math display">\[
\begin{align*}
    V(s_k) =&amp;~ \mathbb{E}[r_k + \gamma V(s_{k+1})] \\
        =&amp;~ \mathbb{E} [r_k + \gamma r_{k+1} + \gamma^2V(s_{k+2})]\\
        =&amp;~ \mathbb{E} \left[\sum_{j=0}^N \gamma^j r_{k+j} +
\gamma^{N+1}V(s_{k+N+1})\right]
\end{align*}
\]</span> where we define <span class="math inline">\(\mathbf{R}^{(N)}
:= \sum_{j=0}^N \gamma^j r_{k+j} + \gamma^{N+1}V(s_{k+N+1})\)</span>.
And hence the value function is updated by <span class="math display">\[
\begin{align*}    
    V^{\text{new}}(s_k) &amp;~\gets
V^{\text{old}}(s_k)+\alpha\left(\overbrace{\underbrace{\sum_{j=0}^N
\gamma^j r_{k+j} + \gamma^{N+1}V^{\text{old}}(s_{k+N+1})}_{\text{TD
target estimate }\mathbf{R_t}} - V^{\text{old}}(s_k)}^{\text{TD
Error}}\right) ~~\forall k \in [n].
\end{align*}
\]</span> By letting <span class="math inline">\(N\)</span> to the
episode size or infinity, this converges to Monte Carlo learning.</p>
<p>There is another type of TD learning algorithms named TD-<span
class="math inline">\(\lambda\)</span>, which calculates cumulative
rewards from TD(0) to TD(N) and taking their weighted some in the
following way <span class="math display">\[
\mathbf{R}^{\lambda} := (1-\lambda)\sum_{n=1}^\infty
\lambda^{n-1}\mathbf{R}^{(n)}.
\]</span></p>
<h2 id="q-learning-1">Q-Learning</h2>
<p>Q-Learning is temporal learning on Q functions, where in its TD(0)
version, the Q functions are updated by <span class="math display">\[
Q^{\text{new}}(s_k,a_k) \gets Q^{\text{old}}(s_k,a_k) + \alpha \left(r_k
+ \gamma\max_{a}Q^{\text{old}}(s_{k+1}, a)-Q^{\text{old}}(s_k,a_k)
\right)
\]</span> Note that here we do not need necessarily to take the optimal
action a to get to the next state, so this TD(0) Q-learning is
<strong>off policy</strong>. In Q-learning, every time when the agent is
about to take an action, a widely used way is by <span
class="math inline">\(\epsilon\)</span>-greedy policy, which means that
it has a probability <span
class="math inline">\(1-\epsilon\in(0,1)\)</span> to take the best
action, and <span class="math inline">\(\epsilon\)</span> to take a
random action from all the action space.</p>
<h2 id="sarsa-state-action-reward-state-action">SARSA:
State-Action-Reward-State-Action</h2>
<p>SARSA is a variant of Q learning which is an
<strong>on-policy</strong> algorithm, by updating Q functions in the
following way <span class="math display">\[
Q^{\text{new}}(s_k,a_k) \gets Q^{\text{old}}(s_k,a_k) + \alpha \left(r_k
+ \gamma {\color{red} Q^{\text{old}}(s_{k+1},
a_{k+1})}-Q^{\text{old}}(s_k,a_k) \right)
\]</span> SARSA also works with TD(N) for any <span
class="math inline">\(N &gt; 0\)</span>.</p>
<h1 id="deep-rl">Deep RL</h1>
<h2 id="deep-policy-network-policy-gradient">Deep Policy Network (Policy
Gradient)</h2>
<p>By introducing the neural network into the policy function, we use a
network to approximate the policy, so a policy function parameterized by
<span class="math inline">\(\theta\)</span> can be written as <span
class="math display">\[
\pi_{\theta}(s,a).
\]</span> Then to update the policy <span
class="math inline">\(\pi\)</span>, we update the parameters <span
class="math inline">\(\theta\)</span> by <span class="math display">\[
\theta^{\text{new}} \gets \theta^{\text{old}} + \alpha \nabla_\theta
\mathbf{R}_\theta
\]</span> We calculate the gradient here, first note that <span
class="math inline">\(\nabla_{\theta}\mathbf{R}_\theta =
\nabla_{\theta}V^{\pi_\theta}(s_0)\)</span>, where <span
class="math inline">\(s\)</span> is the starting state. By policy
gradient theorem we have <span class="math display">\[
\nabla_{\theta}V^{\pi_\theta}(s_0) \propto \sum_{s \in
S}\nu^{\pi_{\theta}}(s)\sum_{a \in A} Q^{\pi_{\theta}}(s,a)\nabla_\theta
\pi_{\theta}(a~|~s). \label{pg_thm1} \tag{1}
\]</span> The proof of policy gradient theorem can be found in <a
href="#proof-of-policy-gradient-theorem">this section</a>. So we have
<span class="math display">\[
\begin{align*}
    \nabla_{\theta}V^{\pi_\theta}(s_0)
\propto&amp;~\sum_{s \in S}\nu^{\pi_{\theta}}(s)\sum_{a \in A}
Q^{\pi_{\theta}}(s,a)\nabla_\theta \pi_{\theta}(a~|~s) \\
=&amp;~ \sum_{x \in S}\nu^{\pi_{\theta}}(s)\sum_{a \in A}
\pi_{\theta}(a~|~s)Q^{\pi_{\theta}}(s,a)
\frac{\nabla_\theta\pi_{\theta}(a~|~s)}{\pi_{\theta}(a~|~s)} \\
=&amp;~ \mathbb{E}_{\pi_\theta}[Q^{\pi_{\theta}}(s,a)\nabla_\theta\log
\pi_\theta(a~|~s)]
\end{align*}
\]</span> We can use it in the update of <span
class="math inline">\(\theta\)</span>.</p>
<h2 id="reinforce">REINFORCE</h2>
<p>In policy gradient, we need to know <span
class="math inline">\(Q^{\pi_\theta}(s,a)\)</span>. There are multiple
ways of estimating it. REINFORCE is a Monte Carlo variant of a policy
gradient algorithm that estimates <span class="math inline">\(Q\)</span>
via sampling several steps. For example, in an environment with a finite
steps (<span class="math inline">\(T\)</span> steps), REINFORCE
calculates the policy gradient by <span class="math display">\[
\nabla_\theta V^{\pi_\theta} =
\mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^T\left(\sum_{t&#39;=t}^T\gamma^{t&#39;-t}r_{t&#39;}\nabla_\theta\log\pi_{\theta}(a_t~|~s_t)\right)\right].
\]</span> The algorithm runs in steps:</p>
<ol type="1">
<li>Sample trajectory <span
class="math inline">\(\{s_1,a_1,r_1,\dots,s_T,a_T,r_T\}\)</span> using
current <span class="math inline">\(\pi_\theta\)</span>;</li>
<li>calculate return <span
class="math inline">\(\hat{R}_t\gets\sum_{t&#39;=t}^T\gamma^{t&#39;-t}r_{t&#39;}\)</span>
for every time step <span class="math inline">\(r\)</span>;</li>
<li>update <span class="math inline">\(\theta\)</span> by <span
class="math inline">\(\theta_{\mathrm{new}}\gets \theta +
\alpha\sum_{t=1}^T\hat{R}_t\nabla_\theta
\log\pi(a_t~|~s_t)\)</span>.</li>
</ol>
<h2 id="deep-q-learning-dqn">Deep Q-Learning (DQN)</h2>
<p>Just like Q-Learning, instead, model the Q function using neural
network parameterized by <span class="math inline">\(\theta\)</span>,
i.e., <span class="math display">\[
Q(s,a) \approx Q_\theta(s,a)
\]</span> Recall that in Q-learning, we update Q value by <span
class="math display">\[
Q^{\text{new}}(s_k,a_k) \gets Q^{\text{old}}(s_k,a_k) + \alpha \left(r_k
+ \gamma\max_{a}Q^{\text{old}}(s_{k+1}, a)-Q^{\text{old}}(s_k,a_k)
\right)
\]</span> So the loss here is defined as <span class="math display">\[
L=\mathbb{E}\left[\left(r_k + \gamma\max_aQ_\theta(s_{k+1}, a) -
Q_\theta(s_k,a_k)\right)^2\right]
\]</span> Here are some tricks of DQN.</p>
<ol type="1">
<li><p>experience replay:</p>
<p>In normal supervised learning, the data will be used to train the
network for multiple times. However in DQN, each data is only used once.
To avoid this, we maintain a buffer to store the 4-tuple of (state,
action, reward, next state). When training, we pick batch of data from
the buffer and use it to train the network.</p></li>
<li><p>target network:</p>
<p>The target of DQN is to approximate <span class="math inline">\(r +
\gamma\max_aQ_\theta(s, a)\)</span>. Since the output of the network is
included in the TD loss, it will be unstable during the training. To
avoid this, we freeze the network by using a copy of it. Let's say
original DQN to be <span
class="math inline">\(Q_{\theta_1}(s,a)\)</span> and a new network
(target network) <span class="math inline">\(Q_{\theta_2}(s,a)\)</span>,
the loss is then calculated as <span class="math display">\[
L =\mathbb{E}\left[\left(r_k + \gamma\max_aQ_{\theta_1}(s_{k+1}, a) -
Q_{\theta_2}(s_k,a_k)\right)^2\right].
\]</span> The first network is updated using gradient descent, while the
target network is only synced with the main network every <span
class="math inline">\(C\)</span> steps.</p></li>
</ol>
<p>Another variant of Deep Q-Learning is Deep Dueling Q Network (DDQN),
in which we split Q network into two parts, the value network <span
class="math inline">\(V_{\theta_1}(s)\)</span> and the advantage network
<span class="math inline">\(A_{\theta_2}(s,a)\)</span> such that <span
class="math display">\[
Q_\theta(s,a) = V_{\theta_1}(s)+A_{\theta_2}(s,a).
\]</span> By this, the value network learns the value function and the
advantage network learns the advantage of an action could take to
current state.</p>
<h2 id="actor-critic-network">Actor-Critic Network</h2>
<p>Recall that in Policy Gradient method, we calculated the gradient as
following <span class="math display">\[
\nabla_{\theta}J_\theta
= \mathbb{E}_{\pi_\theta}[Q^{\pi_{\theta}}(s,a)\nabla_\theta\log
\pi_\theta(a~|~s)].
\]</span> Actually there is a generalized expression that <span
class="math display">\[
\nabla_{\theta}J_\theta=
\mathbb{E}_{\pi_\theta}[\psi_t\cdot\nabla_\theta\log \pi_\theta(a~|~s)],
\]</span> where <span class="math inline">\(\psi_t\)</span> can be one
of</p>
<ol type="1">
<li><span
class="math inline">\(\sum_{t&#39;=0}^T\gamma^{t&#39;}r_t&#39;\)</span>:
total return of the trajectory;</li>
<li><span
class="math inline">\(\sum_{t&#39;=t}^T\gamma^{t&#39;-t}r_{t&#39;}\)</span>:
return after action <span class="math inline">\(a_t\)</span>;</li>
<li><span class="math inline">\(Q^{\pi_\theta}(s_r, a_t)\)</span>: Q
function;</li>
<li><span class="math inline">\(A^{\pi_\theta}(s_t, a_t)\)</span>:
advantage function;</li>
<li><span class="math inline">\(r_t+\gamma V^{\pi_\theta}(s_{t+1}) -
V^{\pi_\theta}(s_t)\)</span>: temporal difference residual.</li>
</ol>
<p>In Actor-Critic setting, we have two neural networks, one is policy
network (actor) <span class="math inline">\(\pi_{\theta_1}(s,a)\)</span>
and the other is value network (critic) <span
class="math inline">\(V_{\theta_2}(s)\)</span>. The loss of the value
function is defined as <span class="math display">\[
L(\theta_2) := (r+\gamma V_{\theta_2}(s_{t+1}) - V_{\theta_2}(s_t))^2.
\]</span> Like we did in the target network in DQN, the first part <span
class="math inline">\(r+\gamma V_{\theta_2}(s_{t+1})\)</span> is TD
target and will not be calculated into gradient, we the gradient will be
<span class="math display">\[
\nabla_{\theta_2}L(\theta_2) = -2(r+\gamma V_{\theta_2}(s_{t+1}) -
V_{\theta_2}(s_t))\nabla_{\theta_2}V_{\theta_2}(s_t).
\]</span> Hence the updated rule of the two networks are <span
class="math display">\[
\begin{align*}
    \theta_1 \gets&amp;~ \theta_1 +
\alpha_{\theta_1}\sum_{t}\delta_t\nabla_{\theta_1}\log\pi_{\theta_1}(a_t~|~s_t)
\\
    \theta_2 \gets&amp;~ \theta_2 +
\alpha_{\theta_2}\sum_{t}\delta_t\nabla_{\theta_2}V_{\theta_2}(s_t),
\end{align*}
\]</span> where <span class="math inline">\(\delta_t := r_t +\gamma
V_{\theta_2}(s_{t+1}) - V_{\theta_2}(s_t)\)</span>.</p>
<h2 id="trpo-trust-region-policy-optimization-paper-link">TRPO: Trust
Region Policy Optimization [<a
href="https://arxiv.org/pdf/1502.05477">paper link</a>]</h2>
<p>All previous policy-based algorithms face a problem of instability
during training. To avoid this, we consider a trust region when updating
the parameters, with some security guarantee. Theoretically it can
guarantee the monotonicity of performance during training.</p>
<p>Given current policy <span class="math inline">\(\pi_\theta\)</span>,
we consider looking for a better parameter <span
class="math inline">\(\theta&#39;\)</span> such that <span
class="math inline">\(J(\theta&#39;) \ge J(\theta)\)</span>. Since the
distribution of starting state <span class="math inline">\(s_0\)</span>
is independent of the policy. We can use expectation under new policy
<span class="math inline">\(\pi_{\theta&#39;}\)</span> to describe
current optimization target: <span class="math display">\[
\begin{align*}
    J(\theta)
=&amp;~\mathbb{E}_{s_0}\left[V^{\pi_\theta}(s_0)\right] \\
=&amp;~\mathbb{E}_{\pi_{\theta&#39;}}\left[\sum_{t=0}^\infty\gamma^t
V^{\pi_{\theta}}(s_t) -
\sum_{t=1}^\infty\gamma^tV^{\pi_\theta}(s_t)\right] \\
=&amp;~-\mathbb{E}_{\pi_{\theta&#39;}}\left[\sum_{t=0}^\infty\gamma^t
(\gamma V^{\pi_{\theta}}(s_{t+1}) - V^{\pi_\theta}(s_t))\right]
\end{align*}
\]</span> Given this, we can calculate the difference between the two
target functions as <span class="math display">\[
\begin{align*}
    J(\theta&#39;) - J(\theta)
=&amp;~ \mathbb{E}_{s_0}\left[V^{\pi_{\theta&#39;}}(s_0)\right] -
\mathbb{E}_{s_0}\left[V^{\pi_\theta}(s_0)\right] \\
=&amp;~ \mathbb{E}_{\pi_{\theta&#39;}}\left[\sum_{t = 0}^\infty\gamma^t
r(s_t,a_t)\right] +
\mathbb{E}_{\pi_{\theta&#39;}}\left[\sum_{t=0}^\infty\gamma^t (\gamma
V^{\pi_{\theta}}(s_{t+1}) - V^{\pi_\theta}(s_t))\right] \\
=&amp;~
\mathbb{E}_{\pi_{\theta&#39;}}\left[\sum_{t=0}^\infty\gamma^t(r(s_t,a_t)
+ \gamma V^{\pi_{\theta}}(s_{t+1}) - V^{\pi_\theta}(s_t))\right]
\end{align*}
\]</span> By defining temporal difference residual as the advantage
<span class="math inline">\(A^{\pi_\theta}(s_t,a_t) := r(s_t,a_t) +
\gamma V^{\pi_{\theta}}(s_{t+1}) - V^{\pi_\theta}(s_t)\)</span>, we have
<span class="math display">\[
\begin{align*}
        J(\theta&#39;) - J(\theta)
    =&amp;~
\mathbb{E}_{\pi_{\theta&#39;}}\left[\sum_{t=0}^\infty\gamma^tA^{\pi_\theta}(s_t,a_t)\right]
\\
    =&amp;~ \sum_{t=1}^\infty \gamma^t\mathbb{E}_{s_t \sim
P_t^{\pi_{\theta&#39;}},
a_t\sim\pi_{\theta&#39;}(\cdot~|~s_t)}[A^{\pi_\theta}(s_t,a_t)] \\
    =&amp;~\frac{1}{1-\gamma}\mathbb{E}_{s \sim \nu^{\pi_{\theta&#39;}},
a\sim\pi_{\theta&#39;}(\cdot~|~s)}[A^{\pi_\theta}(s,a)]
\end{align*}
\]</span> where <span class="math inline">\(\nu^{\pi_\theta}(s)\)</span>
is the state visitation distribution, defined as <span
class="math display">\[
\nu^\pi (s):=(1-\gamma) \sum_{t=0}^\infty \gamma^tP_t^\pi(s),
\]</span> where <span class="math inline">\(P_t^\pi(s)\)</span> is the
probability that by policy <span class="math inline">\(\pi\)</span>,
agent is at state <span class="math inline">\(s\)</span> in time step
<span class="math inline">\(t\)</span>. So we are now looking for a
parameter <span class="math inline">\(\theta&#39;\)</span> with <span
class="math inline">\(\mathbb{E}_{s \sim \nu^{\pi_{\theta&#39;}},
a\sim\pi_{\theta&#39;}(\cdot~|~s)}[A^{\pi_\theta}(s,a)] \ge 0\)</span>,
then it will guarantee <span class="math inline">\(J(\theta&#39;) \ge
J(\theta)\)</span>. But it is hard or impossible to solve this directly.
So we ignore the difference between the state visitation distribution of
the two policies and use the distribution of the old policy <span
class="math inline">\(\pi_\theta\)</span>, hence we define the following
optimization target <span class="math display">\[
L_{\theta}(\theta&#39;) = J(\theta) + \frac{1}{1-\gamma}\mathbb{E}_{s
\sim \nu^{\pi_{\theta}},
a\sim\pi_{\theta&#39;}(\cdot~|~s)}[A^{\pi_\theta}(s,a)].
\]</span> For the actions, we can use importance sampling and hence we
get <span class="math display">\[
L_{\theta}(\theta&#39;) = J(\theta) + \frac{1}{1-\gamma}\mathbb{E}_{s
\sim \nu^{\pi_{\theta}},
a\sim\pi_{\theta}(\cdot~|~s)}\left[\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta}(a~|~s)}A^{\pi_\theta}(s,a)\right].
\]</span> To make the policies are close enough, we use Kullback-Leibler
Divergence constraint and hence we get the following optimization
problem <span class="math display">\[
\begin{align}
    \max_{\theta&#39;}&amp;~~L_\theta(\theta&#39;)\notag\\
     \text{s.t.}&amp;~~
\mathbb{E}_{s\sim\nu^{\pi_{\theta}}}[D_{KL}(\pi_{\theta}(\cdot~|~s),
\pi_{\theta&#39;}(\cdot~|~s))] \le \delta.\label{trpo}\tag{2}
\end{align}
\]</span> The constraint here actually defines a KL ball in the policy
space, called <strong>trust region</strong>.</p>
<h2 id="ppo-proximal-policy-optimization-paper-link">PPO: Proximal
Policy Optimization [<a href="https://arxiv.org/pdf/1707.06347">paper
link</a>]</h2>
<p>Consider the same optimization problem in TRPO <span
class="math display">\[
\begin{align*}    
\max_{\theta&#39;}&amp;~~\mathbb{E}_{s \sim \nu^{\pi_{\theta_k}},
a\sim\pi_{\theta_k}(\cdot~|~s)}\left[\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta}(a~|~s)}A^{\pi_{\theta_k}}(s,a)\right]\\     
\text{s.t.}&amp;~~
\mathbb{E}_{s\sim\nu^{\pi_{\theta_k}}}[D_{KL}(\pi_{\theta_k}(\cdot~|~s),
\pi_{\theta&#39;}(\cdot~|~s))] \le \delta.
\end{align*}
\]</span> There are some methods to solve this optimization, including
Taylor approximation, conjugate gradient method and linear search. But
in general it is still hard to solve. So PPO was introduced. There are
two types of PPO, PPO-Penalty and PPO-Clip.</p>
<h3 id="ppo-penalty">PPO-Penalty</h3>
<p>PPO-penalty use the method of Lagrange multipliers to put the
constraint of KL divergence into the optimization function, hence we
have the unconstrained optimization <span class="math display">\[
\theta\gets {\arg\max}_{\theta&#39;}\mathbb{E}_{s \sim
\nu^{\pi_{\theta_k}},
a\sim\pi_{\theta_k}(\cdot~|~s)}\left[\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta_k}(a~|~s)}A^{\pi_{\theta_k}}(s,a)
- \beta D_{KL}(\pi_{\theta_k}(\cdot~|~s),
\pi_{\theta&#39;}(\cdot~|~s))\right].
\]</span> Let <span class="math inline">\(d_k =
D_{KL}^{\nu^{\pi_{\theta_k}}}(\pi_{\theta_k},
\pi_{\theta&#39;})\)</span>, the rule of updating <span
class="math inline">\(\beta\)</span> is</p>
<ul>
<li>If <span class="math inline">\(d_k &lt; \delta/1.5\)</span>, let
<span class="math inline">\(\beta_{k+1} = \beta_k/2\)</span></li>
<li>If <span class="math inline">\(d_k &gt;1.5 \cdot\delta\)</span>, let
<span class="math inline">\(\beta_{k+1} = 2\beta_k\)</span></li>
<li>else, let <span class="math inline">\(\beta_{k+1} =
\beta_k\)</span>.</li>
</ul>
<p>where <span class="math inline">\(\delta\)</span> is a preset
hyperparameter.</p>
<h3 id="ppo-clip">PPO-Clip</h3>
<p>PPO-Clip is more straightforward, by restricting the difference of
the policies directly in the optimization function using both, i.e.,
<span class="math display">\[
\theta_{k+1}\gets {\arg\max}_{\theta&#39;}\mathbb{E}_{s \sim
\nu^{\pi_{\theta_k}},
a\sim\pi_{\theta_k}(\cdot~|~s)}\left[\min\left(\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta_k}(a~|~s)}A^{\pi_{\theta_k}}(s,a),
\mathrm{clip}\left(\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta_k}(a~|~s)},
1-\epsilon, 1+\epsilon\right)\cdot
A^{\pi_{\theta_k}}(s,a)\right)\right].
\]</span></p>
<h3 id="gae-generalized-advantage-estimation">GAE: Generalized Advantage
Estimation</h3>
<p>In order to apply TRPO or PPO, we still need to know the values of
advantage function <span
class="math inline">\(A^{\pi_{\theta_k}}(s,a)\)</span>. One way we can
approximate know the advantage is by GAE. Let <span
class="math inline">\(\delta_t = r_t + \gamma V(s_t+1) - V_t\)</span> be
the temporal difference, where <span class="math inline">\(V\)</span> is
a learned value function. Hence we can define for different steps: <span
class="math display">\[
\begin{align*}
    A_t^{(1)} = &amp;~\delta_t &amp;= -V(s_t) + r_t + \gamma V(s_{t+1})
\\
    A_t^{(2)} = &amp;~ \delta_t + \gamma\delta_{t+1} &amp;= -V(s_t) +
r_t + \gamma r_{t+1}+\gamma^2 V(s_{t+2})\\
    \vdots &amp; &amp;\vdots\\
    A_t^{(k)} = &amp;~\sum_{l=0}^{k-1}\gamma^l\delta_{t+1} &amp;=-V(s_t)
+ r_t + \gamma r_{t+1}+\gamma^2+\dots+ \gamma^kV(s_{t+k})
\end{align*}
\]</span> And we calculate the weighted average over them to get: <span
class="math display">\[
\begin{align*}
    A_t^{\mathrm{GAE}}
:=&amp;~(1-\lambda)(A_t^{(1)}+ A_t^{(2)}+A_t^{(3)}+\dots) \\
=&amp;~ (1-\lambda)(\delta_t +
\lambda(\delta_t+\lambda\delta_{t+1})+\lambda^2(\delta_t+\lambda\delta_{t+1}+\lambda^2\delta_{t+2})+\dots)
\\
=&amp;~(1-\lambda)(\delta_t(1+\lambda+\lambda^2+\dots)+\gamma\delta_{t+1}(\lambda+\lambda^2+\lambda^3+\dots)+\dots)\\
=&amp;~(1-\lambda)(\delta_t\frac{1}{1-\lambda}+\gamma\delta_{t+1}\frac{\lambda}{1-\lambda}
+ \gamma^2\delta_{t+2}\frac{\lambda^2}{1-\lambda} + \dots) \\
=&amp;~\sum_{l=0}^\infty(\gamma\lambda)^l\delta_{t+l},
\end{align*}
\]</span> where <span class="math inline">\(\lambda \in [0,1]\)</span>
is a hyperparameter. For <span class="math inline">\(\lambda=0\)</span>,
this becomes <span class="math inline">\(\delta_t\)</span>, and for
<span class="math inline">\(\lambda=1\)</span>, this becomes average
over <span class="math inline">\(A_t^{(k)}\)</span>'s.</p>
<h1 id="rl-in-language-models">RL in Language Models</h1>
<p>Some RL techniques are widely used in LLM alignment, though they are
not intentionally designed for this.</p>
<h2 id="rlhf-reinforcement-learning-with-human-feedback">RLHF:
Reinforcement Learning with Human Feedback</h2>
<p>It's hard to teach LLMs generating ``good'' text that aligns with
human preference via regular Supervised Fine-Tuning (SFT). However, we
can do that by RLHF. Typically, given a pretrained language model, there
are several steps of RLHF:</p>
<ol type="1">
<li>Gathering data by generating multiple responses from the LM, and use
human annotation to rank the responses</li>
<li>Train a reward model for predicting the human preference with future
reward responses.</li>
<li>Use RL methods to finetune the model to maximize the rewards, which
will increase the probability of generating responses human like and
decrease the probability of generating responses that human
dislike.</li>
</ol>
<h2 id="reward-modeling">Reward Modeling</h2>
<p>Given a reward model <span
class="math inline">\(R_{\theta}(q,a)\)</span> parametrized by <span
class="math inline">\(\theta\)</span> and predict the rating of the
question-answer pair <span class="math inline">\((q,a)\)</span>,
following the Bradley-Terry model, which defines the probability that a
rater prefers <span class="math inline">\(a_i\)</span> over <span
class="math inline">\(a_j\)</span> as <span class="math display">\[
\Pr[a_i \succ a_j] = \frac{\exp(R_\theta(q,a_i))}{\exp(R_\theta(q,a_i))
+ \exp(R_\theta(q,a_j))}.
\]</span> Taking the negative log-likelihood of the probability, we get
the training objective loss function for pairs <span
class="math inline">\((q,a_i)\)</span> and <span
class="math inline">\((q,a_j)\)</span> as <span class="math display">\[
L(\theta) = -\log\sigma(R_\theta(q,a_i)) - \exp(R_\theta(q,a_j)).
\]</span></p>
<h2 id="ppo-in-rlhf">PPO in RLHF</h2>
<p>In this case, we typically train the critic model (used as value
function) to be aligned with the output of the reward model. The
objective can be written as <span class="math display">\[
L(\theta_{c}) = \mathbb{E}_t[(V_{\theta_c}(s_t) - R_{\theta_r}(s_T))^2],
\]</span> where <span class="math inline">\(R_{\theta_r}\)</span> is the
pretrained reward model. And instead of using PPO-clip or penalty alone
and optimize the objective function, we update the policy by calculating
the loss of it to maximize the objective using both methods: <span
class="math display">\[
L_{\text{ppo}}(\theta) =\mathbb{E}_{s \sim \nu^{\pi_{\theta_k}},
a\sim\pi_{\theta_k}(\cdot~|~s)}\left[\min\left(\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta_k}(a~|~s)}A^{\pi_{\theta_k}}(s,a),
\mathrm{clip}\left(\frac{\pi_{\theta&#39;}(a~|~s)}{\pi_{\theta_k}(a~|~s)},
1-\epsilon, 1+\epsilon\right)\cdot A^{\pi_{\theta_k}}(s,a)\right)\right]
- \mathbb{E}_{s \sim \nu^{\pi_{\theta_k}}}\left[\beta
D_{KL}(\pi_{\theta_k}(\cdot~|~s), \pi_{\theta&#39;}(\cdot~|~s))\right]
\]</span> Adding them together, we get the loss function to optimize in
RLHF using PPO: <span class="math display">\[
L = L(\theta_r) + L_{\text{ppo}}(\theta).
\]</span></p>
<h2 id="grpo-group-relative-policy-optimization">GRPO: Group Relative
Policy Optimization</h2>
<p><img src="/images/image-20250209201756802.png" title="Demonstration of PPO and GRPO" width="700"/></p>
<p>GRPO is proposed to avoid the use of the additional value function
approximation. Specifically, for each question <span
class="math inline">\(q\)</span>, GRPO samples a group of outputs <span
class="math inline">\(\{o_1, o_2, \dots, o_G\}\)</span> from current
policy <span class="math inline">\(\pi_{\theta_k}\)</span>, then use
them to optimize <span class="math display">\[
\begin{align*}
&amp;L_{\text{GRPO}}(\theta) = \mathop{\mathbb{E}}\limits_{q\sim{\cal
Q}, \{o_i\}_{i\in[G]}\sim\pi_{\theta_k}(\cdot~|~q)}\\
&amp;\left[\frac{1}{G}\sum_{i=1}^G\frac{1}{|o_i|}\left\{\min\left\{\frac{\pi_{\theta}(o_{i,t}~|~q,o_{1,&lt;t})}{\pi_{\theta_k}(o_{i,t}~|~q,o_{1,&lt;t})}A^{\pi_{\theta_k}}_{i,t},\mathrm{clip}\left(\frac{\pi_{\theta}(o_{i,t}~|~q,o_{1,&lt;t})}{\pi_{\theta_k}(o_{i,t}~|~q,o_{1,&lt;t})},
1-\epsilon,
1+\epsilon\right)A^{\pi_{\theta_k}}_{i,t}\right\}\right\}-\beta
D_{KL}(\pi_\theta~|~\pi_{\theta_k})\right],
\end{align*}
\]</span> where <span class="math inline">\(\epsilon\)</span> and <span
class="math inline">\(\beta\)</span> are hyper parameters. The KL
divergence here is estimated by unbiased estimator <span
class="math display">\[
D_{KL}=\frac{\pi_{\theta_k}(o_{i,t}~|~q,o_{1,&lt;t})}{\pi_{\theta}(o_{i,t}~|~q,o_{1,&lt;t})}
-
\log\frac{\pi_{\theta_k}(o_{i,t}~|~q,o_{1,&lt;t})}{\pi_{\theta}(o_{i,t}~|~q,o_{1,&lt;t})}.
\]</span> <span class="math inline">\(A^{\pi_{\theta_k}}_{i,t}\)</span>
is the advantage calculated based on relative rewards of the outputs
inside each group only. For <strong>outcome based RL</strong>, i.e., the
reward model outputs rewards based on the outcome for each of the
outcomes <span
class="math inline">\(\mathbf{r}:=\{r_1,r_2,\dots,r_G\}\)</span>, we
assign the advantage of each internal token with the normalized reward,
i.e., <span class="math display">\[
\hat{A}_{i,t} \gets
\frac{r_i-\mathrm{mean}(\mathbf{r})}{\mathrm{std}(\mathbf{r})}.
\]</span> For <strong>process based RL</strong>, i.e., the reward model
outputs rewards based on every reasoning step of each outcomes <span
class="math inline">\(\mathbf{R}
:=\{\{r_1^{\mathrm{ind}(1)},\dots,r_1^{\mathrm{ind}(k_1)}\},\dots,\{r_G^{\mathrm{ind}(1)},\dots,r_G^{\mathrm{ind}(k_G)}\}\}\)</span>,
where <span class="math inline">\(k_j\)</span> is the <span
class="math inline">\(\mathrm{ind}(j)\)</span> is the ending token index
of <span class="math inline">\(j\)</span>-th step, and <span
class="math inline">\(k_i\)</span> is the total number of steps in the
<span class="math inline">\(i\)</span>-th output. We assign the
advantage by <span class="math display">\[
\begin{align*}
\hat{r}_i^{\mathrm{ind}(j)} \gets&amp;~
\frac{r_i^{\mathrm{ind}(j)}-\mathrm{mean}(\mathbf{R})}{\mathrm{std}(\mathbf{R})}
\\
\hat{A}_{i,t} \gets&amp;~ \sum_{\mathrm{ind}(j)\ge
t}\hat{r}_i^{\mathrm{ind}(j)}.
\end{align*}
\]</span></p>
<h1 id="appendix">Appendix</h1>
<h2 id="proof-of-policy-gradient-theorem">Proof of Policy Gradient
Theorem</h2>
<p>Here we give the proof for Eq.<span
class="math inline">\(\eqref{pg_thm1}\)</span>.</p>
<p>We have <span class="math display">\[
\begin{align*}
    \nabla_{\theta}V^{\pi_\theta}(s)
=&amp;~ \nabla_\theta\left(\sum_{a \in A}\pi_\theta(a~|~s) \cdot
Q^{\pi_\theta}(s,a) \right) \\
=&amp;~ \sum_{a \in A} \left(\nabla_\theta
\pi_\theta(a~|~s)Q^{\pi_\theta}(s,a) + \pi_\theta(a~|~s)\nabla_\theta
Q^{\pi_\theta}(s,a) \right)\\
=&amp;~ \sum_{a \in A} \left(\nabla_\theta
\pi_\theta(a~|~s)Q^{\pi_\theta}(s,a) + \pi_\theta(a~|~s)\nabla_\theta
\sum_{s&#39;,r}\Pr[s&#39;,r~|~s,a](r+\gamma V^{\pi_\theta}(s&#39;))
\right)\\
=&amp;~ \sum_{a \in A} \left(\nabla_\theta
\pi_\theta(a~|~s)Q^{\pi_\theta}(s,a) + \gamma
\pi_\theta(a~|~s)\sum_{s&#39;,r}\Pr[s&#39;,r~|~s,a]\gamma \nabla_\theta
V^{\pi_\theta}(s&#39;) \right)\\
=&amp;~ \sum_{a \in A} \left(\nabla_\theta
\pi_\theta(a~|~s)Q^{\pi_\theta}(s,a) + \gamma
\pi_\theta(a~|~s)\sum_{s&#39;}\Pr[s&#39;~|~s,a]\gamma \nabla_\theta
V^{\pi_\theta}(s&#39;) \right)\\
\end{align*}
\]</span> Define <span class="math inline">\(\phi(s) := \sum_{a \in
A}\nabla_\theta\pi_\theta(a~|~s)Q^{\pi_\theta}(s,a)\)</span>. Define
<span class="math inline">\(p^{\pi_\theta}(s \rightarrow x, k)\)</span>
to be the the probability of policy <span
class="math inline">\(\pi\)</span> starting from state <span
class="math inline">\(s\)</span> to arrive at state <span
class="math inline">\(x\)</span> after <span
class="math inline">\(k\)</span> steps. Hence we have <span
class="math display">\[
\begin{align*}
    \nabla_{\theta}V^{\pi_\theta}(s)
=&amp;~ \phi(s)+\gamma\sum_{a\in
A}\pi_\theta(a~|~s)\sum_{s&#39;}\Pr[s&#39;~|~s,a]\gamma \nabla_\theta
V^{\pi_\theta}(s&#39;) \\
=&amp;~ \phi(s) + \gamma \sum_{a \in A}\sum_{s&#39;}
\pi_\theta(a~|~s)\Pr[s&#39;|s,a]\nabla_\theta V^{\pi_\theta}(s&#39;) \\
=&amp;~ \phi(s) + \gamma \sum_{s&#39;}p^{\pi_\theta}(s \rightarrow
s&#39;, 1)\nabla_\theta V^{\pi_\theta}(s&#39;) \\
=&amp;~ \phi(s) + \gamma \sum_{s&#39;}p^{\pi_\theta}(s \rightarrow
s&#39;, 1)\left(\phi(s&#39;) + \gamma\sum_{s&#39;&#39;}p^{\pi_\theta}(s
\rightarrow s&#39;&#39;, 2)\nabla_\theta
V^{\pi_\theta}(s&#39;&#39;)\right) \\
=&amp;~ \phi(s) + \gamma \sum_{s&#39;}p^{\pi_\theta}(s \rightarrow
s&#39;, 1)\phi(s&#39;) + \gamma^2 \sum_{s&#39;&#39;}p^{\pi_\theta}(s
\rightarrow s&#39;&#39;, 2)\nabla_\theta V^{\pi_\theta}(s&#39;&#39;) \\
=&amp;~ \cdots \\
=&amp;~ \sum_{x \in S}\sum_{k=0}^\infty \gamma^k p^{\pi_\theta}(s
\rightarrow x, k)\phi(x).
\end{align*}
\]</span> Define <span class="math inline">\(\eta(s) = \sum_{k=0}^\infty
\gamma^k p^{\pi_\theta}(s \rightarrow x, k)\)</span>. We get <span
class="math display">\[
\begin{align*}
\nabla_\theta V^{\pi_\theta}(s)
=&amp;~ \sum_{x \in S}\eta(x)\phi(x) \\
=&amp;~ \underbrace{\sum_{x \in S}\eta(x)}_{\text{Constant}} \cdot
\sum_{x \in S}\frac{\eta(x)}{\sum_{x \in S}\eta(x)}\phi(x) \\
\propto&amp;~ \sum_{x \in S}\frac{\eta(x)}{\sum_{x \in S}\eta(x)}\phi(x)
\\
=&amp;~ \sum_{x \in S} \nu^{\pi_\theta}(s) \sum_{a \in
A}Q^{\pi_\theta}(s,a)\nabla_\theta\pi_\theta(a~|~s),
\end{align*}
\]</span> where <span class="math inline">\(\nu^{\pi_\theta}(s)\)</span>
is the state visitation distribution, defined as <span
class="math display">\[
\nu^\pi (s):=(1-\gamma) \sum_{t=0}^\infty \gamma^tP_t^\pi(s),
\]</span> where <span class="math inline">\(P_t^\pi(s)\)</span> is the
probability that by policy <span class="math inline">\(\pi\)</span>,
agent is at state <span class="math inline">\(s\)</span> in time step
<span class="math inline">\(t\)</span>.</p>
]]></content>
      <tags>
        <tag>Research</tag>
      </tags>
  </entry>
  <entry>
    <title>å­¦æœŸæ€»ç»“</title>
    <url>/2022/02/16/%E5%AD%A6%E6%9C%9F%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>å»å¹´çš„æ—¶å€™ï¼ŒæŠ“ç€æš‘å‡çš„å°¾å·´æ­å»ºäº†è‡ªå·±çš„åšå®¢ï¼Œä½†ä»é‚£ä¹‹åï¼Œä¼¼ä¹ä¹Ÿæ²¡æœ‰å†å†™è¿‡åšæ–‡ã€‚å¦‚æœä¸è€ƒè™‘ç”¨æ¥åšæµ‹è¯•çš„é‚£ä¸¤ç¯‡ï¼Œè¿™åº”è¯¥ç®—æ˜¯ç¬¬ä¸€ç¯‡åšæ–‡äº†ã€‚</p>
<p>è½¬çœ¼ä¸€ä¸ªå­¦æœŸå’Œå¯’å‡å°±è¿™ä¹ˆè¿‡å»ï¼Œæ€»ç®—å¾—ç©ºï¼Œä¸”æœ‰é—²å¿ƒæ¥å†™ä¸€äº›ä¸œè¥¿ã€‚æ¥è¿‘åŠå¹´çš„æ—¶é—´æ²¡æœ‰æ‘¸è‡ªå·±çš„åšå®¢ï¼Œç«ŸèŠ±äº†åŠå°æ—¶é‡æ¸©äº†ä¸€éhexoçš„æ­å»ºå’Œä½¿ç”¨ï¼Œæ‰å¼€å§‹åŠ¨ç¬”å†™ä¸‹è¿™ç¯‡ã€‚</p>
<p>ç®€å•ç®—ç®—ï¼Œå·®ä¸å¤šæ­£å¥½å…­ä¸ªæœˆï¼Œä¸é•¿ä¸çŸ­çš„ä¸€æ®µæ—¶é—´ã€‚å­¦åˆ°äº†å¾ˆå¤šæ–°çš„ä¸œè¥¿ï¼Œæ€è€ƒäº†å¾ˆå¤šæ–°çš„é—®é¢˜ï¼Œæœ‰é«˜å…‰æœ‰æŒ«è´¥ã€‚Anywayï¼Œæˆ‘ä»ç„¶æœŸæœ›è‡ªå·±çš„åšå®¢æ˜¯æŠ€æœ¯å‘çš„ï¼Œé…¸è…çš„æ–‡å­—åœ¨è¿™é‡Œå°±ä¸å¿…å¤šè¯´äº†ã€‚</p>
<h2 id="å­¦ä¹ ç›¸å…³">å­¦ä¹ ç›¸å…³</h2>
<p>å…ˆä»å­¦ä¸šç›¸å…³çš„ä¸œè¥¿æ¥å¼€å§‹å§ã€‚</p>
<h3 id="å­¦æ ¡è¯¾ç¨‹">å­¦æ ¡è¯¾ç¨‹</h3>
<p>åœ¨è¯¾ç¨‹æ–¹é¢ï¼Œè¿™ä¸€å­¦æœŸæ˜¯ä»å­¦æ ¡è¯¾ç¨‹å­¦åˆ°äº†ä¸€äº›æœ‰ç”¨çš„çŸ¥è¯†çš„ï¼Œåˆ—ä¸¾å¦‚ä¸‹ï¼š</p>
<ul>
<li><p>Webä¿¡æ¯å¤„ç†ä¸åº”ç”¨ï¼ˆ<a
href="http://staff.ustc.edu.cn/~tongxu/webinfo/">è¯¾ç¨‹ä¸»é¡µ</a>ï¼‰</p>
<p>éå¸¸æœ‰æ„æ€çš„ä¸€é—¨è¯¾ç¨‹ï¼Œå¾ç«¥è€å¸ˆï¼ˆ<a
href="http://staff.ustc.edu.cn/~tongxu/">è€å¸ˆçš„ä¸»é¡µ</a>ï¼‰ä¹Ÿæ˜¯éå¸¸é£è¶£å¹½é»˜çš„ä¸€ä¸ªäººï¼Œè¯¾ç¨‹æ¶‰åŠçŸ¥è¯†éå¸¸ä¹‹å¹¿èŒƒç¹æ‚ï¼Œä½†è€å¸ˆçš„PPTåˆ¶ä½œè‰¯å¥½ï¼Œé€‚åˆå¼‚æ­¥å­¦ä¹ ã€‚å¤§è‡´æ¶‰åŠäº†webä¿¡æ¯çš„æœé›†ï¼ˆçˆ¬è™«ï¼‰ï¼Œå¤„ç†ï¼ˆåˆ†ç±»ï¼Œèšç±»ç­‰å¤§æ•°æ®çŸ¥è¯†ï¼‰ï¼Œæ£€ç´¢ï¼ˆç½‘é¡µç´¢å¼•ï¼Œæœç´¢å¼•æ“ï¼Œå¤šæ¨¡æ€æ£€ç´¢ç­‰ï¼‰ã€‚æ¯•ç«Ÿæ˜¯é€‰ä¿®è¯¾ï¼Œå¤§å¤šæ•°çŸ¥è¯†ä¸€ç¬”å¸¦è¿‡ï¼Œä½†ä½œä¸ºwebä¿¡æ¯ï¼Œå¤§æ•°æ®çŸ¥è¯†çš„å…¥é—¨è¯¾ç¨‹è¿˜æ˜¯éå¸¸ä¸é”™çš„ã€‚è¯¥å­¦æœŸæ€»å…±ä¸‰æ¬¡å®éªŒï¼Œå¤§è‡´å†…å®¹å’Œæ€»ç»“å¦‚ä¸‹ï¼š</p>
<ul>
<li>å®éªŒä¸€ï¼šé¦–å…ˆå¯¹æ‰€ç»™çš„æ•°æ®é›†ï¼ˆç¾å›½ç»æµç›¸å…³æ–°é—»æ•°æ®é›†ï¼‰è¿›è¡Œé¢„å¤„ç†ï¼ˆè¿™é‡Œæ¶‰åŠåˆ°<span
class="math inline">\(NLP\)</span>çš„å·¥å…·å’ŒçŸ¥è¯†ï¼‰ï¼Œç„¶ååœ¨æ•°æ®é›†ä¸Šæ„å»ºä¸€ä¸ªç®€å•çš„æœç´¢å¼•æ“ï¼Œè¦æ±‚å®ç°å¸ƒå°”æ£€ç´¢å’Œè¯­ä¹‰æ£€ç´¢ã€‚é¡¹ç›®åœ°å€ï¼š<a
href="https://github.com/SGEthan/Info_Retrieving">SGEthan/Info_Retrieving</a></li>
<li>å®éªŒäºŒï¼šçŸ¥è¯†è¡¨ç¤ºå­¦ä¹ ç›¸å…³ï¼Œè¦æ±‚è¿›è¡Œå…³ç³»é¢„æµ‹ï¼Œå³ï¼Œåœ¨æ‰€ç»™æ•°æ®é›†ä¸Šï¼Œç»™å‡ºå®ä½“
<span class="math inline">\(entity:\mathbf{e}_1\)</span> å’Œå…³ç³» <span
class="math inline">\(relation:\mathbf{r}\)</span> ï¼Œè¦æ±‚è®¡ç®—ç»™å‡ºä¸
<span class="math inline">\(\mathbf{e_1}\)</span> æœ‰ç€å…³ç³» <span
class="math inline">\(\mathbf{r}\)</span> çš„å®ä½“ <span
class="math inline">\(entity:\mathbf{e_1}\)</span> çš„é¢„æµ‹ã€‚é¡¹ç›®åœ°å€ï¼š<a
href="https://github.com/SGEthan/Relation_Prediction">SGEthan/Relation_Prediction</a></li>
<li>å®éªŒä¸‰ï¼šæ¨èç³»ç»Ÿçš„æ„å»ºï¼Œè¦æ±‚åœ¨æ‰€ç»™æ•°æ®é›†çš„åŸºç¡€ä¸Šå¯¹æ¯ä¸ªç”¨æˆ·è¿›è¡ŒéŸ³ä¹çš„æ¨èã€‚æˆ‘ä»¬é‡‡ç”¨äº†<strong>åŸºäºç‰©å“çš„ååŒè¿‡æ»¤æ¨è</strong>ã€‚é¡¹ç›®åœ°å€ï¼š<a
href="https://github.com/SGEthan/Recommend">SGEthan/Recommend</a></li>
</ul></li>
<li><p>æœºå™¨å­¦ä¹  ï¼ˆ<a
href="https://miralab.ai/course/ml_2021fall/">è¯¾ç¨‹ä¸»é¡µ</a>ï¼‰</p>
<p>æ˜¯æˆ‘åœ¨USTCä¸¤å¹´åŠä»¥æ¥ä¸Šè¿‡çš„æœ€ç¡¬æ ¸çš„ä¸€é—¨è¯¾ç¨‹ï¼Œæ²¡æœ‰ä¹‹ä¸€ï¼Œè¢«ç§°ä¸º<strong>è¥¿åŒºæ•°å­¦ä¹‹å·…</strong>ã€‚è¿™é—¨è¯¾ç¨‹çš„æ•°å­¦æ¶‰åŠèŒƒå›´ä¹‹å¹¿æ³›ï¼Œéš¾åº¦ä¹‹å¤§ï¼Œæ˜¯è®¡ç§‘å’Œä¿¡é™¢å¤§å¤šæ•°è¯¾ç¨‹æ‰€ä¸åŠçš„ï¼Œè¿™æˆ–è®¸æ˜¯ç‹æ°è€å¸ˆï¼ˆ<a
href="https://miralab.ai/people/jie-wang/">è€å¸ˆçš„ä¸»é¡µ</a>ï¼‰çš„ä¸ªäººç‰¹ç‚¹ï¼Œä½†ä¹Ÿç¡®å®å¥‘åˆç§‘å¤§æ‰å®çš„æ•°ç†åŸºç¡€é£æ°”ï¼›è¯¾ç¨‹è¿˜æœ‰ä¸€ä¸ªç‰¹è‰²ï¼Œä½¿ç”¨å…¨è‹±æ–‡çš„å‚è€ƒèµ„æ–™ï¼Œä½œä¸šå’Œè€ƒè¯•ä¹Ÿå…¨éƒ¨è¦æ±‚è‹±è¯­ä½œç­”ï¼Œä¹Ÿæ˜¯å¤§å­¦ä»¥æ¥çš„ç¬¬ä¸€æ¬¡ã€‚</p>
<p>ä»æœ€åŸºç¡€çš„æ•°å­¦éƒ¨åˆ†è®²èµ·ï¼Œè€å¸ˆç”¨ä¸€ä¸ªå­¦æœŸä»æœ€åº•å±‚çš„æ•°å­¦å‘ä¸Šæ„å»ºäº†å­¦ä¹ ç†è®ºçš„æ¡†æ¶ï¼Œå¹¶ä¸”ä»‹ç»äº†ä¸€äº›åº”ç”¨çš„ç†è®ºå’Œç®—æ³•ã€‚è¯¾ç¨‹æ¶‰åŠçŸ¥è¯†å†…å®¹åˆ—ä¸¾å¦‚ä¸‹ï¼š</p>
<ul>
<li>çº¿æ€§å›å½’ Linear Regression</li>
<li>åå·®-æ–¹å·®åˆ†è§£ Bias Variance Decomposition</li>
<li>è´å¶æ–¯çº¿æ€§å›å½’ Bayesian Linear Regression</li>
<li>åˆ†æåŸºç¡€ Basics of Analysis</li>
<li>å‡¸ä¼˜åŒ–ç›¸å…³ Convex Sets, Convex Functions, Convex Optimization
Problems &amp; Separation Theorems</li>
<li>æ¬¡æ¢¯åº¦æ–¹æ³• Subdifferentials</li>
<li>å†³ç­–æ ‘ Decision Tree</li>
<li>æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ Naive Bayes Classifier</li>
<li>é€»è¾‘å›å½’ Logistic Regression</li>
<li>æ”¯æŒå‘é‡æœºï¼ˆä»¥åŠæ‹‰æ ¼æœ—æ—¥å¯¹å¶æ–¹æ³•ï¼‰ Support Vector Machine, with
Lagrange Duality</li>
<li>ç¥ç»ç½‘ç»œä¸å·ç§¯ç¥ç»ç½‘ç»œ Neural Networks &amp; Convolutional Neural
Networks</li>
<li>ä¸»æˆåˆ†åˆ†æ Principal Component Analysis</li>
<li>å¼ºåŒ–å­¦ä¹  Reinforcement Learning, with Multi-armed Bandits
Problem</li>
</ul>
<p>ç‹æ°è€å¸ˆå¯¹äºå­¦ç”Ÿçš„è¦æ±‚éå¸¸é«˜ï¼Œä½œä¸šé¢˜ç›®çš„éš¾åº¦éå¸¸ä¹‹å¤§ï¼Œæ¯æ¬¡ä½œä¸šå¤§çº¦éœ€è¦è‡³å°‘20ä¸ªå°æ—¶æ¥å®Œæˆï¼Œè€ƒè¯•éš¾åº¦æ¯”ä½œä¸šç•¥å°ï¼Œä½†æœŸä¸­æœŸæœ«çš„å…¨ç­å‡åˆ†éƒ½æ²¡æœ‰åŠæ ¼ã€‚è™½ç„¶å¾ˆç—›è‹¦ï¼Œä½†ä¸å¯å¦è®¤ï¼Œè¿™ä¸€é—¨è¯¾ç¨‹è®©æˆ‘åœ¨ä¸€ä¸ªå­¦æœŸå­¦åˆ°äº†éå¸¸å¤šçš„çŸ¥è¯†ï¼Œä¹Ÿé”»ç‚¼äº†åº”å¯¹è‹±æ–‡èµ„æ–™çš„èƒ½åŠ›ã€‚</p>
<p>è¯¾ç¨‹çš„final projectæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œè¦æ±‚æˆ‘ä»¬è®­ç»ƒä¸€ä¸ªAgentï¼Œåœ¨Atari
Pongæ¸¸æˆä¸­å–å¾—ä¸€å®šçš„æˆæœï¼Œè€Œä¸”ä¸ºäº†é”»ç‚¼æˆ‘ä»¬çš„èƒ½åŠ›ï¼Œä¸å…è®¸ä½¿ç”¨ä»»ä½•è‡ªåŠ¨æ±‚å¯¼çš„å·¥å…·åŒ…ã€‚æˆ‘ä»¬ç»„æœ€ç»ˆé€‰æ‹©ä½¿ç”¨Policy
Gradientæ–¹æ³•å»å®Œæˆï¼ˆäº‹å®ä¸Šæ˜¯å› ä¸ºä¸€å¼€å§‹é‡‡ç”¨çš„DQNæ–¹æ³•æ²¡æœ‰workï¼‰ï¼Œé¡¹ç›®åœ°å€ï¼š<a
href="https://github.com/SGEthan/RL_PG_Atari_Pong">SGEthan/RL_PG_Atari_Pong</a>
ã€‚</p></li>
<li><p>å…¶ä»–è¯¾ç¨‹</p>
<p>æœ‰ä¸€è¯´ä¸€ï¼Œé™¤äº†ä»¥ä¸Šä¸¤é—¨è¯¾ï¼Œè¿™å­¦æœŸå…¶ä»–è¯¾ç¨‹ç»™æˆ‘å¸¦æ¥çš„æ”¶ç›Šå¯ä»¥ç›´æ¥å¿½ç•¥ï¼Œè¿™å…¶ä¸­æœ‰ï¼š</p>
<ul>
<li>è®¡ç®—æœºç½‘ç»œï¼šæˆ‘æ ¡è®¡ç½‘åªèƒ½è¯´æ˜¯çº¯æ–‡ç§‘ï¼Œé ç€æœŸä¸­æœŸæœ«å‰çš„çªå‡»ï¼Œå­¦åˆ°äº†ä¸€äº›ä¸œè¥¿çš„ï¼Œä½†ä¸œè¥¿å±å®ä¸å¤šï¼Œå¤§æ¦‚å°±æ˜¯ä¸€äº›è®¡ç½‘çš„ç†è®ºçŸ¥è¯†å’Œç®€å•åº”ç”¨ã€‚ä¸ªäººè®¤ä¸ºè®¡ç®—æœºç½‘ç»œçŸ¥è¯†çš„å­¦ä¹ è¿˜æ˜¯å¾—é å®è·µï¼Œè€Œä¸”ç¡®å®æœ‰å¾ˆå¤šæœ‰è¶£çš„ç›¸å…³é¡¹ç›®ã€‚å¯’å‡åœ¨å®¶é‡ŒæŠ˜è…¾äº†ï¼š
<ul>
<li>æ‰¾ISPè¦æ¥äº†å…¬ç½‘IPï¼Œåœ¨å®¶é‡Œè€ç”µè„‘å¼€äº†ä¸ªæœåŠ¡ï¼Œç¡¬ç›˜æ˜ å°„å‡ºå»ï¼Œå½“ä½œä¸€ä¸ªNASä½¿ç”¨ã€‚å› ä¸ºç”µè„‘å¤ªè€ï¼Œä¹Ÿæ²¡æœ‰åˆ·NASçš„æ“ä½œç³»ç»Ÿï¼Œä»…ä»…æ˜¯ç®€å•è·‘äº†ä¸ªæœåŠ¡ï¼Œç”¨çš„æ˜¯<a
href="http://iscute.cn/chfs">chfs</a>ï¼Œåœ¨è¿™é‡Œç‰¹åˆ«æ„Ÿè°¢ä¸€ä¸‹é¡¹ç›®çš„å¼€å‘è€…ï¼Œæˆ‘é‡åˆ°çš„bugèƒ½å¤ŸåŠæ—¶ç»™äºˆé‚®ä»¶å›é¦ˆã€‚</li>
<li>æ‘¸äº†ä¸€ä¸‹V2Rayï¼Œé‡æ–°æ­äº†è‡ªå·±çš„æœåŠ¡ï¼Œè™½ç„¶ä»ç„¶ç”¨äº†åˆ«äººçš„ä¸€æŠŠæ¢­è„šæœ¬ã€‚ä¸‹å­¦æœŸé€‰è¿‡ä¿¡å®‰è¯¾ç¨‹ä¹‹åå¯èƒ½ä¼šè€ƒè™‘è‡ªå·±å®ç°ä¸€ä¸ªåè®®è¯•è¯•çœ‹ã€‚</li>
</ul></li>
<li>äººå·¥æ™ºèƒ½å¯¼è®ºï¼šçº¯æ–‡ç§‘ï¼ŒæœŸæœ«è€ƒå‰ä¸€å¤©å­¦å®Œ900é¡µçš„AIMAï¼Œæå‰äº¤å·å±…ç„¶è¿˜è€ƒå¾—ä¸é”™ï¼Œå±å®æ²¡å•¥ç”¨çš„ä¸€é—¨è¯¾</li>
<li>æ¨¡å¼è¯†åˆ«å¯¼è®ºï¼šäº†è§£äº†ä¸€äº›ä¼ ç»Ÿçš„ç»Ÿè®¡å­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬æ¦‚ç‡ï¼Œå‡ ä½•ï¼Œèšç±»æ–¹æ³•ã€‚å­¦åˆ°äº†ä¸€äº›ä¸œè¥¿ï¼Œä¹Ÿå†™äº†ä¸€äº›æ²¡å•¥ç”¨çš„å°é¡¹ç›®ï¼Œç”šè‡³éƒ½æ‡’å¾—ä¼ åˆ°Githubçš„é‚£ç§ã€‚æœ€è‰çš„æ˜¯ï¼ŒæœŸæœ«è€ƒè¯•å‰ï¼ŒæŠŠæ¯ä¸€ç§ç®—æ³•éƒ½å¤ä¹ çš„æ»šç“œçƒ‚ç†Ÿï¼Œç»“æœä»–å¨˜çš„è€ƒäº†ä¸€å¼ çº¯æ–‡ç§‘è¯•å·ï¼Œè¿™é“å‹è½´é¢˜å¯èƒ½ä¼šè®©æˆ‘è®°ä½å¾ˆä¹…ï¼šâ€œæ¨¡å¼è¯†åˆ«çš„ä¸€èˆ¬æµç¨‹æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿè¯·æŒ‰ç…§ä¸Šè¿°æµç¨‹è®¾è®¡ä¸€ä¸ªè¿‡ç¨‹æ¥è¿›è¡Œç¾½æ¯›çƒçš„ç­›é€‰â€ï¼Œæ•´å¼ å·å­çš„æ•°å­¦ä¸è¶…è¿‡å°å­¦æ°´å¹³ï¼Œä¸€é“ç®—æ³•éƒ½æ²¡è€ƒï¼Œå…¨æ˜¯æ¦‚å¿µï¼Œå…¨é ç¡¬ç³Šï¼Œæœ€åæˆç»©è‡ªç„¶ä¹Ÿä¸ç”šç†æƒ³ï¼Œå¥¶å¥¶çš„ã€‚</li>
<li>ç¼–è¯‘åŸç†ï¼šä¸ªäººå¯¹äºåº•å±‚æŠ€æœ¯æ²¡æœ‰ä¸æ¯«å…´è¶£ï¼Œè¿™é—¨è¯¾å…¨é é˜Ÿå‹å¸¦é£ï¼Œéå¸¸æ„Ÿè°¢æˆ‘çš„é˜Ÿå‹åŒ…å®¹æˆ‘æ‘¸é±¼ä¸€å­¦æœŸï¼ˆå½“ç„¶ä¹Ÿæœ‰å¯èƒ½æ˜¯å› ä¸ºæˆ‘MLå¤§ä½œä¸šå¸¦ä»–é£äº†ï¼‰ã€‚</li>
<li>éšæœºè¿‡ç¨‹ï¼šè®¡ç§‘çš„æœ€åä¸€é—¨æ•°ç†åŸºç¡€è¯¾ï¼Œè¿˜æ˜¯æ¯”è¾ƒæœ‰ç”¨çš„ï¼ŒMarkovè¿‡ç¨‹åœ¨AIæ–¹é¢çš„åº”ç”¨ç›¸å½“ä¹‹å¹¿æ³›ï¼Œç°åœ¨æ¥è§¦çš„å¾ˆå¤šLearning
Theoryéƒ½ä¸å¯é¿å…çš„éœ€è¦éšæœºè¿‡ç¨‹çš„çŸ¥è¯†å’Œæ–¹æ³•ï¼ŒæŒºåæ‚”æ²¡æœ‰å¥½å¥½å­¦ï¼Œä¸è¿‡æœŸæœ«é€Ÿæˆçš„ç»“æœä¹ŸæŒºæ»¡æ„ï¼Œè¿™æ ·çš„çŸ¥è¯†å¯†åº¦ï¼Œæ„Ÿè§‰ä¹Ÿä¸å¤ªéœ€è¦ä¸€ä¸ªå­¦æœŸçš„æ—¶é—´æ¥å­¦ä¹ ã€‚</li>
<li>çƒ­å­¦ï¼šæˆ‘çš„è¯„ä»·æ˜¯ï¼šå¯„ï¼Œè£…é€¼è£…äº†ä¸€ä¸ªå­¦æœŸï¼ŒæœŸæœ«è€ƒè¯•æå‰äº¤å·è¿˜è°ƒæˆåŠ©æ•™ï¼Œç»“æœå·é¢è®¡ç®—å…¨å´©ï¼Œå·®ç‚¹æŒ‚ç§‘ã€‚æ€»ç»“ä¹‹ï¼šå†åœ¨è¯¾ç¨‹ç¾¤è£…é€¼æˆ‘å°±æ˜¯å•¥bã€‚</li>
</ul></li>
</ul>
<h3 id="çæŠ˜è…¾">çæŠ˜è…¾</h3>
<p>å­¦æœŸä¸­è¿˜æ˜¯æŠ˜è…¾äº†ä¸€äº›æœ‰æ„æ€çš„ä¸œè¥¿çš„ï¼Œä¹Ÿæœ‰ä¸€äº›ä¸œè¥¿è®°å½•ä¸‹æ¥ï¼š</p>
<ul>
<li><p>ä¸€å¼€å§‹æ˜¯æƒ³å¼„<a
href="https://github.com/ageitgey/face_recognition">face_recognition</a>ï¼Œä¸€ç•ªæŠ˜è…¾ä¹‹åï¼Œä¸ºäº†ä½¿ç”¨CUDAï¼Œå¼„äº†WSLï¼Œä¸ºäº†ä½¿ç”¨WSLçš„GUIï¼Œå¼„äº†Windows
11ã€‚ç„¶è€Œä»å®‰è£…å¥½Windows
11ä¹‹ååˆ°ç°åœ¨å‡ ä¸ªæœˆï¼Œä¹Ÿæ²¡é‡æ–°å¼„èµ·æ¥face_recognitionã€‚è¿™é‡Œæœ‰ä¸€äº›è®°å½•ï¼š</p>
<ul>
<li><p>CUDAçš„å®‰è£…è¿‡ç¨‹ç—›è‹¦ä¸å ªï¼Œä»¥å›½å†…çš„ç½‘ç»œç¯å¢ƒï¼Œå„ç§æºç«™å´©çš„å´©ï¼Œæ…¢çš„æ…¢ï¼Œæ•´ä¸ªå®‰è£…è¿‡ç¨‹å±å®æŠ˜ç£¨ï¼Œé…ç½®ç¯å¢ƒæ•´ä¸ªè¿‡ç¨‹èŠ±è´¹äº†è¿‘ä¸€å‘¨æ—¶é—´ï¼Œä¸€ç›´åœ¨è·Ÿé­”å¹»çš„ç½‘ç»œbattleï¼Œä¹Ÿå› æ­¤æŠ˜è…¾äº†å„ç§å¥‡å¥‡æ€ªæ€ªçš„ä»£ç†æ–¹æ³•ï¼Œä»¥ä¸GFWæ–—æ™ºæ–—å‹‡ã€‚å¦‚æœæœ‰æœºä¼šå›é¡¾ä¸€éçš„è¯ï¼Œæˆ‘ä¼šå°†æ•´ä¸ªæŠ˜è…¾è¿‡ç¨‹è®°å½•ä¸‹æ¥ã€‚ï¼ˆå¸Œæœ›æ²¡æœ‰æœºä¼šå›é¡¾äº†ğŸ™ï¼‰</p></li>
<li><p>ä¸åŒUbuntuç‰ˆæœ¬çš„aptæºæ˜¯æœ‰åŒºåˆ«çš„ï¼Œå¦‚æœä¸ä¸€è‡´ï¼Œå°±ä¼šå¯¼è‡´æŠ¥é”™ï¼š</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">E: Unable to correct problems, you have held broken packages.</span><br></pre></td></tr></table></figure>
<p>ä¸åŒçš„æºåç§°å¯¹åº”å¦‚ä¸‹ï¼š</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Ubuntu 12.04 (LTS)ä»£å·ä¸ºpreciseã€‚</span><br><span class="line">Ubuntu 14.04 (LTS)ä»£å·ä¸ºtrustyã€‚</span><br><span class="line">Ubuntu 15.04 ä»£å·ä¸ºvividã€‚</span><br><span class="line">Ubuntu 15.10 ä»£å·ä¸ºwilyã€‚</span><br><span class="line">Ubuntu 16.04 (LTS)ä»£å·ä¸ºxenialã€‚</span><br><span class="line">Ubuntu 18.04 (LTS)ä»£å·ä¸ºbionicã€‚</span><br><span class="line">Ubuntu 20.04 (LTS)ä»£å·ä¸ºfocalã€‚</span><br></pre></td></tr></table></figure>
<p>å‚è€ƒèµ„æ–™ï¼š<a
href="https://blog.csdn.net/woshiheweigui/article/details/115557020">ubuntu20.04
apt å®‰è£…æŠ¥ E: Unable to correct problems, you have held broken
packages._woshiheweiguiçš„åšå®¢-CSDNåšå®¢</a></p></li>
</ul></li>
<li><p>æŠ˜è…¾æ¨¡å¼è¯†åˆ«å¤§ä½œä¸šçš„æ—¶å€™è®°å½•äº†è¿™äº›ä¸œè¥¿ï¼š</p>
<ul>
<li><p>å¯ä»¥ç”¨å¸ƒå°”æ•°ç»„æ¥å¯¹<code>ndarray</code>è¿›è¡Œæˆªå–ï¼Œå³ç»™å®šä¸€ä¸ªåŒç»´ç›¸åŒshapeçš„å¸ƒå°”æ•°ç»„ï¼Œå°†è¿™ä¸ªå¸ƒå°”æ•°ç»„ä½œä¸ºç´¢å¼•ï¼Œå°±å¯ä»¥å¾—åˆ°å¯¹åº”å¸ƒå°”å€¼ä¸ºçœŸçš„ä½ç½®çš„æ•°å­—æ„æˆçš„ä¸€ç»´æ•°ç»„ã€‚å®éªŒä¸­çš„ä¾‹å­ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">col = x_pca[label == <span class="number">1</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>è¿™é‡Œçš„<code>label</code>æ˜¯ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼Œä»…å«<code>0</code>å’Œ<code>1</code>ä¸¤ç§å…ƒç´ ã€‚æ•…<code>label == 1</code>åœ¨è¿™é‡Œä¼šè¿”å›ä¸€ä¸ªç›¸åŒå¤§å°çš„å¸ƒå°”æ•°ç»„ï¼Œ<code>1</code>å…ƒç´ å¯¹åº”ç´¢å¼•ä½ç½®çš„å…ƒç´ ä¸º<code>True</code>ï¼Œåä¹‹ä¸º<code>False</code>ã€‚</p>
<p><code>x_pca</code>æ˜¯ä¸€ä¸ªæ•°æ®é›†ï¼Œ<code>768*8</code>çš„<code>ndarray</code>ï¼Œå¦‚æ­¤æˆªå–ï¼Œå¯ä»¥å¾—åˆ°<code>x_pca</code>ä¸­æ‰€æœ‰è¾“å‡ºä¸º<code>1</code>çš„æ‰€å¯¹åº”çš„æ ·æœ¬ã€‚</p></li>
<li><p>åœ¨ <span class="math inline">\(k-Fold\)</span>
äº¤å‰éªŒè¯æ—¶ï¼Œæˆ‘ä»¬æ‰€ç”¨åˆ°çš„æ•°æ®æ˜¯<strong>è®­ç»ƒé›†ä¸­çš„å…¨éƒ¨æ•°æ®</strong>ï¼Œ<strong>ä¸åŒ…å«æµ‹è¯•é›†</strong></p></li>
<li><p><code>numpy.ravel()</code>å’Œ<code>numpy.flatten()</code>çš„åŒºåˆ«ï¼š<code>ravel()</code>è¿”å›çš„æ˜¯ä¸€ä¸ª<strong>è§†å›¾</strong>ï¼Œä¹Ÿå°±æ„å‘³ç€ï¼Œå¯¹å…¶çš„ä¿®æ”¹ï¼Œæ˜¯ä¼šå½±å“åˆ°åŸçŸ©é˜µçš„ï¼›ä½†<code>flatten()</code>è¿”å›çš„æ˜¯ä¸€ä¸ª<strong>æ‹·è´</strong>ï¼Œä¹Ÿå°±æ„å‘³ç€ï¼Œå¯¹å…¶çš„ä¿®æ”¹ï¼Œä¸ä¼šå½±å“åˆ°åŸçŸ©é˜µ</p></li>
</ul></li>
</ul>
<h3 id="ç§‘ç ”ç›¸å…³">ç§‘ç ”ç›¸å…³</h3>
<p>è¿™éƒ¨åˆ†ä¸»è¦æ˜¯å¯’å‡çš„å·¥ä½œäº†ï¼Œæ•´ä¸ªå¯’å‡åŸºæœ¬éƒ½äº¤ç»™äº†è¯»è®ºæ–‡ã€‚ä¸Šå­¦æœŸè™½ç„¶ï¼ˆåä¹‰ä¸Šï¼‰åŠ å…¥äº†è€å¸ˆçš„å®éªŒå®¤ï¼Œåšæ•°æ®éšç§å’Œå®‰å…¨ç›¸å…³å·¥ä½œï¼Œä½†æ˜¯ç”±äºè‡ªå·±è¯¾ç¨‹å¤ªå¤šï¼Œæ—¶é—´å¤ªç´§å¼ ï¼ˆå¤§å¤šäº¤ç»™äº†ç‹è€…è£è€€ï¼‰ï¼Œå¹¶æ²¡æœ‰èŠ±è´¹ä»€ä¹ˆç²¾åŠ›å»åšè¿™ä¸ªäº‹æƒ…ã€‚æœŸæœ«å’Œå›½å¤–çš„æ•™æˆè”ç³»å¯¹æ¥ä¹‹åï¼Œä¸€æ•´ä¸ªå¯’å‡éƒ½åœ¨è¯»è®ºæ–‡ï¼Œä¸»è¦é›†ä¸­åœ¨Learning
Theoryï¼Œç•¥å¾®çœ‹èµ·æ¥åƒapplied techä¸€äº›çš„ï¼Œä¹Ÿéƒ½æ˜¯å¾ˆç†è®ºçš„Graph
Learningé—®é¢˜ã€‚åˆšå¼€å§‹è¯»è®ºæ–‡çš„æ—¶å€™æœ‰ä¸€äº›è®°å½•ï¼Œåæ¥é€æ¸é­”å¹»ï¼Œå°±æ‘†çƒ‚äº†ï¼Œæ²¡æœ‰å†å°†æ–°çš„æ¦‚å¿µä»¥ç¬”è®°çš„å½¢å¼è®°å½•ä¸‹æ¥ã€‚è¯»ç¬¬ä¸€ç¯‡è®ºæ–‡çš„æ—¶å€™è®°å½•äº†è¿™æ ·å‡ ä¸ªNotation/Theoremï¼š</p>
<ul>
<li><p><span class="math inline">\(\tilde{O}(g(n))\)</span>ï¼š</p>
<p>stands for <span
class="math inline">\(O(g(n)\log{g(n)})\)</span>.</p></li>
<li><p>Union Bound: <span class="math display">\[
\mathbb{P}[\mathop{\cup}\limits_{i}A_i]\leq \sum_i\mathbb{P}[A_i]
\]</span></p></li>
<li><p>Hoeffding's Inequality:</p>
<p>Let <span class="math inline">\(Z_1,\dots,Z_n\)</span> be independent
bounded random variables with <span
class="math inline">\(Z_i\in[a,b]\)</span> for all <span
class="math inline">\(i\)</span>, where <span
class="math inline">\(-\infty&lt;a\le b&lt;\infty\)</span>. Then <span
class="math display">\[
\mathbb{P}\left(\frac{1}{n}\sum_{i=1}^{n}(Z_i-\mathbb{E}[Z_i])\ge
t\right)\le\exp\left(-\frac{2nt^2}{(b-a)^2}\right)
\]</span> and <span class="math display">\[
\mathbb{P}\left(\frac{1}{n}\sum_{i=1}^{n}(Z_i-\mathbb{E}[Z_i])\le
-t\right)\ge\exp\left(-\frac{2nt^2}{(b-a)^2}\right)
\]</span> for all <span class="math inline">\(t\ge 0\)</span></p></li>
</ul>
<p>ç°åœ¨åœ¨ç­‰ç€é€‰æ‹©ä¸€ä¸ªç¡®å®šçš„æ–¹å‘ï¼Œæ¥æ­£å¼å¼€å§‹è‡ªå·±çš„æœ¬ç§‘ç§‘ç ”ã€‚å¸Œæœ›èƒ½åœ¨æœªæ¥åŠå¹´å†…åšå‡ºä¸€äº›æˆæœå§ï¼Œè¦ä¸ç„¶å±å®æ²¡æœ‰å¤§å­¦è¯»äº†â€¦â€¦</p>
<h2 id="å­¦ä¹ ä¹‹å¤–">å­¦ä¹ ä¹‹å¤–</h2>
<p>åŠå¹´å¤šä»¥æ¥ç»å†äº†å¾ˆå¤šäº‹æƒ…ï¼Œä¹Ÿæœ‰å¾ˆå¤šæˆ–æ·±åˆ»æˆ–å¥‡æ€ªçš„æ€è€ƒï¼Œéš¾ä»¥æ€»ç»“ä¹‹ï¼Œä¹Ÿä¸é€‚åˆä½œä¸ºåšæ–‡æ”¾ä¸Šæ¥ï¼Œåœ¨æ­¤å°±æ‘˜è‡ªå·±çš„è·¨å¹´æ€»ç»“ï¼Œæ¥æ›¿ä»£ä¸€ä¸‹å§ã€‚</p>
<h3 id="è·¨å¹´æ€»ç»“">è·¨å¹´æ€»ç»“</h3>
<p>ï¼ˆå†™äº2021å¹´12æœˆåº•ï¼Œåœ¨ç©ºé—´è·¨å¹´çš„æ—¶å€™å‘å‡ºï¼‰</p>
<p>å¼€å§‹å†™çš„æ—¶å€™çªç„¶æƒŠè§‰ï¼Œä¸Šä¸€æ¬¡åœ¨ç©ºé—´å†™è¿™ç§é•¿ç¯‡çš„ï¼Œå›å¿†æ€»ç»“æ€§çš„æ–‡å­—ï¼Œç«Ÿæ˜¯ä¸¤å¹´å‰çš„è·¨å¹´å¤œã€‚ä¸çŸ¥ä¸è§‰ï¼Œå·²ç»700å¤šå¤©äº†ã€‚</p>
<p>æ›¾äºä¸¤å¹´å‰å®šä¸‹çš„ç›®æ ‡å’Œè®¡åˆ’ï¼Œå› ä¸ºè‡ªå·±ï¼Œå› ä¸ºå…¶ä»–çš„ä¸€äº›äººå’Œä¸€äº›äº‹ï¼Œæ€»ä¸å¦‚ä»Šçš„äº‹å®æœ‰æ‰€åå·®ã€‚å¥½åœ¨ï¼Œåçš„å¹¶ä¸ç®—ç‰¹åˆ«å¤šã€‚</p>
<p>å»å¹´çš„è·¨å¹´å¤œï¼Œæˆ‘å¤„åœ¨å‰æ‰€æœªæœ‰ï¼Œå¦‚ä»Šä¹Ÿæœªå†å¤ç°è¿‡çš„æ»¡è¶³ä¸­ï¼Œé‚£æ˜¯ä»Šå¤©ä¸ºæ­¢ï¼Œæœ€æ¥è¿‘è‡ªå·±å¸Œå†€çš„çŠ¶æ€çš„æ—¶åˆ»ã€‚å½¼æ—¶å½¼åˆ»ï¼Œå¹¸ç¦æ„Ÿå‹å€’äº†ä¸€åˆ‡ï¼Œè¿æ€è€ƒéƒ½å˜å¾—ä¸å†å¿…è¦ï¼Œä¹Ÿå°±å¹¶æ— è‡ªæˆ‘çš„æ€»ç»“ä¸è®°å½•ã€‚
å¦‚ä»Šå¦‚æ­¤ï¼Œä»é‚£èˆ¬æ¢¦å¢ƒå è½ä¹‹åï¼Œåˆç»å†äº†æ›´å¤šçš„æƒ…èŠ‚æ•…äº‹ï¼Œè¿™ä¸­é—´çš„æ€è€ƒå’Œè‡ªçœï¼Œä¹Ÿå€¼å¾—è¢«è®°å½•å†™ä¸‹ã€‚</p>
<p>ä¸¤å¹´ï¼Œ18å²ï¼Œ19å²ã€‚</p>
<p>æˆå¹´ä¹‹åçš„ä¸€æ®µæ—¶é—´ï¼Œé‡è§çš„äººï¼Œç»å†çš„äº‹ï¼Œæ‰€æ–½äºæˆ‘çš„å½±å“ï¼ŒçŠ¹å¦‚è„±èƒæ¢éª¨ä¸€èˆ¬ã€‚å¦‚ä»Šå›æœ›
ï¼Œè‡ªå·±å·²è¿œä¸æ˜¯æ›¾ç»é‚£æ ·äº†ã€‚</p>
<p>ä¸ä¸€äº›äººä»è¿œåˆ°è¿‘ï¼Œä¸ä¸€äº›äººä»è¿‘åˆ°è¿œï¼Œç†™ç†™æ”˜æ”˜ï¼Œæ“¦è‚©è€Œè¿‡ã€‚ä¸å¹¸åœ°å¤±å»äº†ä¸€äº›äººï¼Œä¹Ÿå¹¸è¿åœ°ç•™ä¸‹äº†ï¼Œå‘ç°äº†ä¸€äº›äººã€‚ç›¸è¯†ï¼Œç›¸ç†Ÿï¼Œç›¸çˆ±ï¼Œç›¸åˆ«ï¼Œä¸°å¯Œä¹‹æ•…äº‹ï¼Œå¤æ‚ä¹‹æƒ…æ„Ÿï¼Œäº¤ç»‡ç»„æˆäº†è¿™ç›¸å½“é­”å¹»çš„ä¸€å¹´å¤šã€‚</p>
<p>ç”±äºè‡ªå·±å¯¹äº‹ä»¶æ—¶é—´ç²¾ç¡®çš„è®°å¿†ï¼Œæ¯ä¸€æ®µç‹¬ç‰¹çš„æ•…äº‹ï¼Œéƒ½æ·±åˆ»äºè„‘ä¸­ï¼Œä»¥çœ‹èµ·æ¥æ°¸ä¸æ¶ˆé€çš„å½¢å¼å­˜åœ¨ç€ï¼Œæ¯æ²‰äºå…¶ä¸­ï¼Œæ€»éš¾ä»¥è‡ªæ‹”ã€‚</p>
<p>â€œæººäºå¯å¿†è€Œä¸å¯è¿½çš„è¿‡å»ï¼Œ æ‰§äºå¯çŸ¥è€Œä¸å¯æ±‚çš„æ¢¦å¢ƒã€‚â€</p>
<p>æœ‰è¿‡ç”œèœœï¼Œä»¥è‡³äºçœ‹åˆ°äº†æœªæ¥çš„å°½å¤´ï¼Œæ²‰äºå…¶ä¸­ï¼Œä¼¼ä¹ä¸€åˆ‡åŸæœ¬çš„è‹¦æ¶©éƒ½æ¶ˆä¹‹æ— å½¢ï¼›æœ‰è¿‡å è½ï¼Œä»¿ä½›æ‰€æœ‰ç”Ÿæ´»çš„æ”¯æŸ±éƒ½ç¦»æˆ‘è€Œå»ï¼Œæœªæ¥çš„å¯èƒ½ä¹Ÿä¸å†æœ‰å¸å¼•åŠ›ï¼Œç”Ÿå‘½å‚äºå²Œå²Œå¯å±çš„ä¸€å¿µä¹‹é—´ï¼›ä¹Ÿæœ‰ç°åœ¨çš„é‡Šç„¶å’Œå¹³æ·¡ï¼Œå³å·²å¦‚ä»Šå¦‚æ­¤ï¼Œä¸å¦¨æ”¾ä¸‹ä¸€åˆ‡æœŸå¾…å’Œæ„¿æ™¯ï¼Œæ‘†è„±æ‰€æœ‰è‡ªæˆ‘é™åˆ¶çš„æ¡æ¢ï¼Œäº«å—æ¯ä¸€ä¸ªç‹¬ç‰¹çš„å½“ä¸‹ã€‚</p>
<p>æ›¾è¢«é—®åˆ°ï¼Œå¦‚æœæœ‰æœºä¼šå¿˜æ‰ä¸€åˆ‡ï¼Œå›åˆ°å‡ å¹´å‰ï¼Œä¼šæ„¿æ„å—ï¼Ÿ</p>
<p>æˆ‘æ–©é’‰æˆªé“çš„ç­”æ¡ˆæ˜¯ï¼Œä¸æ„¿æ„ã€‚è™½ç„¶æœ‰ä¸æ»¡ï¼Œæœ‰å›°éš¾ï¼Œæœ‰è¿™æ ·é‚£æ ·çš„æŒ«æŠ˜å’Œé—æ†¾ï¼Œä½†å¿…é¡»è¦æ‰¿è®¤ï¼Œè¿™æ®µç»å†æ˜¯ä¸°å¯Œçš„ï¼Œæ˜¯ç²¾å½©çš„ï¼Œæ˜¯æˆ‘ä¹‹æ‰€ä»¥æˆä¸ºæˆ‘çš„å¿…è¦ç¼˜ç”±ã€‚</p>
<p>â€œæˆ‘æœ€è¿‘ç»å†çš„ï¼Œå¯çœŸæœ‰ç‚¹å¤šäº†â€</p>
<p>â€œè¿™ç®—æ˜¯æˆå¹´ä»ªå¼å—â€</p>
<p>â€œå¦‚æœæ˜¯ï¼Œé‚£å®ƒçš„æ•ˆæœå¯çœŸå¥½â€</p>
<p>æ—¶å…‰èè‹’ï¼Œå¦‚ä»Šçš„æˆ‘ï¼Œå·²ä¸å†ä¼šå¦‚ä¸¤å¹´å‰ä¸€æ ·ï¼Œå†™ä¸‹å…·ä½“è€Œç»†è‡´çš„æœŸæœ›å’Œæ„¿æ™¯ã€‚å”¯æ„¿ï¼Œå¹³å®‰å–œä¹ï¼Œä¸‡äº‹èƒœæ„ã€‚</p>
<h3 id="æ„¿æ™¯">æ„¿æ™¯</h3>
<p>å€’æ²¡ä»€ä¹ˆç‰¹åˆ«çš„æœŸæœ›ï¼Œå°±å¸Œæœ›ä¸€åˆ‡é¡ºåˆ©å§ï¼Œç§‘ç ”ï¼Œå­¦ä¸šï¼Œèº«ä½“ï¼Œæƒ…ç»ªã€‚</p>
<p>éº»äº†ï¼Œå¼€å¤´è¿˜è¯´æƒ³è®©è‡ªå·±çš„åšå®¢æ˜¯çº¯æŠ€æœ¯å‘æ¥ç€ï¼Œç»“æœæœ€åè¿˜æ˜¯å†™äº†è¿™äº›é…¸é…¸çš„æ–‡å­—ã€‚å°±å†™åˆ°è¿™é‡Œç½¢ã€‚</p>
]]></content>
      <tags>
        <tag>Thoughts</tag>
      </tags>
  </entry>
  <entry>
    <title>LinuxæœåŠ¡ç«¯ç¨‹åºçš„ä¸€æ¬¡å°è¯•</title>
    <url>/2021/08/28/Linux%E6%9C%8D%E5%8A%A1%E7%AB%AF%E7%A8%8B%E5%BA%8F%E7%9A%84%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95/</url>
    <content><![CDATA[<h1 id="æœåŠ¡å™¨ç«¯ç¨‹åºè®¾è®¡æ–‡æ¡£">æœåŠ¡å™¨ç«¯ç¨‹åºè®¾è®¡æ–‡æ¡£</h1>
<h2 id="ç³»ç»Ÿæ¦‚è¿°">ç³»ç»Ÿæ¦‚è¿°</h2>
<p>ä¸ºäº†å®ç°å¹¿åŸŸç½‘äº”å­æ£‹è¿æ¥å¯¹æˆ˜ï¼Œå³åœ¨ä¸åŒå±€åŸŸç½‘ä¸‹çš„æœºå™¨äº’ç›¸è¿æ¥ï¼Œä¸”åœ¨ä¸è€ƒè™‘å†…ç½‘ç©¿é€ç­‰å®ç°æ‰‹æ®µçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæœåŠ¡å™¨ç«¯ï¼Œç”¨äºå¯¹æˆ˜åŒæ–¹æ•°æ®çš„è½¬å‘ï¼ŒæœåŠ¡å™¨ç«¯ç¨‹åºéƒ¨ç½²äºäº‘æœåŠ¡å™¨ï¼Œç”¨äºç¤ºä¾‹çš„æœåŠ¡å™¨IPåœ°å€ä¸º<code>101.34.252.176</code>ï¼ŒæœåŠ¡æä¾›å•†ä¸º<a
href="https://cloud.tencent.com/">è…¾è®¯äº‘</a>ã€‚</p>
<h2 id="è¿è¡Œç¯å¢ƒ">è¿è¡Œç¯å¢ƒ</h2>
<p>ç”¨äºç¤ºä¾‹çš„äº‘æœåŠ¡å™¨ç³»ç»Ÿç¯å¢ƒä¸º<code>CentOS 7.6 64bit</code></p>
<h2 id="åŠŸèƒ½è®¾è®¡">åŠŸèƒ½è®¾è®¡</h2>
<p>åœ¨æœåŠ¡å™¨ç«¯ç¨‹åºå¼€å¯çš„æƒ…å†µä¸‹ï¼Œä¼šç›‘å¬ä¸‰ä¸ªç«¯å£ï¼Œ<code>CONF_PORT</code>ï¼Œ<code>HOST_PORT</code>ï¼Œ<code>GUEST_PORT</code>ã€‚å…¶ä¸­ï¼Œ<code>CONF_PORT</code>ç”¨äºæ¥æ”¶å®¢æˆ·ç«¯ç¨‹åºçš„ç”³è¯·ï¼Œå¦å¤–ä¸¤ä¸ªç«¯å£ç”¨äºå»ºç«‹è¿æ¥ï¼Œå®ç°ä¿¡æ¯çš„è½¬å‘ã€‚å…·ä½“è¿‡ç¨‹å¦‚ä¸‹ï¼š</p>
<ol type="1">
<li>å®¢æˆ·ç«¯å‘æœåŠ¡å™¨ç«¯å‘å‡ºç”³è¯·ï¼Œè¿æ¥åˆ°<code>CONF_PORT</code>ã€‚</li>
<li>æœåŠ¡å™¨ç«¯å‘<code>CONF_PORT</code>å‘é€å‡ºå½“å‰çš„æ¸¸æˆçŠ¶æ€<code>Game_Status</code>ï¼Œåˆ†ä¸º<code>VACANT</code>ï¼Œ<code>WAITING</code>ï¼Œ<code>ONGOING</code>ï¼Œåˆ†åˆ«è¡¨ç¤ºç©ºé—²ï¼Œå·²æœ‰ç©å®¶ç­‰å¾…ä¸­ï¼Œä»¥åŠæ¸¸æˆè¿›è¡Œä¸­ã€‚
<ul>
<li>è‹¥å½“å‰ä¸ºç©ºé—²ï¼Œåˆ™å½“å‰ç”³è¯·çš„å®¢æˆ·ç«¯å°†ä¸<code>HOST_PORT</code>å»ºç«‹è¿æ¥ï¼Œç­‰å¾…å¦ä¸€ä½ç©å®¶è¿›å…¥æ¸¸æˆï¼Œå¹¶å°†æ¸¸æˆçŠ¶æ€<code>Game_Status</code>ä¿®æ”¹ä¸º<code>WAITING</code>ã€‚</li>
<li>è‹¥å½“å‰ä¸ºç­‰å¾…ï¼Œåˆ™å½“å‰ç”³è¯·çš„å®¢æˆ·ç«¯å°†ä¸<code>GUEST_PORT</code>å»ºç«‹è¿æ¥ï¼Œå¼€å§‹æ¸¸æˆï¼Œå¹¶å°†å¹¶å°†æ¸¸æˆçŠ¶æ€<code>Game_Status</code>ä¿®æ”¹ä¸º<code>ONGOING</code>ã€‚</li>
<li>è‹¥å½“å‰ä¸ºæ¸¸æˆä¸­ï¼Œåˆ™æ–­å¼€è¿æ¥ï¼Œå¹¶è¿”å›ä¿¡æ¯ã€‚</li>
</ul></li>
<li>å½“åŒæ–¹ç©å®¶éƒ½åˆ†åˆ«å»ºç«‹è¿æ¥åï¼Œå¼€å§‹è¿›è¡Œå¯¹æˆ˜ï¼ŒæœåŠ¡å™¨åŒæ—¶ç›‘å¬åŒæ–¹çš„ä¿¡æ¯ï¼Œå¹¶ä¸ä½œä¿®æ”¹åœ°è½¬å‘ã€‚</li>
<li>å½“ä¸€å±€æ¸¸æˆç»“æŸä¹‹åï¼Œç©å®¶æ–­å¼€è¿æ¥ï¼Œå°†æ¸¸æˆçŠ¶æ€<code>Game_Status</code>ä¿®æ”¹ä¸º<code>VACANT</code>ï¼Œå¹¶ä¸”è¿˜åŸä¸ºæœ€åˆç›‘å¬çŠ¶æ€ã€‚</li>
</ol>
<h2 id="å…³é”®ç®—æ³•è®¾è®¡">å…³é”®ç®—æ³•è®¾è®¡</h2>
<h3 id="socket">Socket</h3>
<p>ç”±äºéœ€è¦å®ç°ä¸å®¢æˆ·ç«¯çš„ä¿¡æ¯äº¤äº’ï¼Œå› æ­¤ä½¿ç”¨<code>Socket</code>è¿›è¡Œä¿¡æ¯çš„ä¼ é€’ã€‚ä¸ºäº†é¿å…ä¿¡æ¯çš„ä¸¢å¤±ï¼Œä¿è¯æ¶ˆæ¯ä¼ é€’çš„å¯é ï¼Œä½¿ç”¨<code>TCP</code>åè®®ï¼Œæµå¼ä¼ è¾“ä¿¡æ¯ã€‚</p>
<p>åˆ†åˆ«åœ¨ä¸åŒå­è¿›ç¨‹ä¸­ä½¿ç”¨<code>Socket</code>ï¼Œå’Œä¸åŒç«¯å£è¿›è¡Œæ¶ˆæ¯çš„æ¥æ”¶ä¸å‘é€ã€‚ä»¥<code>HOST_PORT</code>ä¸ºä¾‹ï¼Œä¸»è¦ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š</p>
<ol type="1">
<li><p>ç”³è¯·<code>Socket</code>ï¼ˆ<code>fd</code>è¡¨ç¤ºæ–‡ä»¶æè¿°ç¬¦ï¼Œç”¨äºæ¥æ”¶<code>socket()</code>å‡½æ•°çš„è¿”å›å€¼ï¼‰</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> socket_fd_Guest = socket(AF_INET, SOCK_STREAM, <span class="number">0</span>);<span class="comment">//Apply for a socket</span></span><br></pre></td></tr></table></figure></li>
<li><p>æœ¬åœ°åœ°å€åˆå§‹åŒ–åŠç»‘å®šSocketï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span>  <span class="title">servaddr_Host</span>;</span></span><br><span class="line"><span class="built_in">memset</span>(&amp;servaddr_Host, <span class="number">0</span>, <span class="keyword">sizeof</span>(servaddr_Host));</span><br><span class="line">servaddr_Host.sin_family = AF_INET;</span><br><span class="line">servaddr_Host.sin_addr.s_addr = htonl(INADDR_ANY);	<span class="comment">//Listen to all Addresses</span></span><br><span class="line">servaddr_Host.sin_port = htons(HOST_PORT);			<span class="comment">//Set listening port as default</span></span><br><span class="line">bind(socket_fd_Host, (struct sockaddr*)&amp;servaddr_Host, <span class="keyword">sizeof</span>(servaddr_Host)ï¼›<span class="comment">//Bind</span></span><br></pre></td></tr></table></figure>
<p>å°†Socketè®¾ç½®ä¸ºå¯¹æ‰€æœ‰IPåœ°å€è¿›è¡Œç›‘å¬ã€‚</p></li>
<li><p>åœ¨å­è¿›ç¨‹ä¸­å¼€å¯å¯¹äºç”³è¯·çš„ç›‘å¬ï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">listen(socket_fd_Host, <span class="number">10</span>);</span><br><span class="line"><span class="keyword">int</span> connect_fd_Host = accept(socket_fd_Host, (struct sockaddr*)<span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br></pre></td></tr></table></figure>
<p>æ–°å»ºä¸€ä¸ªæè¿°ç¬¦<code>connnect_fd_Host</code>ç”¨äºæ¥æ”¶<code>accept()</code>ä¼ é€’å›çš„Socketæè¿°ç¬¦ã€‚</p></li>
<li><p>å¯¹æ¶ˆæ¯çš„è¯»å–å’Œå‘é€ï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">recv(connect_fd_Guest, buff_H, <span class="number">4096</span>, <span class="number">0</span>);</span><br><span class="line">send(connect_fd_Guest, buff_G, length, <span class="number">0</span>);</span><br></pre></td></tr></table></figure>
<p>å…¶ä¸­<code>buff_H</code>å’Œ<code>buff_G</code>åˆ†åˆ«æ˜¯æ¥æ”¶å’Œå‘é€æ¶ˆæ¯çš„ç¼“å†²åŒºï¼Œä¸º<code>char*</code>ç±»å‹ã€‚</p></li>
</ol>
<h3 id="å¤šè¿›ç¨‹">å¤šè¿›ç¨‹</h3>
<p>ä¸ºäº†å®ç°å¯¹ä¸‰ä¸ªç«¯å£çš„åŒæ—¶ç›‘å¬ï¼Œä»¥åŠäº’ä¸å¹²æ‰°çš„å¹¶è¡Œè¿è¡Œï¼Œé€‰æ‹©ä½¿ç”¨<code>MultiProcess</code>å®ç°è¿™é¡¹åŠŸèƒ½ã€‚å½“æœåŠ¡å™¨å¤„äºæ­£å¸¸ç›‘å¬çŠ¶æ€æ—¶ï¼Œè¿›ç¨‹æ ‘å¦‚ä¸‹ï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">Server(<span class="number">4257</span>)â”€â”¬â”€Server(<span class="number">15329</span>)<span class="comment">//Listening to CONF_PORT</span></span><br><span class="line">             â”œâ”€Server(<span class="number">15330</span>)<span class="comment">//Listening to HOST_PORT</span></span><br><span class="line">             â””â”€Server(<span class="number">15331</span>)<span class="comment">//Listening to GUEST_PORT</span></span><br></pre></td></tr></table></figure>
<p>å¦‚å›¾ï¼Œä¸»è¿›ç¨‹<code>Serverï¼ˆ4257ï¼‰</code>æœ‰ä¸‰ä¸ªå­è¿›ç¨‹ï¼Œåˆ†åˆ«å®ç°å¯¹äºä¸‰ä¸ªç«¯å£çš„ç›‘å¬ã€‚</p>
<p>å½“<code>CONF_PORT</code>æ”¶åˆ°æ¥è‡ªå®¢æˆ·ç«¯çš„ç”³è¯·æ—¶ï¼Œä¼šä»<code>Game_Status</code>æ–‡ä»¶è·å–å½“å‰çš„æ¸¸æˆçŠ¶æ€ï¼Œå¹¶å°†å…¶å‘é€ç»™å‘å‡ºç”³è¯·çš„å®¢æˆ·ç«¯ã€‚</p>
<p>å½“æ¸¸æˆæˆåŠŸå»ºç«‹çš„æ—¶å€™ï¼Œç›‘å¬<code>HOST_PORT</code>å’Œ<code>GUEST_PORT</code>çš„ä¸¤ä¸ªè¿›ç¨‹å°†åˆ†åˆ«<code>fork()</code>ä¸€ä¸ªå­è¿›ç¨‹ï¼Œç”¨äºå‘å¯¹åº”ç«¯å£å‘é€ä¿¡æ¯ï¼Œå®ç°å‘é€å’Œç›‘å¬çš„åˆ†ç¦»ã€‚åœ¨è¿™ç§çŠ¶æ€ä¸‹ï¼Œè¿›ç¨‹æ ‘å¦‚å›¾æ‰€ç¤ºï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">Server(<span class="number">4257</span>)â”€â”¬â”€Server(<span class="number">15329</span>)<span class="comment">//Listening to CONF_PORT</span></span><br><span class="line">             â”œâ”€Server(<span class="number">15330</span>)-Server(<span class="number">15897</span>)<span class="comment">//Listening to HOST_PORT</span></span><br><span class="line">             â””â”€Server(<span class="number">15331</span>)-Server(<span class="number">16372</span>)<span class="comment">//Listening to GUEST_PORT</span></span><br></pre></td></tr></table></figure>
<h3 id="è¿›ç¨‹é—´é€šä¿¡">è¿›ç¨‹é—´é€šä¿¡</h3>
<p>ç”±äºç¨‹åºæ¶‰åŠåˆ°å¤šä¸ªè¿›ç¨‹ï¼Œè€Œä¸åŒè¿›ç¨‹ä¹‹é—´çš„å˜é‡æ˜¯ä¸å…±äº«çš„ï¼Œä¸”ä¸åŒè¿›ç¨‹ä¹‹é—´æœ‰é€šä¿¡çš„éœ€æ±‚ï¼Œå› æ­¤éœ€è¦ä½¿ç”¨ä¸€äº›æ–¹æ³•ï¼Œå®ç°è¿›ç¨‹ä¹‹é—´çš„é€šä¿¡ã€‚è¿™é‡Œä¸»è¦ç”¨äº†ä¸‰ç§æ–¹æ³•ï¼šå…±äº«æ–‡ä»¶ï¼Œç®¡é“é€šä¿¡ï¼Œä»¥åŠä¿¡å·æœºåˆ¶ã€‚</p>
<h4 id="å…±äº«æ–‡ä»¶">å…±äº«æ–‡ä»¶</h4>
<p>å…³äºæ¸¸æˆçŠ¶æ€<code>Game_Status</code>çš„å­˜å‚¨å’Œè¯»å–ï¼Œé€‰æ‹©ä½¿ç”¨æ–‡ä»¶ï¼Œåœ¨ç›®å½•ä¸‹å»ºç«‹ä¸€ä¸ª<code>Game_Status</code>æ–‡ä»¶ï¼Œç”¨äºå­˜å‚¨å½“å‰çš„æ¸¸æˆçŠ¶æ€ã€‚æœåŠ¡å™¨ç¨‹åºæ¯æ¬¡è¿è¡Œçš„æ—¶å€™ï¼Œéƒ½ä¼šå°†å…¶åˆå§‹åŒ–ä¸º<code>VACANT</code>ï¼Œä»¥ä¾›åç»­è¿›ç¨‹è¯»å–å’Œä¿®æ”¹ã€‚</p>
<p>è¯»å–è¿‡ç¨‹å¦‚ä»£ç æ‰€ç¤ºï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">Game_Status_fd = open(<span class="string">&quot;Game_Status&quot;</span>, O_RDONLY);</span><br><span class="line">read(Game_Status_fd, &amp;Game_Status, <span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">close(Game_Status_fd);</span><br></pre></td></tr></table></figure>
<p>å…¶ä¸­<code>Game_Status_fd</code>å’Œ<code>Game_Status</code>å‡ä¸º<code>int</code>å‹å˜é‡ï¼Œ<code>Game_Status_fd</code>ç”¨ä½œæ–‡ä»¶æè¿°ç¬¦ï¼Œ<code>Game_Status</code>ç”¨äºè¯»å–æ–‡ä»¶ä¸­å­˜å‚¨çš„å•ä¸ª<code>int</code>å‹æ•°æ®ï¼Œè¡¨ç¤ºä¸åŒçŠ¶æ€ï¼Œå¦‚ä¸‹ï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VACANT 0</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> WAITING 1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ONGOING 2</span></span><br></pre></td></tr></table></figure>
<p>å½“<code>HOST</code>ç©å®¶å»ºç«‹è¿æ¥æ—¶ï¼Œå°†ä¼šå¯¹<code>Game_Status</code>æ–‡ä»¶è¿›è¡Œä¿®æ”¹ï¼Œä¿®æ”¹è¿‡ç¨‹å¦‚ä»£ç æ‰€ç¤ºï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">Game_Status = WAITING;</span><br><span class="line">Game_Status_fd = open(<span class="string">&quot;Game_Status&quot;</span>, O_WRONLY);</span><br><span class="line">write(Game_Status_fd, &amp;Game_Status, <span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">close(Game_Status_fd);</span><br></pre></td></tr></table></figure>
<p>ç”±æ­¤ï¼Œå¯ä»¥å®ç°å¯¹æ¸¸æˆçŠ¶æ€çš„å…±äº«è®¿å­˜ã€‚</p>
<h4 id="ç®¡é“é€šä¿¡">ç®¡é“é€šä¿¡</h4>
<p>åœ¨åŒæ–¹éƒ½å»ºç«‹è¿æ¥æ—¶ï¼Œæ€»å…±æœ‰å››ä¸ªè¿›ç¨‹ï¼Œåˆ†åˆ«å®ç°åŒæ–¹æ¶ˆæ¯çš„æ¥æ”¶å’Œå‘é€ï¼Œè¿™å°±éœ€è¦åœ¨ä¸åŒè¿›ç¨‹ä¹‹é—´äº’ç›¸ä¼ é€’ä¿¡æ¯ï¼Œæ‰èƒ½å®ç°æ¶ˆæ¯çš„å®æ—¶è½¬å‘ï¼Œè¿™é‡Œé€‰æ‹©ä½¿ç”¨åŒ¿åç®¡é“<code>pipe()</code>æ¥å®ç°æ­¤é¡¹åŠŸèƒ½ã€‚</p>
<ol type="1">
<li><p>ä¸»è¿›ç¨‹ä¸­åˆå§‹åŒ–ä¸¤ä¸ªç®¡é“ï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(pipe(fd_1) == <span class="number">-1</span>)</span><br><span class="line">&#123;</span><br><span class="line">    perror(<span class="string">&quot;pipe&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> *write_H2G = &amp;fd_1[<span class="number">1</span>];</span><br><span class="line"><span class="keyword">int</span> *read_H2G = &amp;fd_1[<span class="number">0</span>];<span class="comment">//H2G:Host to Guest</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(pipe(fd_2) == <span class="number">-1</span>)</span><br><span class="line">&#123;</span><br><span class="line">    perror(<span class="string">&quot;pipe&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> *write_G2H = &amp;fd_2[<span class="number">1</span>];</span><br><span class="line"><span class="keyword">int</span> *read_G2H = &amp;fd_2[<span class="number">0</span>];<span class="comment">//G2H:Guest to Host</span></span><br></pre></td></tr></table></figure>
<p>å…¶ä¸­<code>fd_1</code>å’Œ<code>fd_2</code>å‡ä¸º<code>int[2]</code>æ•°ç»„ï¼Œè€Œ<code>write_H2G</code>ï¼Œ<code>read_H2G</code>ï¼Œ<code>write_G2H</code>ï¼Œ<code>read_G2H</code>æ˜¯ä¸ºäº†åé¢è°ƒç”¨æ–¹ä¾¿è€Œåˆå§‹åŒ–çš„åˆ«åã€‚</p></li>
<li><p>åˆå§‹åŒ–ç®¡é“ä¹‹åï¼Œä¸»è¿›ç¨‹åˆ†åˆ«<code>fork()</code>ä¸¤ä¸ªå­è¿›ç¨‹ç”¨äºå®ç°ä¿¡æ¯äº¤æµï¼Œå¹¶åœ¨<code>fork()</code>ç»“æŸä¹‹åå…³é—­è‡ªå·±çš„å››ä¸ªè¯»å†™ç«¯å£ï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">close(*write_H2G);</span><br><span class="line">close(*read_G2H);</span><br><span class="line">close(*write_G2H);</span><br><span class="line">close(*read_H2G);</span><br></pre></td></tr></table></figure>
<p>ä¸¤ä¸ªå­è¿›ç¨‹åˆ†åˆ«å…³é—­ä¸å¯¹æ–¹å¯¹åº”çš„è¯»å†™ç«¯å£ï¼š</p>
<ul>
<li><p><code>Host</code>è¿›ç¨‹ï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">close(*write_G2H);</span><br><span class="line">close(*read_H2G);</span><br></pre></td></tr></table></figure></li>
<li><p><code>Client</code>è¿›ç¨‹ï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">close(*write_H2G);</span><br><span class="line">close(*read_G2H);</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><p>è¿™æ ·å®ç°æ¶ˆæ¯çš„å®æ—¶è½¬å‘ï¼ˆä»¥<code>Host</code>è¿›ç¨‹ä¸ºä¾‹ï¼‰ï¼š</p>
<ul>
<li><p>æ¯å½“Socketä¼ æ¥æ¶ˆæ¯çš„æ—¶å€™ï¼Œéƒ½å°†æ¶ˆæ¯ä¸ä½œæ”¹åŠ¨åœ°ä¼ é€’è‡³ç®¡é“çš„å†™ç«¯ï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">    n = recv(connect_fd_Host, buff_H, <span class="number">4096</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">-1</span> || n == <span class="number">0</span>)</span><br><span class="line">    &#123;<span class="comment">//when connexion shut down</span></span><br><span class="line">        kill(My_Child, SIGKILL);</span><br><span class="line">        close(connect_fd_Host);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    write(*write_H2G, buff_H, n);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>æ¯å½“æ²¡æœ‰æ–°æ¶ˆæ¯æ—¶ï¼Œä¾¿é˜»å¡äº<code>recv()</code>å‡½æ•°å¤„ã€‚</p></li>
<li><p>æ–°å¼€ä¸€ä¸ªå­è¿›ç¨‹ï¼Œå®ç°ï¼šæ¯å½“ç®¡é“å¦ä¸€ç«¯æœ‰æ–°æ¶ˆæ¯ä¼ é€’æ¥ï¼Œå°†æ¶ˆæ¯ä¸åšæ”¹åŠ¨åœ°ç»æœ‰Socketå‘é€å‡ºå»ï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">    length = read(*read_G2H, buff_G, <span class="number">4096</span>);</span><br><span class="line">    send(connect_fd_Host, buff_G, length, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ol>
<p>ç”±æ­¤ï¼Œå¯ä»¥å®ç°æ¶ˆæ¯çš„å®æ—¶è½¬å‘</p>
<h4 id="ä¿¡å·æœºåˆ¶">ä¿¡å·æœºåˆ¶</h4>
<p>é€šè¿‡ä»¥ä¸Šå‡ ç§æ–¹å¼ï¼Œå·²ç»å¯ä»¥å®ç°æ¶ˆæ¯çš„è½¬å‘ï¼Œä½†è¿˜å­˜åœ¨ä¸€äº›é—®é¢˜ã€‚å¦‚åœ¨ç¨‹åºç»ˆæ­¢æ—¶ï¼Œä¸»è¿›ç¨‹çš„ç»“æŸä¸ä¼šè®©å„ä¸ªå­è¿›ç¨‹ä¹Ÿç»“æŸï¼Œä»¥åŠå­è¿›ç¨‹æ²¡æœ‰è‰¯å¥½çš„ç»“æŸå’Œå›æ”¶æœºåˆ¶ï¼Œå› æ­¤ä¼šé€ æˆåƒµå°¸è¿›ç¨‹å’Œå­¤å„¿è¿›ç¨‹çš„å­˜åœ¨ã€‚è¿™æ˜¯éœ€è¦è§£å†³çš„ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨ä¸»è¿›ç¨‹ç»“æŸçš„æ—¶å€™ï¼Œå­è¿›ç¨‹ä¹Ÿèƒ½åŒæ­¥ç»“æŸï¼Œä»¥åŠå½“æœ‰æŸä¸€æ–¹æ–­å¼€è¿æ¥çš„æ—¶å€™ï¼Œå„ä¸ªå­è¿›ç¨‹èƒ½å¤Ÿç»“æŸï¼ŒåŒæ—¶è®©ä¸»è¿›ç¨‹å›åˆ°æœ€åŸåˆçš„ç›‘å¬çŠ¶æ€ã€‚</p>
<p>è¿™é‡Œé€‰æ‹©ä½¿ç”¨Linuxç³»ç»Ÿçš„ä¿¡å·ï¼ˆ<code>SIGNAL</code>ï¼‰æœºåˆ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</p>
<ul>
<li><p>åœ¨ä¸»è¿›ç¨‹ä¸­ï¼Œä¸ºäº†åœ¨æ¥æ”¶å¤–ç•Œä¿¡å·çš„åŒæ—¶èƒ½å¤Ÿæ€æ­»æ‰€æœ‰çš„å­è¿›ç¨‹ï¼Œå®šä¹‰äº†è¿™æ ·çš„å¤„ç†å‡½æ•°ï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">handler</span><span class="params">(<span class="keyword">int</span> signum)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; <span class="number">3</span>; i++)</span><br><span class="line">        kill(pid[i], SIGUSR1);</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>å…¶ä¸­<code>pid[3]</code>ä¸º<code>pid_t</code>å‹æ•°ç»„ï¼Œå­˜å‚¨äº†ä¸‰ä¸ªå­è¿›ç¨‹çš„<code>pid</code>ã€‚</p>
<p>åŒæ—¶åœ¨ä¸»è¿›ç¨‹ä¸­å°†ä¿¡å·<code>SIGINT</code>å’Œ<code>SIGTERM</code>ä¸ä¹‹è¿æ¥ï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">signal(SIGINT, handler);</span><br><span class="line">signal(SIGTERM, handler);</span><br></pre></td></tr></table></figure>
<p>ç”±æ­¤å¯ä»¥å®ç°ï¼Œä¸»è¿›ç¨‹åœ¨æ”¶åˆ°<code>ctrl+C</code>çš„ä¿¡å·ä»¥åŠç³»ç»Ÿ<code>kill</code>æŒ‡ä»¤æ—¶ï¼Œå¯ä»¥å‘æ‰€æœ‰çš„å­è¿›ç¨‹å‘é€ä¸€ä¸ªè‡ªå®šä¹‰ä¿¡å·<code>SIGUSR1</code>ã€‚</p></li>
<li><p>åœ¨å­è¿›ç¨‹ä¸­ï¼Œä¸ºäº†åœ¨æ¥æ”¶ä¸»è¿›ç¨‹ä¿¡å·çš„åŒæ—¶èƒ½å¤Ÿæ€æ­»å¯¹åº”çš„å­è¿›ç¨‹ï¼Œå®šä¹‰äº†è¿™æ ·çš„å¤„ç†å‡½æ•°ï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Be_Killed_Handler</span><span class="params">(<span class="keyword">int</span> signum)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    kill(My_Child, SIGKILL);</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>å…¶ä¸­<code>My_Child</code>ä¸º<code>pid_t</code>å‹å˜é‡ï¼Œå­˜å‚¨äº†å„è‡ªå¯¹åº”çš„å­è¿›ç¨‹<code>pid</code>ã€‚</p>
<p>å¹¶åœ¨å­è¿›ç¨‹ä¸­å°†ä¿¡å·<code>SIGUSR1</code>ä¸ä¹‹ç›¸è¿ï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">signal(SIGUSR1, Be_Killed_Handler);</span><br></pre></td></tr></table></figure>
<p>ç”±æ­¤å¯ä»¥å®ç°ï¼Œå½“å­è¿›ç¨‹æ”¶åˆ°æ¥è‡ªä¸»è¿›ç¨‹çš„<code>SIGUSR1</code>æ—¶ï¼Œå¯ä»¥æ€æ­»å­™è¿›ç¨‹ï¼Œç„¶åè‡ªæˆ‘ç»“æŸã€‚</p></li>
<li><p>åœ¨å­™è¿›ç¨‹ä¸­ï¼Œä¸ºäº†é˜²æ­¢å˜æˆå­¤å„¿è¿›ç¨‹ï¼Œå®šä¹‰äº†è¿™æ ·çš„å¤„ç†å‡½æ•°ï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Pipe_Handler</span><span class="params">(<span class="keyword">int</span> signum)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>å¹¶åœ¨å­è¿›ç¨‹ä¸­å°†ä¿¡å·<code>SIGPIPE</code>ä¸ä¹‹ç›¸è¿ï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">signal(SIGPIPE, Pipe_Handler);</span><br></pre></td></tr></table></figure>
<p>ç”±æ­¤å¯ä»¥å®ç°ï¼Œåœ¨ç®¡é“å¦ä¸€ç«¯å…³é—­æ—¶ï¼Œå­™è¿›ç¨‹è‡ªå·±ç»“æŸï¼Œè€Œä¸ä¼šåœ¨å¯¹åº”å­è¿›ç¨‹ç»“æŸåæˆä¸ºå­¤å„¿è¿›ç¨‹ã€‚</p></li>
</ul>
<h4 id="å…¶ä»–">å…¶ä»–</h4>
<p>é™¤äº†ä»¥ä¸Šä¸‰ç§æ–¹å¼ä¹‹å¤–ï¼Œè¿˜åœ¨ä¸»è¿›ç¨‹å¾ªç¯çš„æœ«å°¾æœ‰è¿™æ ·çš„è¯­å¥ï¼š</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> Exit_Pid = wait(<span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> i;</span><br><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; <span class="number">3</span>; i++)</span><br><span class="line">	kill(pid[i], SIGUSR1);</span><br></pre></td></tr></table></figure>
<p>ä¸»è¿›ç¨‹ä¼šé˜»å¡åœ¨è¿™é‡Œï¼Œç­‰å¾…ç€ç¬¬ä¸€ä¸ªå­è¿›ç¨‹çš„ç»“æŸï¼Œä¸€æ—¦æœ‰ä¸€ä¸ªå­è¿›ç¨‹ç»“æŸï¼Œä¸»è¿›ç¨‹å°†ä¼šç»“æŸæ‰€æœ‰çš„å­è¿›ç¨‹ï¼Œç„¶åå›åˆ°å¾ªç¯çš„å¼€å¤´ï¼Œå›åˆ°ç­‰å¾…è¿æ¥çš„çŠ¶æ€ã€‚</p>
<h2 id="éƒ¨ç½²ä¸è¿è¡Œ">éƒ¨ç½²ä¸è¿è¡Œ</h2>
<p>æœåŠ¡å™¨ç«¯ä»£ç å‘½åä¸º<code>Server.c</code>ï¼Œåœ¨<code>Linux</code>æœåŠ¡å™¨ä¸Šç¼–è¯‘è¿è¡Œçš„æŒ‡ä»¤å¦‚ä¸‹ï¼š</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ gcc Server.c -o Server</span><br><span class="line">$ nohup ./Server &amp;</span><br></pre></td></tr></table></figure>
<p>è¿™é‡Œä½¿ç”¨<code>nohup</code>ä»¥åŠ<code>&amp;</code>æ˜¯ä¸ºäº†è®©ç¨‹åºèƒ½å¤Ÿåœ¨åå°è¿è¡Œï¼Œå¹¶ä¸”å…³é—­ç»ˆç«¯ä¹Ÿèƒ½æŒç»­è¿è¡Œã€‚</p>
<h2 id="å¿ƒå¾—ä¸å±•æœ›">å¿ƒå¾—ä¸å±•æœ›</h2>
<p>æœåŠ¡ç«¯ç¨‹åºçš„ç¼–å†™æ¶‰åŠåˆ°äº†å¾ˆå¤šLinuxæ“ä½œç³»ç»Ÿä»¥åŠè®¡ç®—æœºç½‘ç»œçš„çš„å†…å®¹ï¼Œæ¶‰åŠèŒƒå›´è¾ƒå¹¿ï¼Œå¯¹äºä¸ªäººèƒ½åŠ›æå‡è¾ƒå¤§ã€‚åæœŸè€ƒè™‘æ·»åŠ è´¦æˆ·ç³»ç»Ÿï¼Œä»¥åŠæ’åç³»ç»Ÿï¼Œä½¿ä¹‹ä½œä¸ºè”ç½‘æ¸¸æˆæ›´åŠ å®Œæ•´ã€‚</p>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>test_my_site</title>
    <url>/2021/08/28/test-my-site/</url>
    <content><![CDATA[
]]></content>
  </entry>
</search>
